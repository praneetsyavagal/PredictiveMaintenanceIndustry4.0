{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bin class.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t420MoGXpZ44",
        "colab_type": "code",
        "outputId": "02ec57cc-c472-42ba-b75c-4024278335de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_rsgYZ3DkQW",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlmEtBPtmSxm",
        "colab_type": "text"
      },
      "source": [
        "RUL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwpxsKsZpwHx",
        "colab_type": "code",
        "outputId": "5f704e16-5bd8-43c3-cf47-86e03d39d61f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "! pip install pandas"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpQlSNiqt-Rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UR-recJrv0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgI6ydT2wlvR",
        "colab_type": "code",
        "outputId": "40bffe6c-c8b8-42d8-8f3a-fabdd47eac56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZh0R7zFIbLT",
        "colab_type": "code",
        "outputId": "6d019da1-fc4c-4972-8b85-d65270b7f798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGSgRrWarro8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn import preprocessing\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHtGHAU-sqmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv('/content/drive/My Drive/sensor.csv',header=0)\n",
        "train_df.drop(train_df.columns[[17]], axis=1, inplace=True)\n",
        "\n",
        "#train_df = pd.read_csv('sensor.csv', header=0)\n",
        "#train_df.drop(train_df.columns[[17]], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TqT6PmvPYoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''\n",
        "for i in range(2,54):\n",
        "  #target = train_df[i]\n",
        "  target = train_df.iloc[:, i]\n",
        "  print(target)\n",
        "  train_df = train_df.assign(InterpolateSLinear=train_df.target.interpolate(method='slinear'))\n",
        "'''\n",
        "#train_df = train_df.interpolate(method ='slinear', limit_direction ='forward') \n",
        "train_df = train_df.interpolate(method = 'linear' , order = 2 , limit_direction ='forward') \n",
        "#train_df.interpolate()\n",
        "#train_df[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyoFluN2AGs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.insert(54, \"RUL\", 0)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnVvIPdnrm5P",
        "colab_type": "code",
        "outputId": "e74be950-cee2-4d29-dd67-78ba6fb76a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "train_df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0        0\n",
              "timestamp         0\n",
              "sensor_00         0\n",
              "sensor_01         0\n",
              "sensor_02         0\n",
              "sensor_03         0\n",
              "sensor_04         0\n",
              "sensor_05         0\n",
              "sensor_06         0\n",
              "sensor_07         0\n",
              "sensor_08         0\n",
              "sensor_09         0\n",
              "sensor_10         0\n",
              "sensor_11         0\n",
              "sensor_12         0\n",
              "sensor_13         0\n",
              "sensor_14         0\n",
              "sensor_16         0\n",
              "sensor_17         0\n",
              "sensor_18         0\n",
              "sensor_19         0\n",
              "sensor_20         0\n",
              "sensor_21         0\n",
              "sensor_22         0\n",
              "sensor_23         0\n",
              "sensor_24         0\n",
              "sensor_25         0\n",
              "sensor_26         0\n",
              "sensor_27         0\n",
              "sensor_28         0\n",
              "sensor_29         0\n",
              "sensor_30         0\n",
              "sensor_31         0\n",
              "sensor_32         0\n",
              "sensor_33         0\n",
              "sensor_34         0\n",
              "sensor_35         0\n",
              "sensor_36         0\n",
              "sensor_37         0\n",
              "sensor_38         0\n",
              "sensor_39         0\n",
              "sensor_40         0\n",
              "sensor_41         0\n",
              "sensor_42         0\n",
              "sensor_43         0\n",
              "sensor_44         0\n",
              "sensor_45         0\n",
              "sensor_46         0\n",
              "sensor_47         0\n",
              "sensor_48         0\n",
              "sensor_49         0\n",
              "sensor_50         0\n",
              "sensor_51         0\n",
              "machine_status    0\n",
              "RUL               0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIbduaOzM9_J",
        "colab_type": "code",
        "outputId": "7872efb3-ad61-4bc9-a1a4-1eea400e511b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        }
      },
      "source": [
        "train_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>sensor_00</th>\n",
              "      <th>sensor_01</th>\n",
              "      <th>sensor_02</th>\n",
              "      <th>sensor_03</th>\n",
              "      <th>sensor_04</th>\n",
              "      <th>sensor_05</th>\n",
              "      <th>sensor_06</th>\n",
              "      <th>sensor_07</th>\n",
              "      <th>sensor_08</th>\n",
              "      <th>sensor_09</th>\n",
              "      <th>sensor_10</th>\n",
              "      <th>sensor_11</th>\n",
              "      <th>sensor_12</th>\n",
              "      <th>sensor_13</th>\n",
              "      <th>sensor_14</th>\n",
              "      <th>sensor_16</th>\n",
              "      <th>sensor_17</th>\n",
              "      <th>sensor_18</th>\n",
              "      <th>sensor_19</th>\n",
              "      <th>sensor_20</th>\n",
              "      <th>sensor_21</th>\n",
              "      <th>sensor_22</th>\n",
              "      <th>sensor_23</th>\n",
              "      <th>sensor_24</th>\n",
              "      <th>sensor_25</th>\n",
              "      <th>sensor_26</th>\n",
              "      <th>sensor_27</th>\n",
              "      <th>sensor_28</th>\n",
              "      <th>sensor_29</th>\n",
              "      <th>sensor_30</th>\n",
              "      <th>sensor_31</th>\n",
              "      <th>sensor_32</th>\n",
              "      <th>sensor_33</th>\n",
              "      <th>sensor_34</th>\n",
              "      <th>sensor_35</th>\n",
              "      <th>sensor_36</th>\n",
              "      <th>sensor_37</th>\n",
              "      <th>sensor_38</th>\n",
              "      <th>sensor_39</th>\n",
              "      <th>sensor_40</th>\n",
              "      <th>sensor_41</th>\n",
              "      <th>sensor_42</th>\n",
              "      <th>sensor_43</th>\n",
              "      <th>sensor_44</th>\n",
              "      <th>sensor_45</th>\n",
              "      <th>sensor_46</th>\n",
              "      <th>sensor_47</th>\n",
              "      <th>sensor_48</th>\n",
              "      <th>sensor_49</th>\n",
              "      <th>sensor_50</th>\n",
              "      <th>sensor_51</th>\n",
              "      <th>machine_status</th>\n",
              "      <th>RUL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2018-04-01 00:00:00</td>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.09201</td>\n",
              "      <td>53.211800</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2018-04-01 00:01:00</td>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.09201</td>\n",
              "      <td>53.211800</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2018-04-01 00:02:00</td>\n",
              "      <td>2.444734</td>\n",
              "      <td>47.35243</td>\n",
              "      <td>53.211800</td>\n",
              "      <td>46.397570</td>\n",
              "      <td>638.888900</td>\n",
              "      <td>73.54598</td>\n",
              "      <td>13.32465</td>\n",
              "      <td>16.03733</td>\n",
              "      <td>15.61777</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>37.86777</td>\n",
              "      <td>48.17723</td>\n",
              "      <td>32.08894</td>\n",
              "      <td>1.708474</td>\n",
              "      <td>420.8480</td>\n",
              "      <td>462.7798</td>\n",
              "      <td>459.6364</td>\n",
              "      <td>2.500062</td>\n",
              "      <td>666.2234</td>\n",
              "      <td>399.9418</td>\n",
              "      <td>880.4237</td>\n",
              "      <td>501.3617</td>\n",
              "      <td>982.7342</td>\n",
              "      <td>631.1326</td>\n",
              "      <td>740.8031</td>\n",
              "      <td>849.8997</td>\n",
              "      <td>454.2390</td>\n",
              "      <td>778.5734</td>\n",
              "      <td>715.6266</td>\n",
              "      <td>661.5740</td>\n",
              "      <td>721.8750</td>\n",
              "      <td>694.7721</td>\n",
              "      <td>441.2635</td>\n",
              "      <td>169.9820</td>\n",
              "      <td>343.1955</td>\n",
              "      <td>200.9694</td>\n",
              "      <td>93.90508</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>31.25000</td>\n",
              "      <td>69.53125</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.770830</td>\n",
              "      <td>41.66666</td>\n",
              "      <td>39.351852</td>\n",
              "      <td>65.39352</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194443</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>241.3194</td>\n",
              "      <td>203.7037</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2018-04-01 00:03:00</td>\n",
              "      <td>2.460474</td>\n",
              "      <td>47.09201</td>\n",
              "      <td>53.168400</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>628.125000</td>\n",
              "      <td>76.98898</td>\n",
              "      <td>13.31742</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>38.57977</td>\n",
              "      <td>48.65607</td>\n",
              "      <td>31.67221</td>\n",
              "      <td>1.579427</td>\n",
              "      <td>420.7494</td>\n",
              "      <td>462.8980</td>\n",
              "      <td>460.8858</td>\n",
              "      <td>2.509521</td>\n",
              "      <td>666.0114</td>\n",
              "      <td>399.1046</td>\n",
              "      <td>878.8917</td>\n",
              "      <td>499.0430</td>\n",
              "      <td>977.7520</td>\n",
              "      <td>625.4076</td>\n",
              "      <td>739.2722</td>\n",
              "      <td>847.7579</td>\n",
              "      <td>474.8731</td>\n",
              "      <td>779.5091</td>\n",
              "      <td>690.4011</td>\n",
              "      <td>686.1111</td>\n",
              "      <td>754.6875</td>\n",
              "      <td>683.3831</td>\n",
              "      <td>446.2493</td>\n",
              "      <td>166.4987</td>\n",
              "      <td>343.9586</td>\n",
              "      <td>193.1689</td>\n",
              "      <td>101.04060</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>72.13541</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>40.88541</td>\n",
              "      <td>39.062500</td>\n",
              "      <td>64.81481</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>66.84028</td>\n",
              "      <td>240.4514</td>\n",
              "      <td>203.1250</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2018-04-01 00:04:00</td>\n",
              "      <td>2.445718</td>\n",
              "      <td>47.13541</td>\n",
              "      <td>53.211800</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>636.458300</td>\n",
              "      <td>76.58897</td>\n",
              "      <td>13.35359</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>39.48939</td>\n",
              "      <td>49.06298</td>\n",
              "      <td>31.95202</td>\n",
              "      <td>1.683831</td>\n",
              "      <td>419.8926</td>\n",
              "      <td>461.4906</td>\n",
              "      <td>468.2206</td>\n",
              "      <td>2.604785</td>\n",
              "      <td>663.2111</td>\n",
              "      <td>400.5426</td>\n",
              "      <td>882.5874</td>\n",
              "      <td>498.5383</td>\n",
              "      <td>979.5755</td>\n",
              "      <td>627.1830</td>\n",
              "      <td>737.6033</td>\n",
              "      <td>846.9182</td>\n",
              "      <td>408.8159</td>\n",
              "      <td>785.2307</td>\n",
              "      <td>704.6937</td>\n",
              "      <td>631.4814</td>\n",
              "      <td>766.1458</td>\n",
              "      <td>702.4431</td>\n",
              "      <td>433.9081</td>\n",
              "      <td>164.7498</td>\n",
              "      <td>339.9630</td>\n",
              "      <td>193.8770</td>\n",
              "      <td>101.70380</td>\n",
              "      <td>42.70833</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>76.82291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>65.10416</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>158.2755</td>\n",
              "      <td>66.55093</td>\n",
              "      <td>242.1875</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220315</th>\n",
              "      <td>220315</td>\n",
              "      <td>2018-08-31 23:55:00</td>\n",
              "      <td>2.407350</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.520830</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>634.722229</td>\n",
              "      <td>64.59095</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>16.65220</td>\n",
              "      <td>15.65393</td>\n",
              "      <td>15.16204</td>\n",
              "      <td>43.17085</td>\n",
              "      <td>54.16052</td>\n",
              "      <td>38.05424</td>\n",
              "      <td>13.265320</td>\n",
              "      <td>420.7993</td>\n",
              "      <td>463.2318</td>\n",
              "      <td>458.3615</td>\n",
              "      <td>2.499117</td>\n",
              "      <td>676.6655</td>\n",
              "      <td>405.7680</td>\n",
              "      <td>894.5920</td>\n",
              "      <td>543.5801</td>\n",
              "      <td>1109.5010</td>\n",
              "      <td>611.1745</td>\n",
              "      <td>700.5885</td>\n",
              "      <td>796.5964</td>\n",
              "      <td>692.1138</td>\n",
              "      <td>779.2067</td>\n",
              "      <td>485.0358</td>\n",
              "      <td>691.6666</td>\n",
              "      <td>974.9999</td>\n",
              "      <td>927.6135</td>\n",
              "      <td>477.3156</td>\n",
              "      <td>266.0334</td>\n",
              "      <td>578.5221</td>\n",
              "      <td>817.5707</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>47.13541</td>\n",
              "      <td>29.16667</td>\n",
              "      <td>71.61458</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>30.208330</td>\n",
              "      <td>38.28125</td>\n",
              "      <td>68.287030</td>\n",
              "      <td>52.37268</td>\n",
              "      <td>48.32176</td>\n",
              "      <td>41.087960</td>\n",
              "      <td>212.3843</td>\n",
              "      <td>153.64580</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>231.1921</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220316</th>\n",
              "      <td>220316</td>\n",
              "      <td>2018-08-31 23:56:00</td>\n",
              "      <td>2.400463</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.564240</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>630.902771</td>\n",
              "      <td>65.83363</td>\n",
              "      <td>15.15480</td>\n",
              "      <td>16.70284</td>\n",
              "      <td>15.65393</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>43.21038</td>\n",
              "      <td>54.52602</td>\n",
              "      <td>38.53485</td>\n",
              "      <td>13.242270</td>\n",
              "      <td>422.1567</td>\n",
              "      <td>463.1928</td>\n",
              "      <td>468.4388</td>\n",
              "      <td>2.618476</td>\n",
              "      <td>676.6547</td>\n",
              "      <td>406.2575</td>\n",
              "      <td>895.5599</td>\n",
              "      <td>541.7014</td>\n",
              "      <td>1106.3710</td>\n",
              "      <td>609.4917</td>\n",
              "      <td>698.4915</td>\n",
              "      <td>800.1906</td>\n",
              "      <td>697.8002</td>\n",
              "      <td>797.5571</td>\n",
              "      <td>510.9510</td>\n",
              "      <td>672.2222</td>\n",
              "      <td>927.0833</td>\n",
              "      <td>907.9463</td>\n",
              "      <td>487.8679</td>\n",
              "      <td>262.2222</td>\n",
              "      <td>568.1035</td>\n",
              "      <td>807.0151</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>46.87500</td>\n",
              "      <td>28.90625</td>\n",
              "      <td>73.17708</td>\n",
              "      <td>30.208332</td>\n",
              "      <td>29.947920</td>\n",
              "      <td>38.28125</td>\n",
              "      <td>66.840280</td>\n",
              "      <td>50.63657</td>\n",
              "      <td>48.03241</td>\n",
              "      <td>40.798610</td>\n",
              "      <td>213.8310</td>\n",
              "      <td>156.25000</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>231.1921</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220317</th>\n",
              "      <td>220317</td>\n",
              "      <td>2018-08-31 23:57:00</td>\n",
              "      <td>2.396528</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.520830</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>625.925903</td>\n",
              "      <td>67.29445</td>\n",
              "      <td>15.08970</td>\n",
              "      <td>16.70284</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>43.12836</td>\n",
              "      <td>55.11779</td>\n",
              "      <td>38.52678</td>\n",
              "      <td>13.188660</td>\n",
              "      <td>420.2166</td>\n",
              "      <td>462.4065</td>\n",
              "      <td>468.6293</td>\n",
              "      <td>2.620500</td>\n",
              "      <td>677.3162</td>\n",
              "      <td>407.1144</td>\n",
              "      <td>892.2204</td>\n",
              "      <td>542.8578</td>\n",
              "      <td>1106.6980</td>\n",
              "      <td>610.9940</td>\n",
              "      <td>703.1645</td>\n",
              "      <td>800.3767</td>\n",
              "      <td>704.6601</td>\n",
              "      <td>799.3120</td>\n",
              "      <td>492.7720</td>\n",
              "      <td>689.3519</td>\n",
              "      <td>924.4791</td>\n",
              "      <td>926.8102</td>\n",
              "      <td>494.1249</td>\n",
              "      <td>260.8372</td>\n",
              "      <td>553.8872</td>\n",
              "      <td>805.5605</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>46.09375</td>\n",
              "      <td>28.64583</td>\n",
              "      <td>77.08333</td>\n",
              "      <td>29.947920</td>\n",
              "      <td>30.208330</td>\n",
              "      <td>39.06250</td>\n",
              "      <td>65.393520</td>\n",
              "      <td>48.90046</td>\n",
              "      <td>48.03241</td>\n",
              "      <td>40.798610</td>\n",
              "      <td>217.3032</td>\n",
              "      <td>155.38190</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>232.0602</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220318</th>\n",
              "      <td>220318</td>\n",
              "      <td>2018-08-31 23:58:00</td>\n",
              "      <td>2.406366</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.520832</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>635.648100</td>\n",
              "      <td>65.09175</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>16.56539</td>\n",
              "      <td>15.74074</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>42.35746</td>\n",
              "      <td>55.99321</td>\n",
              "      <td>38.89159</td>\n",
              "      <td>13.173460</td>\n",
              "      <td>420.5700</td>\n",
              "      <td>457.0362</td>\n",
              "      <td>459.7941</td>\n",
              "      <td>2.514596</td>\n",
              "      <td>672.6165</td>\n",
              "      <td>404.3277</td>\n",
              "      <td>887.9969</td>\n",
              "      <td>539.3630</td>\n",
              "      <td>1103.9550</td>\n",
              "      <td>605.7183</td>\n",
              "      <td>697.3713</td>\n",
              "      <td>793.7070</td>\n",
              "      <td>706.9692</td>\n",
              "      <td>793.0610</td>\n",
              "      <td>490.2170</td>\n",
              "      <td>687.0370</td>\n",
              "      <td>931.7708</td>\n",
              "      <td>915.4362</td>\n",
              "      <td>484.1161</td>\n",
              "      <td>261.3184</td>\n",
              "      <td>559.4439</td>\n",
              "      <td>807.0808</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>45.83333</td>\n",
              "      <td>28.38542</td>\n",
              "      <td>78.64583</td>\n",
              "      <td>29.947916</td>\n",
              "      <td>30.208332</td>\n",
              "      <td>40.62500</td>\n",
              "      <td>64.236110</td>\n",
              "      <td>47.74306</td>\n",
              "      <td>48.32176</td>\n",
              "      <td>40.509258</td>\n",
              "      <td>222.5116</td>\n",
              "      <td>153.93520</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>234.0856</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220319</th>\n",
              "      <td>220319</td>\n",
              "      <td>2018-08-31 23:59:00</td>\n",
              "      <td>2.396528</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.520832</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>639.814800</td>\n",
              "      <td>65.45634</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>16.65220</td>\n",
              "      <td>15.65393</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>42.62814</td>\n",
              "      <td>56.49642</td>\n",
              "      <td>39.40957</td>\n",
              "      <td>13.125930</td>\n",
              "      <td>421.2080</td>\n",
              "      <td>468.9915</td>\n",
              "      <td>456.5726</td>\n",
              "      <td>2.487299</td>\n",
              "      <td>676.5834</td>\n",
              "      <td>405.6293</td>\n",
              "      <td>897.8508</td>\n",
              "      <td>542.0950</td>\n",
              "      <td>1108.8270</td>\n",
              "      <td>608.5364</td>\n",
              "      <td>698.0792</td>\n",
              "      <td>800.0387</td>\n",
              "      <td>703.6251</td>\n",
              "      <td>800.2143</td>\n",
              "      <td>496.4068</td>\n",
              "      <td>686.1111</td>\n",
              "      <td>917.7083</td>\n",
              "      <td>926.3979</td>\n",
              "      <td>489.0367</td>\n",
              "      <td>258.4387</td>\n",
              "      <td>558.0558</td>\n",
              "      <td>811.1204</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>45.31250</td>\n",
              "      <td>27.86458</td>\n",
              "      <td>77.86458</td>\n",
              "      <td>29.947916</td>\n",
              "      <td>30.208332</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>62.789350</td>\n",
              "      <td>46.29630</td>\n",
              "      <td>48.90046</td>\n",
              "      <td>40.219910</td>\n",
              "      <td>227.4306</td>\n",
              "      <td>150.46300</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>234.0856</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220320 rows × 55 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0            timestamp  ...  machine_status  RUL\n",
              "0                0  2018-04-01 00:00:00  ...          NORMAL    0\n",
              "1                1  2018-04-01 00:01:00  ...          NORMAL    0\n",
              "2                2  2018-04-01 00:02:00  ...          NORMAL    0\n",
              "3                3  2018-04-01 00:03:00  ...          NORMAL    0\n",
              "4                4  2018-04-01 00:04:00  ...          NORMAL    0\n",
              "...            ...                  ...  ...             ...  ...\n",
              "220315      220315  2018-08-31 23:55:00  ...          NORMAL    0\n",
              "220316      220316  2018-08-31 23:56:00  ...          NORMAL    0\n",
              "220317      220317  2018-08-31 23:57:00  ...          NORMAL    0\n",
              "220318      220318  2018-08-31 23:58:00  ...          NORMAL    0\n",
              "220319      220319  2018-08-31 23:59:00  ...          NORMAL    0\n",
              "\n",
              "[220320 rows x 55 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyCA3JWhumBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "k = 0\n",
        "for i in range(len(train_df)):\n",
        "    if(train_df[\"machine_status\"][i] == \"BROKEN\" ):\n",
        "        count = 0\n",
        "        for j in range(i,k-1,-1):\n",
        "            train_df.at[j,'RUL']= count\n",
        "            count = count + 1\n",
        "        k = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCpXOU0VpJKm",
        "colab_type": "code",
        "outputId": "9bb2170d-0b40-4fd4-abee-41a1927a2126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "\n",
        "n = len(train_df)-1\n",
        "while(train_df[\"machine_status\"][n] != \"BROKEN\"):\n",
        "    n -= 1   \n",
        "train_df = train_df[:n+1]        \n",
        "train_df[\"machine_status\"][n+1] = 0    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlHUvnzKyZuj",
        "colab_type": "code",
        "outputId": "011df071-77f5-44ce-9f98-1eb5c2fc87d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "from pandas import read_csv, DataFrame\n",
        "from numpy.random import seed\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h46Dj6SRwSkB",
        "colab_type": "code",
        "outputId": "44e3b86a-6b1d-44d2-a871-7eec65723869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "train_df.drop(['machine_status'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBk9fV-zwXYf",
        "colab_type": "code",
        "outputId": "8aa1193d-889d-4b09-d72a-b2ba211a0cc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "\n",
        "train_df.drop(['Unnamed: 0'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ5P2v4WMUQC",
        "colab_type": "code",
        "outputId": "21c29f5b-921a-4ae3-eca7-9f03556fada3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        }
      },
      "source": [
        "\n",
        "\n",
        "train_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>sensor_00</th>\n",
              "      <th>sensor_01</th>\n",
              "      <th>sensor_02</th>\n",
              "      <th>sensor_03</th>\n",
              "      <th>sensor_04</th>\n",
              "      <th>sensor_05</th>\n",
              "      <th>sensor_06</th>\n",
              "      <th>sensor_07</th>\n",
              "      <th>sensor_08</th>\n",
              "      <th>sensor_09</th>\n",
              "      <th>sensor_10</th>\n",
              "      <th>sensor_11</th>\n",
              "      <th>sensor_12</th>\n",
              "      <th>sensor_13</th>\n",
              "      <th>sensor_14</th>\n",
              "      <th>sensor_16</th>\n",
              "      <th>sensor_17</th>\n",
              "      <th>sensor_18</th>\n",
              "      <th>sensor_19</th>\n",
              "      <th>sensor_20</th>\n",
              "      <th>sensor_21</th>\n",
              "      <th>sensor_22</th>\n",
              "      <th>sensor_23</th>\n",
              "      <th>sensor_24</th>\n",
              "      <th>sensor_25</th>\n",
              "      <th>sensor_26</th>\n",
              "      <th>sensor_27</th>\n",
              "      <th>sensor_28</th>\n",
              "      <th>sensor_29</th>\n",
              "      <th>sensor_30</th>\n",
              "      <th>sensor_31</th>\n",
              "      <th>sensor_32</th>\n",
              "      <th>sensor_33</th>\n",
              "      <th>sensor_34</th>\n",
              "      <th>sensor_35</th>\n",
              "      <th>sensor_36</th>\n",
              "      <th>sensor_37</th>\n",
              "      <th>sensor_38</th>\n",
              "      <th>sensor_39</th>\n",
              "      <th>sensor_40</th>\n",
              "      <th>sensor_41</th>\n",
              "      <th>sensor_42</th>\n",
              "      <th>sensor_43</th>\n",
              "      <th>sensor_44</th>\n",
              "      <th>sensor_45</th>\n",
              "      <th>sensor_46</th>\n",
              "      <th>sensor_47</th>\n",
              "      <th>sensor_48</th>\n",
              "      <th>sensor_49</th>\n",
              "      <th>sensor_50</th>\n",
              "      <th>sensor_51</th>\n",
              "      <th>RUL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-04-01 00:00:00</td>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-04-01 00:01:00</td>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-04-01 00:02:00</td>\n",
              "      <td>2.444734</td>\n",
              "      <td>47.352430</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397570</td>\n",
              "      <td>638.888900</td>\n",
              "      <td>73.54598</td>\n",
              "      <td>13.32465</td>\n",
              "      <td>16.03733</td>\n",
              "      <td>15.61777</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>37.86777</td>\n",
              "      <td>48.17723</td>\n",
              "      <td>32.08894</td>\n",
              "      <td>1.708474</td>\n",
              "      <td>420.8480</td>\n",
              "      <td>462.7798</td>\n",
              "      <td>459.6364</td>\n",
              "      <td>2.500062</td>\n",
              "      <td>666.2234</td>\n",
              "      <td>399.9418</td>\n",
              "      <td>880.4237</td>\n",
              "      <td>501.3617</td>\n",
              "      <td>982.7342</td>\n",
              "      <td>631.1326</td>\n",
              "      <td>740.8031</td>\n",
              "      <td>849.8997</td>\n",
              "      <td>454.2390</td>\n",
              "      <td>778.5734</td>\n",
              "      <td>715.6266</td>\n",
              "      <td>661.5740</td>\n",
              "      <td>721.8750</td>\n",
              "      <td>694.7721</td>\n",
              "      <td>441.2635</td>\n",
              "      <td>169.9820</td>\n",
              "      <td>343.1955</td>\n",
              "      <td>200.9694</td>\n",
              "      <td>93.90508</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>31.25000</td>\n",
              "      <td>69.53125</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.770830</td>\n",
              "      <td>41.66666</td>\n",
              "      <td>39.351852</td>\n",
              "      <td>65.39352</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194443</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>241.3194</td>\n",
              "      <td>203.7037</td>\n",
              "      <td>17153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-04-01 00:03:00</td>\n",
              "      <td>2.460474</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.16840</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>628.125000</td>\n",
              "      <td>76.98898</td>\n",
              "      <td>13.31742</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>38.57977</td>\n",
              "      <td>48.65607</td>\n",
              "      <td>31.67221</td>\n",
              "      <td>1.579427</td>\n",
              "      <td>420.7494</td>\n",
              "      <td>462.8980</td>\n",
              "      <td>460.8858</td>\n",
              "      <td>2.509521</td>\n",
              "      <td>666.0114</td>\n",
              "      <td>399.1046</td>\n",
              "      <td>878.8917</td>\n",
              "      <td>499.0430</td>\n",
              "      <td>977.7520</td>\n",
              "      <td>625.4076</td>\n",
              "      <td>739.2722</td>\n",
              "      <td>847.7579</td>\n",
              "      <td>474.8731</td>\n",
              "      <td>779.5091</td>\n",
              "      <td>690.4011</td>\n",
              "      <td>686.1111</td>\n",
              "      <td>754.6875</td>\n",
              "      <td>683.3831</td>\n",
              "      <td>446.2493</td>\n",
              "      <td>166.4987</td>\n",
              "      <td>343.9586</td>\n",
              "      <td>193.1689</td>\n",
              "      <td>101.04060</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>72.13541</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>40.88541</td>\n",
              "      <td>39.062500</td>\n",
              "      <td>64.81481</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>66.84028</td>\n",
              "      <td>240.4514</td>\n",
              "      <td>203.1250</td>\n",
              "      <td>17152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-04-01 00:04:00</td>\n",
              "      <td>2.445718</td>\n",
              "      <td>47.135410</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>636.458300</td>\n",
              "      <td>76.58897</td>\n",
              "      <td>13.35359</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>39.48939</td>\n",
              "      <td>49.06298</td>\n",
              "      <td>31.95202</td>\n",
              "      <td>1.683831</td>\n",
              "      <td>419.8926</td>\n",
              "      <td>461.4906</td>\n",
              "      <td>468.2206</td>\n",
              "      <td>2.604785</td>\n",
              "      <td>663.2111</td>\n",
              "      <td>400.5426</td>\n",
              "      <td>882.5874</td>\n",
              "      <td>498.5383</td>\n",
              "      <td>979.5755</td>\n",
              "      <td>627.1830</td>\n",
              "      <td>737.6033</td>\n",
              "      <td>846.9182</td>\n",
              "      <td>408.8159</td>\n",
              "      <td>785.2307</td>\n",
              "      <td>704.6937</td>\n",
              "      <td>631.4814</td>\n",
              "      <td>766.1458</td>\n",
              "      <td>702.4431</td>\n",
              "      <td>433.9081</td>\n",
              "      <td>164.7498</td>\n",
              "      <td>339.9630</td>\n",
              "      <td>193.8770</td>\n",
              "      <td>101.70380</td>\n",
              "      <td>42.70833</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>76.82291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>65.10416</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>158.2755</td>\n",
              "      <td>66.55093</td>\n",
              "      <td>242.1875</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166436</th>\n",
              "      <td>2018-07-25 13:56:00</td>\n",
              "      <td>2.313889</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>71.464119</td>\n",
              "      <td>73.00980</td>\n",
              "      <td>14.36632</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.12587</td>\n",
              "      <td>38.34975</td>\n",
              "      <td>49.88500</td>\n",
              "      <td>31.94489</td>\n",
              "      <td>12.285810</td>\n",
              "      <td>421.4294</td>\n",
              "      <td>458.9118</td>\n",
              "      <td>456.3463</td>\n",
              "      <td>2.459503</td>\n",
              "      <td>666.8698</td>\n",
              "      <td>398.0684</td>\n",
              "      <td>879.7014</td>\n",
              "      <td>536.2166</td>\n",
              "      <td>1091.1240</td>\n",
              "      <td>629.8835</td>\n",
              "      <td>740.9833</td>\n",
              "      <td>981.7558</td>\n",
              "      <td>497.2223</td>\n",
              "      <td>995.5433</td>\n",
              "      <td>563.5882</td>\n",
              "      <td>676.8519</td>\n",
              "      <td>949.9999</td>\n",
              "      <td>990.9057</td>\n",
              "      <td>564.7968</td>\n",
              "      <td>342.3690</td>\n",
              "      <td>498.5037</td>\n",
              "      <td>804.9069</td>\n",
              "      <td>55.82830</td>\n",
              "      <td>53.38541</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>93.48958</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>45.138890</td>\n",
              "      <td>252.3148</td>\n",
              "      <td>69.15509</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>198.2060</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166437</th>\n",
              "      <td>2018-07-25 13:57:00</td>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923611</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>72.47576</td>\n",
              "      <td>14.33015</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.76968</td>\n",
              "      <td>15.16204</td>\n",
              "      <td>38.58192</td>\n",
              "      <td>49.96759</td>\n",
              "      <td>31.46310</td>\n",
              "      <td>12.213550</td>\n",
              "      <td>420.6030</td>\n",
              "      <td>460.6981</td>\n",
              "      <td>467.5996</td>\n",
              "      <td>2.603880</td>\n",
              "      <td>665.2674</td>\n",
              "      <td>399.1180</td>\n",
              "      <td>879.2195</td>\n",
              "      <td>534.4346</td>\n",
              "      <td>1089.8550</td>\n",
              "      <td>628.6547</td>\n",
              "      <td>738.9200</td>\n",
              "      <td>979.7875</td>\n",
              "      <td>504.8127</td>\n",
              "      <td>1004.9950</td>\n",
              "      <td>569.5920</td>\n",
              "      <td>719.9074</td>\n",
              "      <td>934.3749</td>\n",
              "      <td>982.0567</td>\n",
              "      <td>559.5000</td>\n",
              "      <td>345.2228</td>\n",
              "      <td>500.3699</td>\n",
              "      <td>804.2908</td>\n",
              "      <td>49.17398</td>\n",
              "      <td>52.60416</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>95.57291</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>253.1829</td>\n",
              "      <td>71.46991</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>202.8356</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166438</th>\n",
              "      <td>2018-07-25 13:58:00</td>\n",
              "      <td>2.322743</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>70.48889</td>\n",
              "      <td>14.46036</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.73351</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>38.87765</td>\n",
              "      <td>49.75908</td>\n",
              "      <td>31.78839</td>\n",
              "      <td>12.180410</td>\n",
              "      <td>419.6096</td>\n",
              "      <td>462.2329</td>\n",
              "      <td>466.0083</td>\n",
              "      <td>2.577086</td>\n",
              "      <td>667.1351</td>\n",
              "      <td>398.3854</td>\n",
              "      <td>881.7244</td>\n",
              "      <td>536.0204</td>\n",
              "      <td>1095.5440</td>\n",
              "      <td>630.5479</td>\n",
              "      <td>743.5366</td>\n",
              "      <td>981.5860</td>\n",
              "      <td>488.0563</td>\n",
              "      <td>1009.6940</td>\n",
              "      <td>559.3523</td>\n",
              "      <td>678.2407</td>\n",
              "      <td>947.9166</td>\n",
              "      <td>1016.9990</td>\n",
              "      <td>558.1667</td>\n",
              "      <td>344.4498</td>\n",
              "      <td>490.6038</td>\n",
              "      <td>796.3863</td>\n",
              "      <td>39.34733</td>\n",
              "      <td>51.82291</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>100.52080</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>49.73958</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>52.08333</td>\n",
              "      <td>61.34259</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>249.1319</td>\n",
              "      <td>72.33796</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>206.8866</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166439</th>\n",
              "      <td>2018-07-25 13:59:00</td>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.789930</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>68.74365</td>\n",
              "      <td>14.43866</td>\n",
              "      <td>16.07350</td>\n",
              "      <td>15.53096</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>38.21849</td>\n",
              "      <td>49.73039</td>\n",
              "      <td>32.32672</td>\n",
              "      <td>12.209670</td>\n",
              "      <td>420.0334</td>\n",
              "      <td>460.0883</td>\n",
              "      <td>460.4448</td>\n",
              "      <td>2.509833</td>\n",
              "      <td>664.8566</td>\n",
              "      <td>399.6012</td>\n",
              "      <td>880.7674</td>\n",
              "      <td>535.4742</td>\n",
              "      <td>1092.3240</td>\n",
              "      <td>628.6292</td>\n",
              "      <td>743.1452</td>\n",
              "      <td>982.4372</td>\n",
              "      <td>511.7988</td>\n",
              "      <td>1025.1100</td>\n",
              "      <td>559.1011</td>\n",
              "      <td>743.5185</td>\n",
              "      <td>960.4166</td>\n",
              "      <td>998.0515</td>\n",
              "      <td>551.9908</td>\n",
              "      <td>339.3840</td>\n",
              "      <td>492.3628</td>\n",
              "      <td>796.2476</td>\n",
              "      <td>53.73619</td>\n",
              "      <td>51.04166</td>\n",
              "      <td>40.62500</td>\n",
              "      <td>104.68750</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>50.52083</td>\n",
              "      <td>52.662040</td>\n",
              "      <td>53.24074</td>\n",
              "      <td>63.36805</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>244.5023</td>\n",
              "      <td>72.62731</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>209.7801</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166440</th>\n",
              "      <td>2018-07-25 14:00:00</td>\n",
              "      <td>2.318808</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>420.503448</td>\n",
              "      <td>72.52040</td>\n",
              "      <td>14.18547</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>36.71748</td>\n",
              "      <td>50.04619</td>\n",
              "      <td>30.74288</td>\n",
              "      <td>12.217860</td>\n",
              "      <td>420.8600</td>\n",
              "      <td>463.5181</td>\n",
              "      <td>455.2702</td>\n",
              "      <td>2.468381</td>\n",
              "      <td>665.9402</td>\n",
              "      <td>399.0930</td>\n",
              "      <td>880.8410</td>\n",
              "      <td>531.8727</td>\n",
              "      <td>1093.5790</td>\n",
              "      <td>626.4651</td>\n",
              "      <td>740.6738</td>\n",
              "      <td>981.4561</td>\n",
              "      <td>490.7728</td>\n",
              "      <td>1023.1660</td>\n",
              "      <td>551.2756</td>\n",
              "      <td>703.7037</td>\n",
              "      <td>971.8749</td>\n",
              "      <td>1013.3760</td>\n",
              "      <td>546.6656</td>\n",
              "      <td>363.0320</td>\n",
              "      <td>522.5594</td>\n",
              "      <td>812.8298</td>\n",
              "      <td>44.77601</td>\n",
              "      <td>50.00000</td>\n",
              "      <td>39.06250</td>\n",
              "      <td>102.86460</td>\n",
              "      <td>35.416660</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>51.30208</td>\n",
              "      <td>52.102000</td>\n",
              "      <td>52.66204</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>43.981480</td>\n",
              "      <td>230.3241</td>\n",
              "      <td>69.73380</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>205.7292</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166441 rows × 53 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  timestamp  sensor_00  sensor_01  ...  sensor_50  sensor_51    RUL\n",
              "0       2018-04-01 00:00:00   2.465394  47.092010  ...   243.0556   201.3889  17155\n",
              "1       2018-04-01 00:01:00   2.465394  47.092010  ...   243.0556   201.3889  17154\n",
              "2       2018-04-01 00:02:00   2.444734  47.352430  ...   241.3194   203.7037  17153\n",
              "3       2018-04-01 00:03:00   2.460474  47.092010  ...   240.4514   203.1250  17152\n",
              "4       2018-04-01 00:04:00   2.445718  47.135410  ...   242.1875   201.3889  17151\n",
              "...                     ...        ...        ...  ...        ...        ...    ...\n",
              "166436  2018-07-25 13:56:00   2.313889  45.833330  ...  1000.0000   198.2060      4\n",
              "166437  2018-07-25 13:57:00   2.315856  45.833332  ...  1000.0000   202.8356      3\n",
              "166438  2018-07-25 13:58:00   2.322743  45.833330  ...  1000.0000   206.8866      2\n",
              "166439  2018-07-25 13:59:00   2.315856  45.789930  ...  1000.0000   209.7801      1\n",
              "166440  2018-07-25 14:00:00   2.318808  45.833332  ...  1000.0000   205.7292      0\n",
              "\n",
              "[166441 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbOabGb9bMxh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Bin Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3HfBku6vmqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "period = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vniy7NAvvwMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.insert(53, \"lab_bin\", 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMLE8tFNvwcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for i in range(len(train_df)):\n",
        "    if(train_df[\"RUL\"][i] < period):\n",
        "        train_df.at[i,'lab_bin'] = 1    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC8u8bjMwxI0",
        "colab_type": "code",
        "outputId": "a810e541-265c-4b20-f5ca-bef1a254feb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        }
      },
      "source": [
        "train_df    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>sensor_00</th>\n",
              "      <th>sensor_01</th>\n",
              "      <th>sensor_02</th>\n",
              "      <th>sensor_03</th>\n",
              "      <th>sensor_04</th>\n",
              "      <th>sensor_05</th>\n",
              "      <th>sensor_06</th>\n",
              "      <th>sensor_07</th>\n",
              "      <th>sensor_08</th>\n",
              "      <th>sensor_09</th>\n",
              "      <th>sensor_10</th>\n",
              "      <th>sensor_11</th>\n",
              "      <th>sensor_12</th>\n",
              "      <th>sensor_13</th>\n",
              "      <th>sensor_14</th>\n",
              "      <th>sensor_16</th>\n",
              "      <th>sensor_17</th>\n",
              "      <th>sensor_18</th>\n",
              "      <th>sensor_19</th>\n",
              "      <th>sensor_20</th>\n",
              "      <th>sensor_21</th>\n",
              "      <th>sensor_22</th>\n",
              "      <th>sensor_23</th>\n",
              "      <th>sensor_24</th>\n",
              "      <th>sensor_25</th>\n",
              "      <th>sensor_26</th>\n",
              "      <th>sensor_27</th>\n",
              "      <th>sensor_28</th>\n",
              "      <th>sensor_29</th>\n",
              "      <th>sensor_30</th>\n",
              "      <th>sensor_31</th>\n",
              "      <th>sensor_32</th>\n",
              "      <th>sensor_33</th>\n",
              "      <th>sensor_34</th>\n",
              "      <th>sensor_35</th>\n",
              "      <th>sensor_36</th>\n",
              "      <th>sensor_37</th>\n",
              "      <th>sensor_38</th>\n",
              "      <th>sensor_39</th>\n",
              "      <th>sensor_40</th>\n",
              "      <th>sensor_41</th>\n",
              "      <th>sensor_42</th>\n",
              "      <th>sensor_43</th>\n",
              "      <th>sensor_44</th>\n",
              "      <th>sensor_45</th>\n",
              "      <th>sensor_46</th>\n",
              "      <th>sensor_47</th>\n",
              "      <th>sensor_48</th>\n",
              "      <th>sensor_49</th>\n",
              "      <th>sensor_50</th>\n",
              "      <th>sensor_51</th>\n",
              "      <th>RUL</th>\n",
              "      <th>lab_bin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-04-01 00:00:00</td>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17155</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-04-01 00:01:00</td>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17154</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-04-01 00:02:00</td>\n",
              "      <td>2.444734</td>\n",
              "      <td>47.352430</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397570</td>\n",
              "      <td>638.888900</td>\n",
              "      <td>73.54598</td>\n",
              "      <td>13.32465</td>\n",
              "      <td>16.03733</td>\n",
              "      <td>15.61777</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>37.86777</td>\n",
              "      <td>48.17723</td>\n",
              "      <td>32.08894</td>\n",
              "      <td>1.708474</td>\n",
              "      <td>420.8480</td>\n",
              "      <td>462.7798</td>\n",
              "      <td>459.6364</td>\n",
              "      <td>2.500062</td>\n",
              "      <td>666.2234</td>\n",
              "      <td>399.9418</td>\n",
              "      <td>880.4237</td>\n",
              "      <td>501.3617</td>\n",
              "      <td>982.7342</td>\n",
              "      <td>631.1326</td>\n",
              "      <td>740.8031</td>\n",
              "      <td>849.8997</td>\n",
              "      <td>454.2390</td>\n",
              "      <td>778.5734</td>\n",
              "      <td>715.6266</td>\n",
              "      <td>661.5740</td>\n",
              "      <td>721.8750</td>\n",
              "      <td>694.7721</td>\n",
              "      <td>441.2635</td>\n",
              "      <td>169.9820</td>\n",
              "      <td>343.1955</td>\n",
              "      <td>200.9694</td>\n",
              "      <td>93.90508</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>31.25000</td>\n",
              "      <td>69.53125</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.770830</td>\n",
              "      <td>41.66666</td>\n",
              "      <td>39.351852</td>\n",
              "      <td>65.39352</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194443</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>241.3194</td>\n",
              "      <td>203.7037</td>\n",
              "      <td>17153</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-04-01 00:03:00</td>\n",
              "      <td>2.460474</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.16840</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>628.125000</td>\n",
              "      <td>76.98898</td>\n",
              "      <td>13.31742</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>38.57977</td>\n",
              "      <td>48.65607</td>\n",
              "      <td>31.67221</td>\n",
              "      <td>1.579427</td>\n",
              "      <td>420.7494</td>\n",
              "      <td>462.8980</td>\n",
              "      <td>460.8858</td>\n",
              "      <td>2.509521</td>\n",
              "      <td>666.0114</td>\n",
              "      <td>399.1046</td>\n",
              "      <td>878.8917</td>\n",
              "      <td>499.0430</td>\n",
              "      <td>977.7520</td>\n",
              "      <td>625.4076</td>\n",
              "      <td>739.2722</td>\n",
              "      <td>847.7579</td>\n",
              "      <td>474.8731</td>\n",
              "      <td>779.5091</td>\n",
              "      <td>690.4011</td>\n",
              "      <td>686.1111</td>\n",
              "      <td>754.6875</td>\n",
              "      <td>683.3831</td>\n",
              "      <td>446.2493</td>\n",
              "      <td>166.4987</td>\n",
              "      <td>343.9586</td>\n",
              "      <td>193.1689</td>\n",
              "      <td>101.04060</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>72.13541</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>40.88541</td>\n",
              "      <td>39.062500</td>\n",
              "      <td>64.81481</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>66.84028</td>\n",
              "      <td>240.4514</td>\n",
              "      <td>203.1250</td>\n",
              "      <td>17152</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-04-01 00:04:00</td>\n",
              "      <td>2.445718</td>\n",
              "      <td>47.135410</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>636.458300</td>\n",
              "      <td>76.58897</td>\n",
              "      <td>13.35359</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>39.48939</td>\n",
              "      <td>49.06298</td>\n",
              "      <td>31.95202</td>\n",
              "      <td>1.683831</td>\n",
              "      <td>419.8926</td>\n",
              "      <td>461.4906</td>\n",
              "      <td>468.2206</td>\n",
              "      <td>2.604785</td>\n",
              "      <td>663.2111</td>\n",
              "      <td>400.5426</td>\n",
              "      <td>882.5874</td>\n",
              "      <td>498.5383</td>\n",
              "      <td>979.5755</td>\n",
              "      <td>627.1830</td>\n",
              "      <td>737.6033</td>\n",
              "      <td>846.9182</td>\n",
              "      <td>408.8159</td>\n",
              "      <td>785.2307</td>\n",
              "      <td>704.6937</td>\n",
              "      <td>631.4814</td>\n",
              "      <td>766.1458</td>\n",
              "      <td>702.4431</td>\n",
              "      <td>433.9081</td>\n",
              "      <td>164.7498</td>\n",
              "      <td>339.9630</td>\n",
              "      <td>193.8770</td>\n",
              "      <td>101.70380</td>\n",
              "      <td>42.70833</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>76.82291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>65.10416</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>158.2755</td>\n",
              "      <td>66.55093</td>\n",
              "      <td>242.1875</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17151</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166436</th>\n",
              "      <td>2018-07-25 13:56:00</td>\n",
              "      <td>2.313889</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>71.464119</td>\n",
              "      <td>73.00980</td>\n",
              "      <td>14.36632</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.12587</td>\n",
              "      <td>38.34975</td>\n",
              "      <td>49.88500</td>\n",
              "      <td>31.94489</td>\n",
              "      <td>12.285810</td>\n",
              "      <td>421.4294</td>\n",
              "      <td>458.9118</td>\n",
              "      <td>456.3463</td>\n",
              "      <td>2.459503</td>\n",
              "      <td>666.8698</td>\n",
              "      <td>398.0684</td>\n",
              "      <td>879.7014</td>\n",
              "      <td>536.2166</td>\n",
              "      <td>1091.1240</td>\n",
              "      <td>629.8835</td>\n",
              "      <td>740.9833</td>\n",
              "      <td>981.7558</td>\n",
              "      <td>497.2223</td>\n",
              "      <td>995.5433</td>\n",
              "      <td>563.5882</td>\n",
              "      <td>676.8519</td>\n",
              "      <td>949.9999</td>\n",
              "      <td>990.9057</td>\n",
              "      <td>564.7968</td>\n",
              "      <td>342.3690</td>\n",
              "      <td>498.5037</td>\n",
              "      <td>804.9069</td>\n",
              "      <td>55.82830</td>\n",
              "      <td>53.38541</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>93.48958</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>45.138890</td>\n",
              "      <td>252.3148</td>\n",
              "      <td>69.15509</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>198.2060</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166437</th>\n",
              "      <td>2018-07-25 13:57:00</td>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923611</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>72.47576</td>\n",
              "      <td>14.33015</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.76968</td>\n",
              "      <td>15.16204</td>\n",
              "      <td>38.58192</td>\n",
              "      <td>49.96759</td>\n",
              "      <td>31.46310</td>\n",
              "      <td>12.213550</td>\n",
              "      <td>420.6030</td>\n",
              "      <td>460.6981</td>\n",
              "      <td>467.5996</td>\n",
              "      <td>2.603880</td>\n",
              "      <td>665.2674</td>\n",
              "      <td>399.1180</td>\n",
              "      <td>879.2195</td>\n",
              "      <td>534.4346</td>\n",
              "      <td>1089.8550</td>\n",
              "      <td>628.6547</td>\n",
              "      <td>738.9200</td>\n",
              "      <td>979.7875</td>\n",
              "      <td>504.8127</td>\n",
              "      <td>1004.9950</td>\n",
              "      <td>569.5920</td>\n",
              "      <td>719.9074</td>\n",
              "      <td>934.3749</td>\n",
              "      <td>982.0567</td>\n",
              "      <td>559.5000</td>\n",
              "      <td>345.2228</td>\n",
              "      <td>500.3699</td>\n",
              "      <td>804.2908</td>\n",
              "      <td>49.17398</td>\n",
              "      <td>52.60416</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>95.57291</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>253.1829</td>\n",
              "      <td>71.46991</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>202.8356</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166438</th>\n",
              "      <td>2018-07-25 13:58:00</td>\n",
              "      <td>2.322743</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>70.48889</td>\n",
              "      <td>14.46036</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.73351</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>38.87765</td>\n",
              "      <td>49.75908</td>\n",
              "      <td>31.78839</td>\n",
              "      <td>12.180410</td>\n",
              "      <td>419.6096</td>\n",
              "      <td>462.2329</td>\n",
              "      <td>466.0083</td>\n",
              "      <td>2.577086</td>\n",
              "      <td>667.1351</td>\n",
              "      <td>398.3854</td>\n",
              "      <td>881.7244</td>\n",
              "      <td>536.0204</td>\n",
              "      <td>1095.5440</td>\n",
              "      <td>630.5479</td>\n",
              "      <td>743.5366</td>\n",
              "      <td>981.5860</td>\n",
              "      <td>488.0563</td>\n",
              "      <td>1009.6940</td>\n",
              "      <td>559.3523</td>\n",
              "      <td>678.2407</td>\n",
              "      <td>947.9166</td>\n",
              "      <td>1016.9990</td>\n",
              "      <td>558.1667</td>\n",
              "      <td>344.4498</td>\n",
              "      <td>490.6038</td>\n",
              "      <td>796.3863</td>\n",
              "      <td>39.34733</td>\n",
              "      <td>51.82291</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>100.52080</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>49.73958</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>52.08333</td>\n",
              "      <td>61.34259</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>249.1319</td>\n",
              "      <td>72.33796</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>206.8866</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166439</th>\n",
              "      <td>2018-07-25 13:59:00</td>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.789930</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>68.74365</td>\n",
              "      <td>14.43866</td>\n",
              "      <td>16.07350</td>\n",
              "      <td>15.53096</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>38.21849</td>\n",
              "      <td>49.73039</td>\n",
              "      <td>32.32672</td>\n",
              "      <td>12.209670</td>\n",
              "      <td>420.0334</td>\n",
              "      <td>460.0883</td>\n",
              "      <td>460.4448</td>\n",
              "      <td>2.509833</td>\n",
              "      <td>664.8566</td>\n",
              "      <td>399.6012</td>\n",
              "      <td>880.7674</td>\n",
              "      <td>535.4742</td>\n",
              "      <td>1092.3240</td>\n",
              "      <td>628.6292</td>\n",
              "      <td>743.1452</td>\n",
              "      <td>982.4372</td>\n",
              "      <td>511.7988</td>\n",
              "      <td>1025.1100</td>\n",
              "      <td>559.1011</td>\n",
              "      <td>743.5185</td>\n",
              "      <td>960.4166</td>\n",
              "      <td>998.0515</td>\n",
              "      <td>551.9908</td>\n",
              "      <td>339.3840</td>\n",
              "      <td>492.3628</td>\n",
              "      <td>796.2476</td>\n",
              "      <td>53.73619</td>\n",
              "      <td>51.04166</td>\n",
              "      <td>40.62500</td>\n",
              "      <td>104.68750</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>50.52083</td>\n",
              "      <td>52.662040</td>\n",
              "      <td>53.24074</td>\n",
              "      <td>63.36805</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>244.5023</td>\n",
              "      <td>72.62731</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>209.7801</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166440</th>\n",
              "      <td>2018-07-25 14:00:00</td>\n",
              "      <td>2.318808</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>420.503448</td>\n",
              "      <td>72.52040</td>\n",
              "      <td>14.18547</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>36.71748</td>\n",
              "      <td>50.04619</td>\n",
              "      <td>30.74288</td>\n",
              "      <td>12.217860</td>\n",
              "      <td>420.8600</td>\n",
              "      <td>463.5181</td>\n",
              "      <td>455.2702</td>\n",
              "      <td>2.468381</td>\n",
              "      <td>665.9402</td>\n",
              "      <td>399.0930</td>\n",
              "      <td>880.8410</td>\n",
              "      <td>531.8727</td>\n",
              "      <td>1093.5790</td>\n",
              "      <td>626.4651</td>\n",
              "      <td>740.6738</td>\n",
              "      <td>981.4561</td>\n",
              "      <td>490.7728</td>\n",
              "      <td>1023.1660</td>\n",
              "      <td>551.2756</td>\n",
              "      <td>703.7037</td>\n",
              "      <td>971.8749</td>\n",
              "      <td>1013.3760</td>\n",
              "      <td>546.6656</td>\n",
              "      <td>363.0320</td>\n",
              "      <td>522.5594</td>\n",
              "      <td>812.8298</td>\n",
              "      <td>44.77601</td>\n",
              "      <td>50.00000</td>\n",
              "      <td>39.06250</td>\n",
              "      <td>102.86460</td>\n",
              "      <td>35.416660</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>51.30208</td>\n",
              "      <td>52.102000</td>\n",
              "      <td>52.66204</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>43.981480</td>\n",
              "      <td>230.3241</td>\n",
              "      <td>69.73380</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>205.7292</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166441 rows × 54 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  timestamp  sensor_00  sensor_01  ...  sensor_51    RUL  lab_bin\n",
              "0       2018-04-01 00:00:00   2.465394  47.092010  ...   201.3889  17155        0\n",
              "1       2018-04-01 00:01:00   2.465394  47.092010  ...   201.3889  17154        0\n",
              "2       2018-04-01 00:02:00   2.444734  47.352430  ...   203.7037  17153        0\n",
              "3       2018-04-01 00:03:00   2.460474  47.092010  ...   203.1250  17152        0\n",
              "4       2018-04-01 00:04:00   2.445718  47.135410  ...   201.3889  17151        0\n",
              "...                     ...        ...        ...  ...        ...    ...      ...\n",
              "166436  2018-07-25 13:56:00   2.313889  45.833330  ...   198.2060      4        1\n",
              "166437  2018-07-25 13:57:00   2.315856  45.833332  ...   202.8356      3        1\n",
              "166438  2018-07-25 13:58:00   2.322743  45.833330  ...   206.8866      2        1\n",
              "166439  2018-07-25 13:59:00   2.315856  45.789930  ...   209.7801      1        1\n",
              "166440  2018-07-25 14:00:00   2.318808  45.833332  ...   205.7292      0        1\n",
              "\n",
              "[166441 rows x 54 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0C5MhmYw5et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn import model_selection\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "\n",
        "from sklearn.metrics import roc_auc_score,roc_curve\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import GRU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqgNFK4dbwrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = train_df['lab_bin']\n",
        "X = train_df.iloc[:, 1:52]\n",
        "X.head()\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.6, random_state = seed(20))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJTRn_wJHIoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "X_train =sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a069MRJvHOSl",
        "colab_type": "code",
        "outputId": "37290cee-4202-4e73-dd31-c97be39466ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, Y_train)\n",
        "y_pred = classifier.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4DGSZwcHXH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(Y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBkBczwISCel",
        "colab_type": "code",
        "outputId": "1e5de0d5-4dd9-4d13-d2fb-ac6dc370cfb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "cm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[66285,    13],\n",
              "       [  253,    26]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWLefBYsT_JT",
        "colab_type": "code",
        "outputId": "ce0184a5-5be5-42a8-d336-18d2f6e64e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "metrics.accuracy_score(Y_test, y_pred, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9960046262222689"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfsN7uIqVOPk",
        "colab_type": "code",
        "outputId": "f1d69039-d6d9-4935-df7c-25cef9edd9f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "metrics.precision_score(Y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3f0f_2a8JJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = 'Logistic Regression'\n",
        "clf_lgrb = LogisticRegression(random_state=123)\n",
        "gs_params = {'C': [.01, 0.1, 1.0, 10], 'solver': ['liblinear', 'lbfgs']}\n",
        "gs_score = 'roc_auc'\n",
        "grid_search = model_selection.GridSearchCV(estimator=clf_lgrb, param_grid=gs_params, cv=5, scoring=gs_score, n_jobs=-1)\n",
        "grid_search.fit(X_train, Y_train)\n",
        "y_pred = grid_search.predict(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8WEofNtTWjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = confusion_matrix(Y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q3GZyezTqPv",
        "colab_type": "code",
        "outputId": "2837f7dd-e040-4b91-cf8c-6d39b42f26d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[66281,    17],\n",
              "       [  251,    28]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE00hlLGTtI6",
        "colab_type": "code",
        "outputId": "e41472f3-08df-4eee-a6dd-b38b38c5247a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "metrics.accuracy_score(Y_test, y_pred, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9959745858179251"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTTLRBWjVU-Z",
        "colab_type": "code",
        "outputId": "2ff96057-2645-43d3-a821-262ca77af2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "metrics.precision_score(Y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6222222222222222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK_h9BI8FtyE",
        "colab_type": "text"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKbnaiSwVjbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svclassifier = SVC(kernel='linear')\n",
        "\n",
        "svclassifier.fit(X_train, Y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzKm1kHqE86b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = confusion_matrix(Y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EQda8fVEuET",
        "colab_type": "code",
        "outputId": "8cbefb5a-f874-4178-ca05-f2077400e102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[66285,    13],\n",
              "       [  253,    26]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqbx6pN4EuO6",
        "colab_type": "code",
        "outputId": "06be8a46-0dd9-44cb-c244-c1294fb9df5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "metrics.accuracy_score(Y_test, y_pred, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9960046262222689"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0u9fH6nFA13",
        "colab_type": "code",
        "outputId": "520a4aa1-83e0-458d-cb4f-9db1041fa87d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "metrics.precision_score(Y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQHtErQUDaF-",
        "colab_type": "text"
      },
      "source": [
        "NN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNA7RFoeDb5g",
        "colab_type": "code",
        "outputId": "33c17d2f-d992-4470-a061-3cebb4e46fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
        "\n",
        "NN.fit(X_train, Y_train)\n",
        "\n",
        "y_pred = NN.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi5u5wWxDneh",
        "colab_type": "code",
        "outputId": "61aad520-e8a2-4ee7-8314-6c03cc57a695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "confusion_matrix(Y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[66251,    47],\n",
              "       [   43,   236]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VZ5d207Dx4K",
        "colab_type": "code",
        "outputId": "cf9eea74-6e42-44fa-b064-4f9878f8d503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "metrics.precision_score(Y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.833922261484099"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDOq0DxZfUlr",
        "colab_type": "code",
        "outputId": "5d3582ad-5929-40e9-96f0-be76b60456b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sensor_00</th>\n",
              "      <th>sensor_01</th>\n",
              "      <th>sensor_02</th>\n",
              "      <th>sensor_03</th>\n",
              "      <th>sensor_04</th>\n",
              "      <th>sensor_05</th>\n",
              "      <th>sensor_06</th>\n",
              "      <th>sensor_07</th>\n",
              "      <th>sensor_08</th>\n",
              "      <th>sensor_09</th>\n",
              "      <th>sensor_10</th>\n",
              "      <th>sensor_11</th>\n",
              "      <th>sensor_12</th>\n",
              "      <th>sensor_13</th>\n",
              "      <th>sensor_14</th>\n",
              "      <th>sensor_16</th>\n",
              "      <th>sensor_17</th>\n",
              "      <th>sensor_18</th>\n",
              "      <th>sensor_19</th>\n",
              "      <th>sensor_20</th>\n",
              "      <th>sensor_21</th>\n",
              "      <th>sensor_22</th>\n",
              "      <th>sensor_23</th>\n",
              "      <th>sensor_24</th>\n",
              "      <th>sensor_25</th>\n",
              "      <th>sensor_26</th>\n",
              "      <th>sensor_27</th>\n",
              "      <th>sensor_28</th>\n",
              "      <th>sensor_29</th>\n",
              "      <th>sensor_30</th>\n",
              "      <th>sensor_31</th>\n",
              "      <th>sensor_32</th>\n",
              "      <th>sensor_33</th>\n",
              "      <th>sensor_34</th>\n",
              "      <th>sensor_35</th>\n",
              "      <th>sensor_36</th>\n",
              "      <th>sensor_37</th>\n",
              "      <th>sensor_38</th>\n",
              "      <th>sensor_39</th>\n",
              "      <th>sensor_40</th>\n",
              "      <th>sensor_41</th>\n",
              "      <th>sensor_42</th>\n",
              "      <th>sensor_43</th>\n",
              "      <th>sensor_44</th>\n",
              "      <th>sensor_45</th>\n",
              "      <th>sensor_46</th>\n",
              "      <th>sensor_47</th>\n",
              "      <th>sensor_48</th>\n",
              "      <th>sensor_49</th>\n",
              "      <th>sensor_50</th>\n",
              "      <th>sensor_51</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.444734</td>\n",
              "      <td>47.352430</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397570</td>\n",
              "      <td>638.888900</td>\n",
              "      <td>73.54598</td>\n",
              "      <td>13.32465</td>\n",
              "      <td>16.03733</td>\n",
              "      <td>15.61777</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>37.86777</td>\n",
              "      <td>48.17723</td>\n",
              "      <td>32.08894</td>\n",
              "      <td>1.708474</td>\n",
              "      <td>420.8480</td>\n",
              "      <td>462.7798</td>\n",
              "      <td>459.6364</td>\n",
              "      <td>2.500062</td>\n",
              "      <td>666.2234</td>\n",
              "      <td>399.9418</td>\n",
              "      <td>880.4237</td>\n",
              "      <td>501.3617</td>\n",
              "      <td>982.7342</td>\n",
              "      <td>631.1326</td>\n",
              "      <td>740.8031</td>\n",
              "      <td>849.8997</td>\n",
              "      <td>454.2390</td>\n",
              "      <td>778.5734</td>\n",
              "      <td>715.6266</td>\n",
              "      <td>661.5740</td>\n",
              "      <td>721.8750</td>\n",
              "      <td>694.7721</td>\n",
              "      <td>441.2635</td>\n",
              "      <td>169.9820</td>\n",
              "      <td>343.1955</td>\n",
              "      <td>200.9694</td>\n",
              "      <td>93.90508</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>31.25000</td>\n",
              "      <td>69.53125</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.770830</td>\n",
              "      <td>41.66666</td>\n",
              "      <td>39.351852</td>\n",
              "      <td>65.39352</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194443</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>241.3194</td>\n",
              "      <td>203.7037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.460474</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.16840</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>628.125000</td>\n",
              "      <td>76.98898</td>\n",
              "      <td>13.31742</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>38.57977</td>\n",
              "      <td>48.65607</td>\n",
              "      <td>31.67221</td>\n",
              "      <td>1.579427</td>\n",
              "      <td>420.7494</td>\n",
              "      <td>462.8980</td>\n",
              "      <td>460.8858</td>\n",
              "      <td>2.509521</td>\n",
              "      <td>666.0114</td>\n",
              "      <td>399.1046</td>\n",
              "      <td>878.8917</td>\n",
              "      <td>499.0430</td>\n",
              "      <td>977.7520</td>\n",
              "      <td>625.4076</td>\n",
              "      <td>739.2722</td>\n",
              "      <td>847.7579</td>\n",
              "      <td>474.8731</td>\n",
              "      <td>779.5091</td>\n",
              "      <td>690.4011</td>\n",
              "      <td>686.1111</td>\n",
              "      <td>754.6875</td>\n",
              "      <td>683.3831</td>\n",
              "      <td>446.2493</td>\n",
              "      <td>166.4987</td>\n",
              "      <td>343.9586</td>\n",
              "      <td>193.1689</td>\n",
              "      <td>101.04060</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>72.13541</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>40.88541</td>\n",
              "      <td>39.062500</td>\n",
              "      <td>64.81481</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>66.84028</td>\n",
              "      <td>240.4514</td>\n",
              "      <td>203.1250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.445718</td>\n",
              "      <td>47.135410</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>636.458300</td>\n",
              "      <td>76.58897</td>\n",
              "      <td>13.35359</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>39.48939</td>\n",
              "      <td>49.06298</td>\n",
              "      <td>31.95202</td>\n",
              "      <td>1.683831</td>\n",
              "      <td>419.8926</td>\n",
              "      <td>461.4906</td>\n",
              "      <td>468.2206</td>\n",
              "      <td>2.604785</td>\n",
              "      <td>663.2111</td>\n",
              "      <td>400.5426</td>\n",
              "      <td>882.5874</td>\n",
              "      <td>498.5383</td>\n",
              "      <td>979.5755</td>\n",
              "      <td>627.1830</td>\n",
              "      <td>737.6033</td>\n",
              "      <td>846.9182</td>\n",
              "      <td>408.8159</td>\n",
              "      <td>785.2307</td>\n",
              "      <td>704.6937</td>\n",
              "      <td>631.4814</td>\n",
              "      <td>766.1458</td>\n",
              "      <td>702.4431</td>\n",
              "      <td>433.9081</td>\n",
              "      <td>164.7498</td>\n",
              "      <td>339.9630</td>\n",
              "      <td>193.8770</td>\n",
              "      <td>101.70380</td>\n",
              "      <td>42.70833</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>76.82291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>65.10416</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>158.2755</td>\n",
              "      <td>66.55093</td>\n",
              "      <td>242.1875</td>\n",
              "      <td>201.3889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166436</th>\n",
              "      <td>2.313889</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>71.464119</td>\n",
              "      <td>73.00980</td>\n",
              "      <td>14.36632</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.12587</td>\n",
              "      <td>38.34975</td>\n",
              "      <td>49.88500</td>\n",
              "      <td>31.94489</td>\n",
              "      <td>12.285810</td>\n",
              "      <td>421.4294</td>\n",
              "      <td>458.9118</td>\n",
              "      <td>456.3463</td>\n",
              "      <td>2.459503</td>\n",
              "      <td>666.8698</td>\n",
              "      <td>398.0684</td>\n",
              "      <td>879.7014</td>\n",
              "      <td>536.2166</td>\n",
              "      <td>1091.1240</td>\n",
              "      <td>629.8835</td>\n",
              "      <td>740.9833</td>\n",
              "      <td>981.7558</td>\n",
              "      <td>497.2223</td>\n",
              "      <td>995.5433</td>\n",
              "      <td>563.5882</td>\n",
              "      <td>676.8519</td>\n",
              "      <td>949.9999</td>\n",
              "      <td>990.9057</td>\n",
              "      <td>564.7968</td>\n",
              "      <td>342.3690</td>\n",
              "      <td>498.5037</td>\n",
              "      <td>804.9069</td>\n",
              "      <td>55.82830</td>\n",
              "      <td>53.38541</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>93.48958</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>45.138890</td>\n",
              "      <td>252.3148</td>\n",
              "      <td>69.15509</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>198.2060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166437</th>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923611</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>72.47576</td>\n",
              "      <td>14.33015</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.76968</td>\n",
              "      <td>15.16204</td>\n",
              "      <td>38.58192</td>\n",
              "      <td>49.96759</td>\n",
              "      <td>31.46310</td>\n",
              "      <td>12.213550</td>\n",
              "      <td>420.6030</td>\n",
              "      <td>460.6981</td>\n",
              "      <td>467.5996</td>\n",
              "      <td>2.603880</td>\n",
              "      <td>665.2674</td>\n",
              "      <td>399.1180</td>\n",
              "      <td>879.2195</td>\n",
              "      <td>534.4346</td>\n",
              "      <td>1089.8550</td>\n",
              "      <td>628.6547</td>\n",
              "      <td>738.9200</td>\n",
              "      <td>979.7875</td>\n",
              "      <td>504.8127</td>\n",
              "      <td>1004.9950</td>\n",
              "      <td>569.5920</td>\n",
              "      <td>719.9074</td>\n",
              "      <td>934.3749</td>\n",
              "      <td>982.0567</td>\n",
              "      <td>559.5000</td>\n",
              "      <td>345.2228</td>\n",
              "      <td>500.3699</td>\n",
              "      <td>804.2908</td>\n",
              "      <td>49.17398</td>\n",
              "      <td>52.60416</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>95.57291</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>253.1829</td>\n",
              "      <td>71.46991</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>202.8356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166438</th>\n",
              "      <td>2.322743</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>70.48889</td>\n",
              "      <td>14.46036</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.73351</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>38.87765</td>\n",
              "      <td>49.75908</td>\n",
              "      <td>31.78839</td>\n",
              "      <td>12.180410</td>\n",
              "      <td>419.6096</td>\n",
              "      <td>462.2329</td>\n",
              "      <td>466.0083</td>\n",
              "      <td>2.577086</td>\n",
              "      <td>667.1351</td>\n",
              "      <td>398.3854</td>\n",
              "      <td>881.7244</td>\n",
              "      <td>536.0204</td>\n",
              "      <td>1095.5440</td>\n",
              "      <td>630.5479</td>\n",
              "      <td>743.5366</td>\n",
              "      <td>981.5860</td>\n",
              "      <td>488.0563</td>\n",
              "      <td>1009.6940</td>\n",
              "      <td>559.3523</td>\n",
              "      <td>678.2407</td>\n",
              "      <td>947.9166</td>\n",
              "      <td>1016.9990</td>\n",
              "      <td>558.1667</td>\n",
              "      <td>344.4498</td>\n",
              "      <td>490.6038</td>\n",
              "      <td>796.3863</td>\n",
              "      <td>39.34733</td>\n",
              "      <td>51.82291</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>100.52080</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>49.73958</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>52.08333</td>\n",
              "      <td>61.34259</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>249.1319</td>\n",
              "      <td>72.33796</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>206.8866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166439</th>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.789930</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>68.74365</td>\n",
              "      <td>14.43866</td>\n",
              "      <td>16.07350</td>\n",
              "      <td>15.53096</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>38.21849</td>\n",
              "      <td>49.73039</td>\n",
              "      <td>32.32672</td>\n",
              "      <td>12.209670</td>\n",
              "      <td>420.0334</td>\n",
              "      <td>460.0883</td>\n",
              "      <td>460.4448</td>\n",
              "      <td>2.509833</td>\n",
              "      <td>664.8566</td>\n",
              "      <td>399.6012</td>\n",
              "      <td>880.7674</td>\n",
              "      <td>535.4742</td>\n",
              "      <td>1092.3240</td>\n",
              "      <td>628.6292</td>\n",
              "      <td>743.1452</td>\n",
              "      <td>982.4372</td>\n",
              "      <td>511.7988</td>\n",
              "      <td>1025.1100</td>\n",
              "      <td>559.1011</td>\n",
              "      <td>743.5185</td>\n",
              "      <td>960.4166</td>\n",
              "      <td>998.0515</td>\n",
              "      <td>551.9908</td>\n",
              "      <td>339.3840</td>\n",
              "      <td>492.3628</td>\n",
              "      <td>796.2476</td>\n",
              "      <td>53.73619</td>\n",
              "      <td>51.04166</td>\n",
              "      <td>40.62500</td>\n",
              "      <td>104.68750</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>50.52083</td>\n",
              "      <td>52.662040</td>\n",
              "      <td>53.24074</td>\n",
              "      <td>63.36805</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>244.5023</td>\n",
              "      <td>72.62731</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>209.7801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166440</th>\n",
              "      <td>2.318808</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>420.503448</td>\n",
              "      <td>72.52040</td>\n",
              "      <td>14.18547</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>36.71748</td>\n",
              "      <td>50.04619</td>\n",
              "      <td>30.74288</td>\n",
              "      <td>12.217860</td>\n",
              "      <td>420.8600</td>\n",
              "      <td>463.5181</td>\n",
              "      <td>455.2702</td>\n",
              "      <td>2.468381</td>\n",
              "      <td>665.9402</td>\n",
              "      <td>399.0930</td>\n",
              "      <td>880.8410</td>\n",
              "      <td>531.8727</td>\n",
              "      <td>1093.5790</td>\n",
              "      <td>626.4651</td>\n",
              "      <td>740.6738</td>\n",
              "      <td>981.4561</td>\n",
              "      <td>490.7728</td>\n",
              "      <td>1023.1660</td>\n",
              "      <td>551.2756</td>\n",
              "      <td>703.7037</td>\n",
              "      <td>971.8749</td>\n",
              "      <td>1013.3760</td>\n",
              "      <td>546.6656</td>\n",
              "      <td>363.0320</td>\n",
              "      <td>522.5594</td>\n",
              "      <td>812.8298</td>\n",
              "      <td>44.77601</td>\n",
              "      <td>50.00000</td>\n",
              "      <td>39.06250</td>\n",
              "      <td>102.86460</td>\n",
              "      <td>35.416660</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>51.30208</td>\n",
              "      <td>52.102000</td>\n",
              "      <td>52.66204</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>43.981480</td>\n",
              "      <td>230.3241</td>\n",
              "      <td>69.73380</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>205.7292</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166441 rows × 51 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sensor_00  sensor_01  sensor_02  ...  sensor_49  sensor_50  sensor_51\n",
              "0        2.465394  47.092010   53.21180  ...   67.70834   243.0556   201.3889\n",
              "1        2.465394  47.092010   53.21180  ...   67.70834   243.0556   201.3889\n",
              "2        2.444734  47.352430   53.21180  ...   67.12963   241.3194   203.7037\n",
              "3        2.460474  47.092010   53.16840  ...   66.84028   240.4514   203.1250\n",
              "4        2.445718  47.135410   53.21180  ...   66.55093   242.1875   201.3889\n",
              "...           ...        ...        ...  ...        ...        ...        ...\n",
              "166436   2.313889  45.833330   53.03819  ...   69.15509  1000.0000   198.2060\n",
              "166437   2.315856  45.833332   53.03819  ...   71.46991  1000.0000   202.8356\n",
              "166438   2.322743  45.833330   52.99479  ...   72.33796  1000.0000   206.8866\n",
              "166439   2.315856  45.789930   53.03819  ...   72.62731  1000.0000   209.7801\n",
              "166440   2.318808  45.833332   52.99479  ...   69.73380  1000.0000   205.7292\n",
              "\n",
              "[166441 rows x 51 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGJkNrThgXuS",
        "colab_type": "code",
        "outputId": "62aebe7a-e745-442f-981a-564d1271b029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "Y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         0\n",
              "1         0\n",
              "2         0\n",
              "3         0\n",
              "4         0\n",
              "         ..\n",
              "166436    1\n",
              "166437    1\n",
              "166438    1\n",
              "166439    1\n",
              "166440    1\n",
              "Name: lab_bin, Length: 166441, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTcUS0cMfWX3",
        "colab_type": "text"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-dmh49Dgd6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = np.column_stack((X, Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnfDQsiOfYfE",
        "colab_type": "code",
        "outputId": "0bbb38c0-255a-42f1-b79a-ee20a413d340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# multivariate data preparation\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "\n",
        "# split a multivariate sequence into samples\n",
        "def split_sequences(sequences, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequences)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the dataset\n",
        "\t\tif end_ix > len(sequences):\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)\n",
        "\n",
        "# define input sequence\n",
        "#print(dataset)\n",
        "# choose a number of time steps\n",
        "n_steps = 1\n",
        "\n",
        "# convert into input/output\n",
        "\n",
        "x, y = split_sequences(dataset, n_steps)\n",
        "#X, y = split_sequences(, n_steps)\n",
        "print(x.shape, y.shape)\n",
        "# summarize the data\n",
        "# for i in range(len(X)):\n",
        "# \tprint(X[i], y[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(166441, 1, 51) (166441,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwGEQVRmhobp",
        "colab_type": "code",
        "outputId": "529cb7ac-5e6e-46c4-ffda-0f1b9b1e685a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random import seed\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size = 0.6, random_state = seed(20))\n",
        "print(X_train)\n",
        "print(\"=================================\")\n",
        "print(Y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[8.50403765e-02 3.95833300e+01 3.92361100e+01 ... 3.29861107e+01\n",
            "   3.38541679e+01 1.00000000e+03]]\n",
            "\n",
            " [[2.39259300e+00 4.51388900e+01 5.02170100e+01 ... 4.83217600e+01\n",
            "   1.95601900e+02 1.78530100e+02]]\n",
            "\n",
            " [[6.87890326e-02 3.41145821e+01 3.74131927e+01 ... 2.66203700e+01\n",
            "   2.77777786e+01 1.00000000e+03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[2.35225700e+00 4.35763900e+01 4.76128500e+01 ... 4.48495400e+01\n",
            "   1.78240700e+02 1.76215300e+02]]\n",
            "\n",
            " [[2.44079900e+00 4.67447900e+01 5.28211784e+01 ... 8.56481500e+01\n",
            "   2.92245400e+02 2.44213000e+02]]\n",
            "\n",
            " [[2.45162000e+00 4.96527700e+01 5.32552100e+01 ... 4.77430573e+01\n",
            "   1.74768500e+02 1.75057900e+02]]]\n",
            "=================================\n",
            "[0. 0. 0. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iiVksr6hDfb",
        "colab_type": "code",
        "outputId": "638759d3-c482-4c4d-f452-8943da6745d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from numpy import array\n",
        "from numpy import hstack\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "\n",
        "n_features = X_train.shape[2]\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='tanh', return_sequences=True, input_shape=(n_steps, n_features)))\n",
        "model.add(LSTM(100, activation='tanh'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='mse',\n",
        "              metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X_train, Y_train, epochs=10, verbose=1)\n",
        "# demonstrate prediction\n",
        "#yhat = model.predict(X_test, verbose=0)\n",
        "#print(yhat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "99864/99864 [==============================] - 33s 333us/step - loss: 0.0049 - acc: 0.9958\n",
            "Epoch 2/10\n",
            "99864/99864 [==============================] - 24s 243us/step - loss: 0.0040 - acc: 0.9958\n",
            "Epoch 3/10\n",
            "99864/99864 [==============================] - 24s 244us/step - loss: 0.0041 - acc: 0.9958\n",
            "Epoch 4/10\n",
            "99864/99864 [==============================] - 25s 249us/step - loss: 0.0041 - acc: 0.9958\n",
            "Epoch 5/10\n",
            "99864/99864 [==============================] - 24s 242us/step - loss: 0.0040 - acc: 0.9959\n",
            "Epoch 6/10\n",
            "99864/99864 [==============================] - 24s 244us/step - loss: 0.0040 - acc: 0.9959\n",
            "Epoch 7/10\n",
            "99864/99864 [==============================] - 24s 243us/step - loss: 0.0040 - acc: 0.9959\n",
            "Epoch 8/10\n",
            "99864/99864 [==============================] - 24s 241us/step - loss: 0.0040 - acc: 0.9959\n",
            "Epoch 9/10\n",
            "99864/99864 [==============================] - 24s 243us/step - loss: 0.0040 - acc: 0.9959\n",
            "Epoch 10/10\n",
            "99864/99864 [==============================] - 24s 241us/step - loss: 0.0040 - acc: 0.9959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff9b6f296d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DxKhtusrAVR",
        "colab_type": "code",
        "outputId": "fa77b2b5-62e5-46b3-a05c-8ac4fdbc10a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "ypred = model.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ad3a51e572ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V648uffX0jGW",
        "colab_type": "code",
        "outputId": "edffb6b8-03cd-416e-9545-ecb6da008db2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "y_pred1 = []\n",
        "for i in range(len(Y_test)):\n",
        "    y_pred1.append(ypred[i][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9d91b2825269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my_pred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0my_pred1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Y_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTY9hb2ZrAoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics.precision_score(y_pred1, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvScpmWy198i",
        "colab_type": "text"
      },
      "source": [
        "LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwfNhMJA2AC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_features = len(X_train[0])\n",
        "n_steps = 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDEpjDpFmkGw",
        "colab_type": "code",
        "outputId": "d6e40556-5caa-4e08-cfdc-3a5afc06f42d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "Y_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130513    0\n",
              "12164     0\n",
              "131371    0\n",
              "135658    0\n",
              "32111     0\n",
              "         ..\n",
              "124308    0\n",
              "31962     0\n",
              "23775     0\n",
              "37135     0\n",
              "92634     0\n",
              "Name: lab_bin, Length: 99864, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu9YCmU878LO",
        "colab_type": "code",
        "outputId": "ba9f0859-c531-4920-81cd-6abf86a294c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99864, 51)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrL0RnG82EqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train= X_train.reshape(len(X_train), n_steps, n_features)\n",
        "X_test=X_test.reshape(len(X_test), n_steps, n_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8rFzm672FCW",
        "colab_type": "code",
        "outputId": "639c6ccd-34ba-43ae-f046-78a5369a0e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99864, 1, 51)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cNW0WSQkXCP",
        "colab_type": "code",
        "outputId": "f82a2b70-d7d4-4537-c077-23c9e70e4690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "Y_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130513    0\n",
              "12164     0\n",
              "131371    0\n",
              "135658    0\n",
              "32111     0\n",
              "         ..\n",
              "124308    0\n",
              "31962     0\n",
              "23775     0\n",
              "37135     0\n",
              "92634     0\n",
              "Name: lab_bin, Length: 99864, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j_4qwZ4nEtq",
        "colab_type": "code",
        "outputId": "a558a333-2efb-41a8-8ae7-008b27274821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "# model.add(LSTM(50, activation='relu', input_shape=(1, n_features)))\n",
        "# model.add(Dense(1))\n",
        "# model.compile(optimizer='adam', loss='mse')\n",
        "model.add(LSTM(10, dropout=0.2, recurrent_dropout=0.2,input_shape=(1, n_features)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, epochs=10, verbose=1)\n",
        "ypred = model.predict(X_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "99864/99864 [==============================] - 33s 335us/step - loss: 0.0557 - acc: 0.9924\n",
            "Epoch 2/10\n",
            "99864/99864 [==============================] - 25s 253us/step - loss: 0.0197 - acc: 0.9959\n",
            "Epoch 3/10\n",
            "99864/99864 [==============================] - 25s 253us/step - loss: 0.0163 - acc: 0.9959\n",
            "Epoch 4/10\n",
            "99864/99864 [==============================] - 25s 252us/step - loss: 0.0147 - acc: 0.9959\n",
            "Epoch 5/10\n",
            "31552/99864 [========>.....................] - ETA: 17s - loss: 0.0135 - acc: 0.9963"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfOLkA8zaPap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# history_lstm = model_lstm.fit(X_train, Y_train, epochs=10, batch_size=64, validation_data=(X_test, Y_test),  shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KuBYX_5t1qY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3qHFaTlbbW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ypred = model_lstm.predict(X_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_HLqkPBkPoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ypred = model.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au4Gw87sbqFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics.precision_score(Y_test, ypred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13j7e7-7Ev16",
        "colab_type": "text"
      },
      "source": [
        "GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDjLC2hnkOxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "# model.add(LSTM(50, activation='relu', input_shape=(1, n_features)))\n",
        "# model.add(Dense(1))\n",
        "# model.compile(optimizer='adam', loss='mse')\n",
        "model.add(GRU(10, dropout=0.2, recurrent_dropout=0.2,input_shape=(1, n_features)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, epochs=10, verbose=1)\n",
        "ypred = model.predict(X_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1EUXyTlAV-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av9vRznrBOP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ypred = model.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbH2CcgRAXEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics.precision_score(Y_test, ypred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpbUSawSaQRA",
        "colab_type": "text"
      },
      "source": [
        "CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW9f1KwCaU1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NcEEMtqzKAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cm = confusion_matrix(y_test, y_pred)\n",
        "# print(cm)\n",
        "# print(metrics.accuracy_score(y_test, y_pred, normalize=True))\n",
        "# print(calcp(cm))\n",
        "# print(calcr(cm))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtPKulgiD0Ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnpUt55U_XPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " dataset = train_df.iloc[:, 1:53]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovphGO9PAuud",
        "colab_type": "code",
        "outputId": "4a8695ed-7729-49de-c125-87e3eabd634e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sensor_00</th>\n",
              "      <th>sensor_01</th>\n",
              "      <th>sensor_02</th>\n",
              "      <th>sensor_03</th>\n",
              "      <th>sensor_04</th>\n",
              "      <th>sensor_05</th>\n",
              "      <th>sensor_06</th>\n",
              "      <th>sensor_07</th>\n",
              "      <th>sensor_08</th>\n",
              "      <th>sensor_09</th>\n",
              "      <th>sensor_10</th>\n",
              "      <th>sensor_11</th>\n",
              "      <th>sensor_12</th>\n",
              "      <th>sensor_13</th>\n",
              "      <th>sensor_14</th>\n",
              "      <th>sensor_16</th>\n",
              "      <th>sensor_17</th>\n",
              "      <th>sensor_18</th>\n",
              "      <th>sensor_19</th>\n",
              "      <th>sensor_20</th>\n",
              "      <th>sensor_21</th>\n",
              "      <th>sensor_22</th>\n",
              "      <th>sensor_23</th>\n",
              "      <th>sensor_24</th>\n",
              "      <th>sensor_25</th>\n",
              "      <th>sensor_26</th>\n",
              "      <th>sensor_27</th>\n",
              "      <th>sensor_28</th>\n",
              "      <th>sensor_29</th>\n",
              "      <th>sensor_30</th>\n",
              "      <th>sensor_31</th>\n",
              "      <th>sensor_32</th>\n",
              "      <th>sensor_33</th>\n",
              "      <th>sensor_34</th>\n",
              "      <th>sensor_35</th>\n",
              "      <th>sensor_36</th>\n",
              "      <th>sensor_37</th>\n",
              "      <th>sensor_38</th>\n",
              "      <th>sensor_39</th>\n",
              "      <th>sensor_40</th>\n",
              "      <th>sensor_41</th>\n",
              "      <th>sensor_42</th>\n",
              "      <th>sensor_43</th>\n",
              "      <th>sensor_44</th>\n",
              "      <th>sensor_45</th>\n",
              "      <th>sensor_46</th>\n",
              "      <th>sensor_47</th>\n",
              "      <th>sensor_48</th>\n",
              "      <th>sensor_49</th>\n",
              "      <th>sensor_50</th>\n",
              "      <th>sensor_51</th>\n",
              "      <th>RUL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.444734</td>\n",
              "      <td>47.352430</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397570</td>\n",
              "      <td>638.888900</td>\n",
              "      <td>73.54598</td>\n",
              "      <td>13.32465</td>\n",
              "      <td>16.03733</td>\n",
              "      <td>15.61777</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>37.86777</td>\n",
              "      <td>48.17723</td>\n",
              "      <td>32.08894</td>\n",
              "      <td>1.708474</td>\n",
              "      <td>420.8480</td>\n",
              "      <td>462.7798</td>\n",
              "      <td>459.6364</td>\n",
              "      <td>2.500062</td>\n",
              "      <td>666.2234</td>\n",
              "      <td>399.9418</td>\n",
              "      <td>880.4237</td>\n",
              "      <td>501.3617</td>\n",
              "      <td>982.7342</td>\n",
              "      <td>631.1326</td>\n",
              "      <td>740.8031</td>\n",
              "      <td>849.8997</td>\n",
              "      <td>454.2390</td>\n",
              "      <td>778.5734</td>\n",
              "      <td>715.6266</td>\n",
              "      <td>661.5740</td>\n",
              "      <td>721.8750</td>\n",
              "      <td>694.7721</td>\n",
              "      <td>441.2635</td>\n",
              "      <td>169.9820</td>\n",
              "      <td>343.1955</td>\n",
              "      <td>200.9694</td>\n",
              "      <td>93.90508</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>31.25000</td>\n",
              "      <td>69.53125</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.770830</td>\n",
              "      <td>41.66666</td>\n",
              "      <td>39.351852</td>\n",
              "      <td>65.39352</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194443</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>241.3194</td>\n",
              "      <td>203.7037</td>\n",
              "      <td>17153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.460474</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.16840</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>628.125000</td>\n",
              "      <td>76.98898</td>\n",
              "      <td>13.31742</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>38.57977</td>\n",
              "      <td>48.65607</td>\n",
              "      <td>31.67221</td>\n",
              "      <td>1.579427</td>\n",
              "      <td>420.7494</td>\n",
              "      <td>462.8980</td>\n",
              "      <td>460.8858</td>\n",
              "      <td>2.509521</td>\n",
              "      <td>666.0114</td>\n",
              "      <td>399.1046</td>\n",
              "      <td>878.8917</td>\n",
              "      <td>499.0430</td>\n",
              "      <td>977.7520</td>\n",
              "      <td>625.4076</td>\n",
              "      <td>739.2722</td>\n",
              "      <td>847.7579</td>\n",
              "      <td>474.8731</td>\n",
              "      <td>779.5091</td>\n",
              "      <td>690.4011</td>\n",
              "      <td>686.1111</td>\n",
              "      <td>754.6875</td>\n",
              "      <td>683.3831</td>\n",
              "      <td>446.2493</td>\n",
              "      <td>166.4987</td>\n",
              "      <td>343.9586</td>\n",
              "      <td>193.1689</td>\n",
              "      <td>101.04060</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>72.13541</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>40.88541</td>\n",
              "      <td>39.062500</td>\n",
              "      <td>64.81481</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>66.84028</td>\n",
              "      <td>240.4514</td>\n",
              "      <td>203.1250</td>\n",
              "      <td>17152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.445718</td>\n",
              "      <td>47.135410</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>636.458300</td>\n",
              "      <td>76.58897</td>\n",
              "      <td>13.35359</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>39.48939</td>\n",
              "      <td>49.06298</td>\n",
              "      <td>31.95202</td>\n",
              "      <td>1.683831</td>\n",
              "      <td>419.8926</td>\n",
              "      <td>461.4906</td>\n",
              "      <td>468.2206</td>\n",
              "      <td>2.604785</td>\n",
              "      <td>663.2111</td>\n",
              "      <td>400.5426</td>\n",
              "      <td>882.5874</td>\n",
              "      <td>498.5383</td>\n",
              "      <td>979.5755</td>\n",
              "      <td>627.1830</td>\n",
              "      <td>737.6033</td>\n",
              "      <td>846.9182</td>\n",
              "      <td>408.8159</td>\n",
              "      <td>785.2307</td>\n",
              "      <td>704.6937</td>\n",
              "      <td>631.4814</td>\n",
              "      <td>766.1458</td>\n",
              "      <td>702.4431</td>\n",
              "      <td>433.9081</td>\n",
              "      <td>164.7498</td>\n",
              "      <td>339.9630</td>\n",
              "      <td>193.8770</td>\n",
              "      <td>101.70380</td>\n",
              "      <td>42.70833</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>76.82291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>65.10416</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>158.2755</td>\n",
              "      <td>66.55093</td>\n",
              "      <td>242.1875</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166436</th>\n",
              "      <td>2.313889</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>71.464119</td>\n",
              "      <td>73.00980</td>\n",
              "      <td>14.36632</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.12587</td>\n",
              "      <td>38.34975</td>\n",
              "      <td>49.88500</td>\n",
              "      <td>31.94489</td>\n",
              "      <td>12.285810</td>\n",
              "      <td>421.4294</td>\n",
              "      <td>458.9118</td>\n",
              "      <td>456.3463</td>\n",
              "      <td>2.459503</td>\n",
              "      <td>666.8698</td>\n",
              "      <td>398.0684</td>\n",
              "      <td>879.7014</td>\n",
              "      <td>536.2166</td>\n",
              "      <td>1091.1240</td>\n",
              "      <td>629.8835</td>\n",
              "      <td>740.9833</td>\n",
              "      <td>981.7558</td>\n",
              "      <td>497.2223</td>\n",
              "      <td>995.5433</td>\n",
              "      <td>563.5882</td>\n",
              "      <td>676.8519</td>\n",
              "      <td>949.9999</td>\n",
              "      <td>990.9057</td>\n",
              "      <td>564.7968</td>\n",
              "      <td>342.3690</td>\n",
              "      <td>498.5037</td>\n",
              "      <td>804.9069</td>\n",
              "      <td>55.82830</td>\n",
              "      <td>53.38541</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>93.48958</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>45.138890</td>\n",
              "      <td>252.3148</td>\n",
              "      <td>69.15509</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>198.2060</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166437</th>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923611</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>72.47576</td>\n",
              "      <td>14.33015</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.76968</td>\n",
              "      <td>15.16204</td>\n",
              "      <td>38.58192</td>\n",
              "      <td>49.96759</td>\n",
              "      <td>31.46310</td>\n",
              "      <td>12.213550</td>\n",
              "      <td>420.6030</td>\n",
              "      <td>460.6981</td>\n",
              "      <td>467.5996</td>\n",
              "      <td>2.603880</td>\n",
              "      <td>665.2674</td>\n",
              "      <td>399.1180</td>\n",
              "      <td>879.2195</td>\n",
              "      <td>534.4346</td>\n",
              "      <td>1089.8550</td>\n",
              "      <td>628.6547</td>\n",
              "      <td>738.9200</td>\n",
              "      <td>979.7875</td>\n",
              "      <td>504.8127</td>\n",
              "      <td>1004.9950</td>\n",
              "      <td>569.5920</td>\n",
              "      <td>719.9074</td>\n",
              "      <td>934.3749</td>\n",
              "      <td>982.0567</td>\n",
              "      <td>559.5000</td>\n",
              "      <td>345.2228</td>\n",
              "      <td>500.3699</td>\n",
              "      <td>804.2908</td>\n",
              "      <td>49.17398</td>\n",
              "      <td>52.60416</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>95.57291</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>253.1829</td>\n",
              "      <td>71.46991</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>202.8356</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166438</th>\n",
              "      <td>2.322743</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>70.48889</td>\n",
              "      <td>14.46036</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.73351</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>38.87765</td>\n",
              "      <td>49.75908</td>\n",
              "      <td>31.78839</td>\n",
              "      <td>12.180410</td>\n",
              "      <td>419.6096</td>\n",
              "      <td>462.2329</td>\n",
              "      <td>466.0083</td>\n",
              "      <td>2.577086</td>\n",
              "      <td>667.1351</td>\n",
              "      <td>398.3854</td>\n",
              "      <td>881.7244</td>\n",
              "      <td>536.0204</td>\n",
              "      <td>1095.5440</td>\n",
              "      <td>630.5479</td>\n",
              "      <td>743.5366</td>\n",
              "      <td>981.5860</td>\n",
              "      <td>488.0563</td>\n",
              "      <td>1009.6940</td>\n",
              "      <td>559.3523</td>\n",
              "      <td>678.2407</td>\n",
              "      <td>947.9166</td>\n",
              "      <td>1016.9990</td>\n",
              "      <td>558.1667</td>\n",
              "      <td>344.4498</td>\n",
              "      <td>490.6038</td>\n",
              "      <td>796.3863</td>\n",
              "      <td>39.34733</td>\n",
              "      <td>51.82291</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>100.52080</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>49.73958</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>52.08333</td>\n",
              "      <td>61.34259</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>249.1319</td>\n",
              "      <td>72.33796</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>206.8866</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166439</th>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.789930</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>68.74365</td>\n",
              "      <td>14.43866</td>\n",
              "      <td>16.07350</td>\n",
              "      <td>15.53096</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>38.21849</td>\n",
              "      <td>49.73039</td>\n",
              "      <td>32.32672</td>\n",
              "      <td>12.209670</td>\n",
              "      <td>420.0334</td>\n",
              "      <td>460.0883</td>\n",
              "      <td>460.4448</td>\n",
              "      <td>2.509833</td>\n",
              "      <td>664.8566</td>\n",
              "      <td>399.6012</td>\n",
              "      <td>880.7674</td>\n",
              "      <td>535.4742</td>\n",
              "      <td>1092.3240</td>\n",
              "      <td>628.6292</td>\n",
              "      <td>743.1452</td>\n",
              "      <td>982.4372</td>\n",
              "      <td>511.7988</td>\n",
              "      <td>1025.1100</td>\n",
              "      <td>559.1011</td>\n",
              "      <td>743.5185</td>\n",
              "      <td>960.4166</td>\n",
              "      <td>998.0515</td>\n",
              "      <td>551.9908</td>\n",
              "      <td>339.3840</td>\n",
              "      <td>492.3628</td>\n",
              "      <td>796.2476</td>\n",
              "      <td>53.73619</td>\n",
              "      <td>51.04166</td>\n",
              "      <td>40.62500</td>\n",
              "      <td>104.68750</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>50.52083</td>\n",
              "      <td>52.662040</td>\n",
              "      <td>53.24074</td>\n",
              "      <td>63.36805</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>244.5023</td>\n",
              "      <td>72.62731</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>209.7801</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166440</th>\n",
              "      <td>2.318808</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>420.503448</td>\n",
              "      <td>72.52040</td>\n",
              "      <td>14.18547</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>36.71748</td>\n",
              "      <td>50.04619</td>\n",
              "      <td>30.74288</td>\n",
              "      <td>12.217860</td>\n",
              "      <td>420.8600</td>\n",
              "      <td>463.5181</td>\n",
              "      <td>455.2702</td>\n",
              "      <td>2.468381</td>\n",
              "      <td>665.9402</td>\n",
              "      <td>399.0930</td>\n",
              "      <td>880.8410</td>\n",
              "      <td>531.8727</td>\n",
              "      <td>1093.5790</td>\n",
              "      <td>626.4651</td>\n",
              "      <td>740.6738</td>\n",
              "      <td>981.4561</td>\n",
              "      <td>490.7728</td>\n",
              "      <td>1023.1660</td>\n",
              "      <td>551.2756</td>\n",
              "      <td>703.7037</td>\n",
              "      <td>971.8749</td>\n",
              "      <td>1013.3760</td>\n",
              "      <td>546.6656</td>\n",
              "      <td>363.0320</td>\n",
              "      <td>522.5594</td>\n",
              "      <td>812.8298</td>\n",
              "      <td>44.77601</td>\n",
              "      <td>50.00000</td>\n",
              "      <td>39.06250</td>\n",
              "      <td>102.86460</td>\n",
              "      <td>35.416660</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>51.30208</td>\n",
              "      <td>52.102000</td>\n",
              "      <td>52.66204</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>43.981480</td>\n",
              "      <td>230.3241</td>\n",
              "      <td>69.73380</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>205.7292</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166441 rows × 52 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sensor_00  sensor_01  sensor_02  ...  sensor_50  sensor_51    RUL\n",
              "0        2.465394  47.092010   53.21180  ...   243.0556   201.3889  17155\n",
              "1        2.465394  47.092010   53.21180  ...   243.0556   201.3889  17154\n",
              "2        2.444734  47.352430   53.21180  ...   241.3194   203.7037  17153\n",
              "3        2.460474  47.092010   53.16840  ...   240.4514   203.1250  17152\n",
              "4        2.445718  47.135410   53.21180  ...   242.1875   201.3889  17151\n",
              "...           ...        ...        ...  ...        ...        ...    ...\n",
              "166436   2.313889  45.833330   53.03819  ...  1000.0000   198.2060      4\n",
              "166437   2.315856  45.833332   53.03819  ...  1000.0000   202.8356      3\n",
              "166438   2.322743  45.833330   52.99479  ...  1000.0000   206.8866      2\n",
              "166439   2.315856  45.789930   53.03819  ...  1000.0000   209.7801      1\n",
              "166440   2.318808  45.833332   52.99479  ...  1000.0000   205.7292      0\n",
              "\n",
              "[166441 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqHNXO8O8T06",
        "colab_type": "code",
        "outputId": "f8727143-264e-491b-d8f9-98ce624c0a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# multivariate data preparation\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "\n",
        "# split a multivariate sequence into samples\n",
        "def split_sequences(sequences, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequences)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the dataset\n",
        "\t\tif end_ix > len(sequences):\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)\n",
        "\n",
        "# define input sequence\n",
        "\n",
        "dataset = dataset.to_numpy()\n",
        "print(dataset)\n",
        "# choose a number of time steps\n",
        "n_steps = 100\n",
        "# convert into input/output\n",
        "X, y = split_sequences(dataset, n_steps)\n",
        "print(X.shape, y.shape)\n",
        "# summarize the data\n",
        "# for i in range(len(X)):\n",
        "# \tprint(X[i], y[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.46539400e+00 4.70920100e+01 5.32118000e+01 ... 2.43055600e+02\n",
            "  2.01388900e+02 1.71550000e+04]\n",
            " [2.46539400e+00 4.70920100e+01 5.32118000e+01 ... 2.43055600e+02\n",
            "  2.01388900e+02 1.71540000e+04]\n",
            " [2.44473400e+00 4.73524300e+01 5.32118000e+01 ... 2.41319400e+02\n",
            "  2.03703700e+02 1.71530000e+04]\n",
            " ...\n",
            " [2.32274300e+00 4.58333300e+01 5.29947900e+01 ... 1.00000000e+03\n",
            "  2.06886600e+02 2.00000000e+00]\n",
            " [2.31585600e+00 4.57899300e+01 5.30381900e+01 ... 1.00000000e+03\n",
            "  2.09780100e+02 1.00000000e+00]\n",
            " [2.31880800e+00 4.58333321e+01 5.29947900e+01 ... 1.00000000e+03\n",
            "  2.05729200e+02 0.00000000e+00]]\n",
            "(166342, 100, 51) (166342,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry2qXwWDFXZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size = 0.6, random_state = seed(20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFYfDz5pEXeh",
        "colab_type": "code",
        "outputId": "6cb6fc64-4ab9-4746-8139-3fad24209fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_features = X.shape[2]\n",
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99805, 100, 51)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGpHtnaXCHRw",
        "colab_type": "code",
        "outputId": "8090a146-438e-4698-a51e-a73fe2d389f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCejAeGsDmwC",
        "colab_type": "code",
        "outputId": "32cef93e-a642-47d5-b03c-52dc494aaa60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model.fit(X_train, Y_train, epochs=10, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "99805/99805 [==============================] - 22s 217us/step - loss: -275575.9461 - acc: 3.0059e-05\n",
            "Epoch 2/10\n",
            "99805/99805 [==============================] - 22s 217us/step - loss: -276042.9212 - acc: 3.0059e-05\n",
            "Epoch 3/10\n",
            "99805/99805 [==============================] - 22s 224us/step - loss: -276042.9216 - acc: 3.0059e-05\n",
            "Epoch 4/10\n",
            "99805/99805 [==============================] - 22s 222us/step - loss: -276042.9216 - acc: 3.0059e-05\n",
            "Epoch 5/10\n",
            "99805/99805 [==============================] - 22s 221us/step - loss: -276042.9218 - acc: 3.0059e-05\n",
            "Epoch 6/10\n",
            "99805/99805 [==============================] - 22s 221us/step - loss: -276042.9219 - acc: 3.0059e-05\n",
            "Epoch 7/10\n",
            "99805/99805 [==============================] - 22s 216us/step - loss: -276042.9214 - acc: 3.0059e-05\n",
            "Epoch 8/10\n",
            "99805/99805 [==============================] - 21s 215us/step - loss: -276042.9213 - acc: 3.0059e-05\n",
            "Epoch 9/10\n",
            "99805/99805 [==============================] - 21s 207us/step - loss: -276042.9217 - acc: 3.0059e-05\n",
            "Epoch 10/10\n",
            "99805/99805 [==============================] - 20s 205us/step - loss: -276042.9218 - acc: 3.0059e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fee1995ccf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niNJBINdEkLQ",
        "colab_type": "code",
        "outputId": "77b2091b-97d7-41ce-9451-984bb199cb72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "yhat = model.predict(X_test, verbose=0)\n",
        "print(yhat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " ...\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0PD3zM2F9iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlODTD8GYsbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpXXJjNiGBPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'cnn_model10EPOCH.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6Zk7uP2ZKyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('cnn_model10EPOCH.sav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkHhKvWMZPSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename2 = 'TEST10EPOCH.sav'\n",
        "pickle.dump(X_test, open(filename2, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j935oP_3dMLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('TEST10EPOCH.sav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4avdrhcZPra4",
        "colab_type": "text"
      },
      "source": [
        "# attention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Czr48_iQseV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
        "                           input_dim=None, output_dim=None, timesteps=None):\n",
        "    '''Apply y.w + b for every temporal slice y of x.\n",
        "    '''\n",
        "    if not input_dim:\n",
        "        # won't work with TensorFlow\n",
        "        input_dim = K.shape(x)[2]\n",
        "    if not timesteps:\n",
        "        # won't work with TensorFlow\n",
        "        timesteps = K.shape(x)[1]\n",
        "    if not output_dim:\n",
        "        # won't work with TensorFlow\n",
        "        output_dim = K.shape(w)[1]\n",
        "\n",
        "    if dropout:\n",
        "        # apply the same dropout pattern at every timestep\n",
        "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
        "        dropout_matrix = K.dropout(ones, dropout)\n",
        "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
        "        x *= expanded_dropout_matrix\n",
        "\n",
        "    # collapse time dimension and batch dimension together\n",
        "    x = K.reshape(x, (-1, input_dim))\n",
        "\n",
        "    x = K.dot(x, w)\n",
        "    if b:\n",
        "        x = x + b\n",
        "    # reshape to 3D tensor\n",
        "    x = K.reshape(x, (-1, timesteps, output_dim))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7sG_dT3QTrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import regularizers, constraints, initializers, activations\n",
        "from keras.layers.recurrent import Recurrent\n",
        "from keras.engine import InputSpec\n",
        "\n",
        "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
        "\n",
        "class AttentionDecoder(Recurrent):\n",
        "\n",
        "    def __init__(self, units, output_dim,\n",
        "                 activation='tanh',\n",
        "                 return_probabilities=False,\n",
        "                 name='AttentionDecoder',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
        "        encoder and outputs the decoded states\n",
        "        :param units: dimension of the hidden state and the attention matrices\n",
        "        :param output_dim: the number of labels in the output space\n",
        "\n",
        "        references:\n",
        "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
        "            \"Neural machine translation by jointly learning to align and translate.\"\n",
        "            arXiv preprint arXiv:1409.0473 (2014).\n",
        "        \"\"\"\n",
        "        self.units = units\n",
        "        self.output_dim = output_dim\n",
        "        self.return_probabilities = return_probabilities\n",
        "        self.activation = activations.get(activation)\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        super(AttentionDecoder, self).__init__(**kwargs)\n",
        "        self.name = name\n",
        "        self.return_sequences = True  # must return sequences\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
        "          for model details that correspond to the matrices here.\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
        "\n",
        "        if self.stateful:\n",
        "            super(AttentionDecoder, self).reset_states()\n",
        "\n",
        "        self.states = [None, None]  # y, s\n",
        "\n",
        "        \"\"\"\n",
        "            Matrices for creating the context vector\n",
        "        \"\"\"\n",
        "\n",
        "        self.V_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='V_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='W_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='U_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.b_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='b_a',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the r (reset) gate\n",
        "        \"\"\"\n",
        "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_r = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_r',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "\n",
        "        \"\"\"\n",
        "            Matrices for the z (update) gate\n",
        "        \"\"\"\n",
        "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_z = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_z',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the proposal\n",
        "        \"\"\"\n",
        "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_p = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_p',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for making the final prediction vector\n",
        "        \"\"\"\n",
        "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
        "                                   name='C_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
        "                                   name='U_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
        "                                   name='W_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
        "                                   name='b_o',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "\n",
        "        # For creating the initial state:\n",
        "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='W_s',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "\n",
        "        self.input_spec = [\n",
        "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, x):\n",
        "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
        "        self.x_seq = x\n",
        "\n",
        "        # apply the a dense layer over the time dimension of the sequence\n",
        "        # do it here because it doesn't depend on any previous steps\n",
        "        # thefore we can save computation time:\n",
        "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
        "                                             input_dim=self.input_dim,\n",
        "                                             timesteps=self.timesteps,\n",
        "                                             output_dim=self.units)\n",
        "\n",
        "        return super(AttentionDecoder, self).call(x)\n",
        "\n",
        "    def get_initial_state(self, inputs):\n",
        "        # apply the matrix on the first time step to get the initial s0.\n",
        "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
        "\n",
        "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
        "        # output_dim)\n",
        "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
        "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
        "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
        "        y0 = K.tile(y0, [1, self.output_dim])\n",
        "\n",
        "        return [y0, s0]\n",
        "\n",
        "    def step(self, x, states):\n",
        "\n",
        "        ytm, stm = states\n",
        "\n",
        "        # repeat the hidden state to the length of the sequence\n",
        "        _stm = K.repeat(stm, self.timesteps)\n",
        "\n",
        "        # now multiplty the weight matrix with the repeated hidden state\n",
        "        _Wxstm = K.dot(_stm, self.W_a)\n",
        "\n",
        "        # calculate the attention probabilities\n",
        "        # this relates how much other timesteps contributed to this one.\n",
        "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
        "                   K.expand_dims(self.V_a))\n",
        "        at = K.exp(et)\n",
        "        at_sum = K.sum(at, axis=1)\n",
        "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
        "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
        "\n",
        "        # calculate the context vector\n",
        "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
        "        # ~~~> calculate new hidden state\n",
        "        # first calculate the \"r\" gate:\n",
        "\n",
        "        rt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_r)\n",
        "            + K.dot(stm, self.U_r)\n",
        "            + K.dot(context, self.C_r)\n",
        "            + self.b_r)\n",
        "\n",
        "        # now calculate the \"z\" gate\n",
        "        zt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_z)\n",
        "            + K.dot(stm, self.U_z)\n",
        "            + K.dot(context, self.C_z)\n",
        "            + self.b_z)\n",
        "\n",
        "        # calculate the proposal hidden state:\n",
        "        s_tp = activations.tanh(\n",
        "            K.dot(ytm, self.W_p)\n",
        "            + K.dot((rt * stm), self.U_p)\n",
        "            + K.dot(context, self.C_p)\n",
        "            + self.b_p)\n",
        "\n",
        "        # new hidden state:\n",
        "        st = (1-zt)*stm + zt * s_tp\n",
        "\n",
        "        yt = activations.softmax(\n",
        "            K.dot(ytm, self.W_o)\n",
        "            + K.dot(stm, self.U_o)\n",
        "            + K.dot(context, self.C_o)\n",
        "            + self.b_o)\n",
        "\n",
        "        if self.return_probabilities:\n",
        "            return at, [yt, st]\n",
        "        else:\n",
        "            return yt, [yt, st]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "            For Keras internal compatability checking\n",
        "        \"\"\"\n",
        "        if self.return_probabilities:\n",
        "            return (None, self.timesteps, self.timesteps)\n",
        "        else:\n",
        "            return (None, self.timesteps, self.output_dim)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "            For rebuilding models on load time.\n",
        "        \"\"\"\n",
        "        config = {\n",
        "            'output_dim': self.output_dim,\n",
        "            'units': self.units,\n",
        "            'return_probabilities': self.return_probabilities\n",
        "        }\n",
        "        base_config = super(AttentionDecoder, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Uwcz9lxP5q1",
        "colab_type": "code",
        "outputId": "acf2cad8-7ec2-410a-bd62-f6fc082f615d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from random import randint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import array_equal\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "#from attention_decoder import AttentionDecoder\n",
        "\n",
        "# generate a sequence of random integers\n",
        "def generate_sequence(length, n_unique):\n",
        "\treturn [randint(0, n_unique-1) for _ in range(length)]\n",
        "\n",
        "# one hot encode sequence\n",
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)\n",
        "\n",
        "# decode a one hot encoded string\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n",
        "\n",
        "# prepare data for the LSTM\n",
        "def get_pair(n_in, n_out, cardinality):\n",
        "\t# generate random sequence\n",
        "\tsequence_in = generate_sequence(n_in, cardinality)\n",
        "\tsequence_out = sequence_in[:n_out] + [0 for _ in range(n_in-n_out)]\n",
        "\t# one hot encode\n",
        "\tX = one_hot_encode(sequence_in, cardinality)\n",
        "\ty = one_hot_encode(sequence_out, cardinality)\n",
        "\t# reshape as 3D\n",
        "\tX = X.reshape((1, X.shape[0], X.shape[1]))\n",
        "\ty = y.reshape((1, y.shape[0], y.shape[1]))\n",
        "\treturn X,y\n",
        "\n",
        "# configure problem\n",
        "n_features = 51\n",
        "n_timesteps_in = 5\n",
        "n_timesteps_out = 2\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(150, input_shape=(n_timesteps_in, n_features), return_sequences=True))\n",
        "model.add(AttentionDecoder(150, n_features))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# train LSTM\n",
        "for epoch in range(50):\n",
        "\t# generate new random sequence\n",
        "\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\t# fit model for one epoch on this sequence\n",
        "\tmodel.fit(X, y, epochs=1, verbose=2)\n",
        "# evaluate LSTM\n",
        "total, correct = 100, 0\n",
        "for _ in range(total):\n",
        "\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\tyhat = model.predict(X, verbose=0)\n",
        "\tif array_equal(one_hot_decode(y[0]), one_hot_decode(yhat[0])):\n",
        "\t\tcorrect += 1\n",
        "print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))\n",
        "# spot check some examples\n",
        "for _ in range(10):\n",
        "\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\tyhat = model.predict(X, verbose=0)\n",
        "\tprint('Expected:', one_hot_decode(y[0]), 'Predicted', one_hot_decode(yhat[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " - 2s - loss: 3.9314 - acc: 0.0000e+00\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.9043 - acc: 0.0000e+00\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.8870 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.8843 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.8475 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.8058 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.8141 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.7850 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.7478 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.7178 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.7274 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.6410 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.6446 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.5723 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.5123 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.4008 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.4592 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.3366 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.3486 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.2930 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.0967 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 2.6441 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 2.5207 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 2.5731 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 2.4822 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 2.1989 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.8524 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.8654 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7730 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5124 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6694 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6400 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.8406 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7282 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.8420 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7594 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6758 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7642 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7516 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.8430 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7783 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6663 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7388 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7354 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6522 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6536 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6752 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.0634 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6296 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6473 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5634 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.0523 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6703 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6307 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5859 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6483 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5407 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5902 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5066 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5533 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5659 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6345 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6422 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5525 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6352 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6118 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5975 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5830 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5769 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7488 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6283 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6127 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6129 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7674 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7419 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5678 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7081 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6123 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5974 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6870 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5616 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6215 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7389 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6285 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4723 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7324 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6515 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5286 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6916 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6486 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5695 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5764 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6745 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6411 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5144 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7205 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6026 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6659 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5571 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6333 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6627 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6658 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6337 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5404 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5046 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5680 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5240 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5915 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5446 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7624 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6580 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6526 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5475 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6115 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4657 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6642 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5817 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5176 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5956 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5477 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5104 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6572 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5212 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7500 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5103 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5746 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5184 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6063 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6952 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5830 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5832 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5979 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7434 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6834 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5127 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5730 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6103 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7238 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.2009 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7231 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6066 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5644 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5004 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6712 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6119 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6136 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7183 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5989 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7474 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4764 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5513 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6072 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6769 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6150 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6113 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6340 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6457 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5344 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5133 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5061 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5830 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6106 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6469 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6397 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4722 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5065 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5104 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6361 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5553 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5259 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6430 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5684 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7735 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.1083 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5826 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5255 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6250 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5561 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6127 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5173 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5942 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6087 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5813 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6957 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6208 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7113 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5580 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7114 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6434 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5825 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5509 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5918 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5525 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5298 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5429 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6621 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5207 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5094 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4341 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4848 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5250 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5006 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4486 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6406 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5478 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5441 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5241 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6164 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5980 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4695 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5522 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5861 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6624 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4002 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5176 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6610 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5946 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6873 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4570 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4864 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5950 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6193 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5374 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6768 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4750 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5595 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4903 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5276 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6257 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5115 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5209 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5511 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4595 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4510 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5780 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5137 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5659 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6445 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5966 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5714 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6571 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6373 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6589 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6262 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5859 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4994 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5189 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5215 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6266 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6280 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6081 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4580 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5234 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5868 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4548 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5054 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5190 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5352 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5406 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6068 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5586 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5350 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6453 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5931 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5065 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6005 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5275 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5362 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6791 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4989 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5020 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5252 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.8251 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4872 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4825 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6394 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5800 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6097 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5526 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5072 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4975 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4008 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5280 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6056 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4872 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5456 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6493 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5134 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5956 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5479 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6705 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5017 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5145 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5769 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5907 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4511 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5267 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5467 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4536 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4938 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4882 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7281 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5101 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5587 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7492 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5031 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5306 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5598 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5218 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4919 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4337 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5110 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4858 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5232 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4166 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5511 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6444 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5355 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4869 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5370 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6740 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6286 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5636 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5472 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4614 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6328 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5102 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5911 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5862 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5018 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5025 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6629 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4354 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4892 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5253 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.0528 - acc: 1.0000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5978 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6429 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5057 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6347 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4936 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5213 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5539 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5947 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4457 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.3784 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4615 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5978 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5071 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5618 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5679 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5855 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4311 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.1370 - acc: 1.0000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4415 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4889 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.8568 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5394 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5338 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6645 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6583 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5380 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5041 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6112 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.3935 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5350 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5777 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5809 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6251 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5007 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5084 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6899 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5097 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4697 - acc: 0.6000\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-f91a92a90a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_timesteps_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_timesteps_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# fit model for one epoch on this sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;31m# evaluate LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                 \u001b[0mval_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0mval_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Last batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshould_run_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m     \"\"\"Prints `message` and the tensor value when evaluated.\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m     \u001b[0mNote\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mprint_tensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0midentical\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m     \u001b[0mwhich\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOtherwise\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0moperation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtaken\u001b[0m \u001b[0minto\u001b[0m \u001b[0maccount\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m     \u001b[0;31m# Returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m     \"\"\"\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_keras_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[1;32m   1473\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m                                                run_metadata_ptr)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TL2_It6Cqw4",
        "colab_type": "text"
      },
      "source": [
        "# Attention based enc-dec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INMiEU6CGdmz",
        "colab_type": "code",
        "outputId": "a63fb5d2-f271-4d86-882f-693e7d53f5dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install keract"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keract\n",
            "  Downloading https://files.pythonhosted.org/packages/38/50/42e135986807064315de6dc40e2013d4db417186768ffa05f9980ea7db83/keract-3.0.2-py3-none-any.whl\n",
            "Collecting tensorflow>=2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 35kB/s \n",
            "\u001b[?25hCollecting keras>=2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 26.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.6/dist-packages (from keract) (1.17.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (1.11.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (1.27.1)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 39.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (0.1.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (0.9.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 51.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (0.34.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.3.1->keract) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.3.1->keract) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow>=2.0->keract) (45.2.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (2.21.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (0.4.1)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (0.2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (3.1.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow, keras, keract\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keract-3.0.2 keras-2.3.1 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llnm3mEHCqMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Lambda, dot, Activation, concatenate\n",
        "\n",
        "\n",
        "def attention_3d_block(hidden_states):\n",
        "    # @author: felixhao28.\n",
        "    # hidden_states.shape = (batch_size, time_steps, hidden_size)\n",
        "    hidden_size = int(hidden_states.shape[2])\n",
        "    # Inside dense layer\n",
        "    #              hidden_states            dot               W            =>           score_first_part\n",
        "    # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\n",
        "    # W is the trainable weight matrix of attention Luong's multiplicative style score\n",
        "    score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n",
        "    #            score_first_part           dot        last_hidden_state     => attention_weights\n",
        "    # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)\n",
        "    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n",
        "    score = dot([score_first_part, h_t], [2, 1], name='attention_score')\n",
        "    attention_weights = Activation('softmax', name='attention_weight')(score)\n",
        "    # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\n",
        "    context_vector = dot([hidden_states, attention_weights], [1, 1], name='context_vector')\n",
        "    pre_activation = concatenate([context_vector, h_t], name='attention_output')\n",
        "    attention_vector = Dense(128, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n",
        "    return attention_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cezgNfssEJCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keract import get_activations\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0lX2xggEUGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_recurrent(n, time_steps, input_dim, attention_column=10):\n",
        "    \"\"\"\n",
        "    Data generation. x is purely random except that it's first value equals the target y.\n",
        "    In practice, the network should learn that the target = x[attention_column].\n",
        "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
        "    :param n: the number of samples to retrieve.\n",
        "    :param time_steps: the number of time steps of your series.\n",
        "    :param input_dim: the number of dimensions of each element in the series.\n",
        "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
        "    :return: x: model inputs, y: model targets\n",
        "    \"\"\"\n",
        "    x = np.random.randint(input_dim, size=(n, time_steps))\n",
        "    x = np.eye(input_dim)[x]\n",
        "    y = x[:, attention_column, :]\n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e92BT-JMEXwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
        "    rnn_out = LSTM(32, return_sequences=True)(inputs)\n",
        "    attention_output = attention_3d_block(rnn_out)\n",
        "    output = Dense(INPUT_DIM, activation='sigmoid', name='output')(attention_output)\n",
        "    m = Model(inputs=[inputs], outputs=[output])\n",
        "    print(m.summary())\n",
        "    return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuCpPckrEcl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = 51\n",
        "TIME_STEPS = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2aiIliSEdLn",
        "colab_type": "code",
        "outputId": "46c1ee75-e7ea-4da5-9551-2c1f9f82b57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "n = 30000\n",
        "inputs, outputs = get_data_recurrent(n, TIME_STEPS, INPUT_DIM)\n",
        "m = get_model()\n",
        "m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "m.fit(x=[inputs], y=outputs, epochs=5, batch_size=64, validation_split=0)\n",
        "\n",
        "num_simulations = 10\n",
        "attention_vectors = np.zeros(shape=(num_simulations, TIME_STEPS))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 20, 100)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 20, 32)       17024       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_score_vec (Dense)     (None, 20, 32)       1024        lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "last_hidden_state (Lambda)      (None, 32)           0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_score (Dot)           (None, 20)           0           attention_score_vec[0][0]        \n",
            "                                                                 last_hidden_state[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "attention_weight (Activation)   (None, 20)           0           attention_score[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "context_vector (Dot)            (None, 32)           0           lstm_1[0][0]                     \n",
            "                                                                 attention_weight[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "attention_output (Concatenate)  (None, 64)           0           context_vector[0][0]             \n",
            "                                                                 last_hidden_state[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "attention_vector (Dense)        (None, 128)          8192        attention_output[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 100)          12900       attention_vector[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 39,140\n",
            "Trainable params: 39,140\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 30000 samples\n",
            "Epoch 1/5\n",
            "30000/30000 [==============================] - 17s 569us/sample - loss: 4.5325 - acc: 0.0224\n",
            "Epoch 2/5\n",
            "30000/30000 [==============================] - 17s 568us/sample - loss: 4.2189 - acc: 0.0610\n",
            "Epoch 3/5\n",
            "30000/30000 [==============================] - 17s 560us/sample - loss: 2.7767 - acc: 0.4040\n",
            "Epoch 4/5\n",
            "30000/30000 [==============================] - 17s 556us/sample - loss: 0.4472 - acc: 0.9936\n",
            "Epoch 5/5\n",
            "30000/30000 [==============================] - 17s 559us/sample - loss: 0.0758 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyfyehJIFA8S",
        "colab_type": "code",
        "outputId": "85e461eb-6e56-42bb-d9e6-8ace28ed44f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "for i in range(num_simulations):\n",
        "    testing_inputs_1, testing_outputs = get_data_recurrent(1, TIME_STEPS, INPUT_DIM)\n",
        "    activations = get_activations(m, testing_inputs_1, layer_name='attention_weight')\n",
        "    attention_vec = np.mean(activations['attention_weight'], axis=0).squeeze()\n",
        "    assert np.abs(np.sum(attention_vec) - 1.0) < 1e-5\n",
        "    attention_vectors[i] = attention_vec\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keract/keract.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(model, nodes_to_evaluate, x, y, auto_compile)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_inputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_targets\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_sample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keract/keract.py\u001b[0m in \u001b[0;36meval_fn\u001b[0;34m(k_inputs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_to_evaluate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m     \u001b[0mNote\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mprint_tensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0midentical\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m     \u001b[0mwhich\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOtherwise\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m     \u001b[0;31m# Returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m     \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[1;32m   1473\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Error while reading resource variable attention_score_vec_1/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/attention_score_vec_1/kernel)\n\t [[{{node attention_score_vec_1/Tensordot/ReadVariableOp}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-2a5c064a4682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_simulations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtesting_inputs_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_recurrent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTIME_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_inputs_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'attention_weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mattention_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_vec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keract/keract.py\u001b[0m in \u001b[0;36mget_activations\u001b[0;34m(model, x, layer_name, nodes_to_evaluate, output_format, auto_compile)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0minput_layer_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0minput_layer_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'input_'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_compile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauto_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcraft_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_format_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keract/keract.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(model, nodes_to_evaluate, x, y, auto_compile)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_inputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_targets\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_sample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keract/keract.py\u001b[0m in \u001b[0;36meval_fn\u001b[0;34m(k_inputs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_to_evaluate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m     \"\"\"Prints `message` and the tensor value when evaluated.\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m     \u001b[0mNote\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mprint_tensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0midentical\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m     \u001b[0mwhich\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOtherwise\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0moperation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtaken\u001b[0m \u001b[0minto\u001b[0m \u001b[0maccount\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m     \u001b[0;31m# Returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m     \"\"\"\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_keras_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[1;32m   1473\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m                                                run_metadata_ptr)\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: 2 root error(s) found.\n  (0) Failed precondition: Error while reading resource variable attention_score_vec_1/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/attention_score_vec_1/kernel)\n\t [[{{node attention_score_vec_1/Tensordot/ReadVariableOp}}]]\n\t [[attention_weight_1/Softmax/_31]]\n  (1) Failed precondition: Error while reading resource variable attention_score_vec_1/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/attention_score_vec_1/kernel)\n\t [[{{node attention_score_vec_1/Tensordot/ReadVariableOp}}]]\n0 successful operations.\n0 derived errors ignored."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncr-czkLFDtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention_vector_final = np.mean(attention_vectors, axis=0)\n",
        "attention_df = pd.DataFrame(attention_vector_final, columns=['attention (%)'])\n",
        "attention_df.plot(kind='bar', title='Attention Mechanism as a function of input dimensions.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}