{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcGz8EDo5brI",
        "colab_type": "code",
        "outputId": "c31bfbbe-d578-40c2-d4a6-ec55d963af81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t420MoGXpZ44",
        "colab_type": "code",
        "outputId": "82cf434b-f775-447c-95ad-c9577e157fe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_rsgYZ3DkQW",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlmEtBPtmSxm",
        "colab_type": "text"
      },
      "source": [
        "RUL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwpxsKsZpwHx",
        "colab_type": "code",
        "outputId": "c1b07032-59f5-438d-e8ef-ed627cd1a397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "! pip install pandas"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpQlSNiqt-Rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UR-recJrv0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgI6ydT2wlvR",
        "colab_type": "code",
        "outputId": "d3ef7401-dcd6-454b-a90f-43f53c7cc22f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZh0R7zFIbLT",
        "colab_type": "code",
        "outputId": "62f2c79e-a896-442a-f1e4-e54590f0e7c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGSgRrWarro8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn import preprocessing\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHtGHAU-sqmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv('/content/drive/My Drive/sensor.csv',header=0)\n",
        "train_df.drop(train_df.columns[[17]], axis=1, inplace=True)\n",
        "\n",
        "#train_df = pd.read_csv('sensor.csv', header=0)\n",
        "#train_df.drop(train_df.columns[[17]], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TqT6PmvPYoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''\n",
        "for i in range(2,54):\n",
        "  #target = train_df[i]\n",
        "  target = train_df.iloc[:, i]\n",
        "  print(target)\n",
        "  train_df = train_df.assign(InterpolateSLinear=train_df.target.interpolate(method='slinear'))\n",
        "'''\n",
        "#train_df = train_df.interpolate(method ='slinear', limit_direction ='forward') \n",
        "train_df = train_df.interpolate(method = 'linear' , order = 1 , limit_direction ='forward') \n",
        "#train_df.interpolate()\n",
        "#train_df[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyoFluN2AGs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.insert(54, \"RUL\", 0)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnVvIPdnrm5P",
        "colab_type": "code",
        "outputId": "da35259f-d167-45a7-eff2-260a0b29cfcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "train_df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0        0\n",
              "timestamp         0\n",
              "sensor_00         0\n",
              "sensor_01         0\n",
              "sensor_02         0\n",
              "sensor_03         0\n",
              "sensor_04         0\n",
              "sensor_05         0\n",
              "sensor_06         0\n",
              "sensor_07         0\n",
              "sensor_08         0\n",
              "sensor_09         0\n",
              "sensor_10         0\n",
              "sensor_11         0\n",
              "sensor_12         0\n",
              "sensor_13         0\n",
              "sensor_14         0\n",
              "sensor_16         0\n",
              "sensor_17         0\n",
              "sensor_18         0\n",
              "sensor_19         0\n",
              "sensor_20         0\n",
              "sensor_21         0\n",
              "sensor_22         0\n",
              "sensor_23         0\n",
              "sensor_24         0\n",
              "sensor_25         0\n",
              "sensor_26         0\n",
              "sensor_27         0\n",
              "sensor_28         0\n",
              "sensor_29         0\n",
              "sensor_30         0\n",
              "sensor_31         0\n",
              "sensor_32         0\n",
              "sensor_33         0\n",
              "sensor_34         0\n",
              "sensor_35         0\n",
              "sensor_36         0\n",
              "sensor_37         0\n",
              "sensor_38         0\n",
              "sensor_39         0\n",
              "sensor_40         0\n",
              "sensor_41         0\n",
              "sensor_42         0\n",
              "sensor_43         0\n",
              "sensor_44         0\n",
              "sensor_45         0\n",
              "sensor_46         0\n",
              "sensor_47         0\n",
              "sensor_48         0\n",
              "sensor_49         0\n",
              "sensor_50         0\n",
              "sensor_51         0\n",
              "machine_status    0\n",
              "RUL               0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIbduaOzM9_J",
        "colab_type": "code",
        "outputId": "4d4b1a91-a121-4d63-fec1-e7bcfddc9553",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        }
      },
      "source": [
        "train_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>sensor_00</th>\n",
              "      <th>sensor_01</th>\n",
              "      <th>sensor_02</th>\n",
              "      <th>sensor_03</th>\n",
              "      <th>sensor_04</th>\n",
              "      <th>sensor_05</th>\n",
              "      <th>sensor_06</th>\n",
              "      <th>sensor_07</th>\n",
              "      <th>sensor_08</th>\n",
              "      <th>sensor_09</th>\n",
              "      <th>sensor_10</th>\n",
              "      <th>sensor_11</th>\n",
              "      <th>sensor_12</th>\n",
              "      <th>sensor_13</th>\n",
              "      <th>sensor_14</th>\n",
              "      <th>sensor_16</th>\n",
              "      <th>sensor_17</th>\n",
              "      <th>sensor_18</th>\n",
              "      <th>sensor_19</th>\n",
              "      <th>sensor_20</th>\n",
              "      <th>sensor_21</th>\n",
              "      <th>sensor_22</th>\n",
              "      <th>sensor_23</th>\n",
              "      <th>sensor_24</th>\n",
              "      <th>sensor_25</th>\n",
              "      <th>sensor_26</th>\n",
              "      <th>sensor_27</th>\n",
              "      <th>sensor_28</th>\n",
              "      <th>sensor_29</th>\n",
              "      <th>sensor_30</th>\n",
              "      <th>sensor_31</th>\n",
              "      <th>sensor_32</th>\n",
              "      <th>sensor_33</th>\n",
              "      <th>sensor_34</th>\n",
              "      <th>sensor_35</th>\n",
              "      <th>sensor_36</th>\n",
              "      <th>sensor_37</th>\n",
              "      <th>sensor_38</th>\n",
              "      <th>sensor_39</th>\n",
              "      <th>sensor_40</th>\n",
              "      <th>sensor_41</th>\n",
              "      <th>sensor_42</th>\n",
              "      <th>sensor_43</th>\n",
              "      <th>sensor_44</th>\n",
              "      <th>sensor_45</th>\n",
              "      <th>sensor_46</th>\n",
              "      <th>sensor_47</th>\n",
              "      <th>sensor_48</th>\n",
              "      <th>sensor_49</th>\n",
              "      <th>sensor_50</th>\n",
              "      <th>sensor_51</th>\n",
              "      <th>machine_status</th>\n",
              "      <th>RUL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2018-04-01 00:00:00</td>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.09201</td>\n",
              "      <td>53.211800</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2018-04-01 00:01:00</td>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.09201</td>\n",
              "      <td>53.211800</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2018-04-01 00:02:00</td>\n",
              "      <td>2.444734</td>\n",
              "      <td>47.35243</td>\n",
              "      <td>53.211800</td>\n",
              "      <td>46.397570</td>\n",
              "      <td>638.888900</td>\n",
              "      <td>73.54598</td>\n",
              "      <td>13.32465</td>\n",
              "      <td>16.03733</td>\n",
              "      <td>15.61777</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>37.86777</td>\n",
              "      <td>48.17723</td>\n",
              "      <td>32.08894</td>\n",
              "      <td>1.708474</td>\n",
              "      <td>420.8480</td>\n",
              "      <td>462.7798</td>\n",
              "      <td>459.6364</td>\n",
              "      <td>2.500062</td>\n",
              "      <td>666.2234</td>\n",
              "      <td>399.9418</td>\n",
              "      <td>880.4237</td>\n",
              "      <td>501.3617</td>\n",
              "      <td>982.7342</td>\n",
              "      <td>631.1326</td>\n",
              "      <td>740.8031</td>\n",
              "      <td>849.8997</td>\n",
              "      <td>454.2390</td>\n",
              "      <td>778.5734</td>\n",
              "      <td>715.6266</td>\n",
              "      <td>661.5740</td>\n",
              "      <td>721.8750</td>\n",
              "      <td>694.7721</td>\n",
              "      <td>441.2635</td>\n",
              "      <td>169.9820</td>\n",
              "      <td>343.1955</td>\n",
              "      <td>200.9694</td>\n",
              "      <td>93.90508</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>31.25000</td>\n",
              "      <td>69.53125</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.770830</td>\n",
              "      <td>41.66666</td>\n",
              "      <td>39.351852</td>\n",
              "      <td>65.39352</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194443</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>241.3194</td>\n",
              "      <td>203.7037</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2018-04-01 00:03:00</td>\n",
              "      <td>2.460474</td>\n",
              "      <td>47.09201</td>\n",
              "      <td>53.168400</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>628.125000</td>\n",
              "      <td>76.98898</td>\n",
              "      <td>13.31742</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>38.57977</td>\n",
              "      <td>48.65607</td>\n",
              "      <td>31.67221</td>\n",
              "      <td>1.579427</td>\n",
              "      <td>420.7494</td>\n",
              "      <td>462.8980</td>\n",
              "      <td>460.8858</td>\n",
              "      <td>2.509521</td>\n",
              "      <td>666.0114</td>\n",
              "      <td>399.1046</td>\n",
              "      <td>878.8917</td>\n",
              "      <td>499.0430</td>\n",
              "      <td>977.7520</td>\n",
              "      <td>625.4076</td>\n",
              "      <td>739.2722</td>\n",
              "      <td>847.7579</td>\n",
              "      <td>474.8731</td>\n",
              "      <td>779.5091</td>\n",
              "      <td>690.4011</td>\n",
              "      <td>686.1111</td>\n",
              "      <td>754.6875</td>\n",
              "      <td>683.3831</td>\n",
              "      <td>446.2493</td>\n",
              "      <td>166.4987</td>\n",
              "      <td>343.9586</td>\n",
              "      <td>193.1689</td>\n",
              "      <td>101.04060</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>72.13541</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>40.88541</td>\n",
              "      <td>39.062500</td>\n",
              "      <td>64.81481</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>66.84028</td>\n",
              "      <td>240.4514</td>\n",
              "      <td>203.1250</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2018-04-01 00:04:00</td>\n",
              "      <td>2.445718</td>\n",
              "      <td>47.13541</td>\n",
              "      <td>53.211800</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>636.458300</td>\n",
              "      <td>76.58897</td>\n",
              "      <td>13.35359</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>39.48939</td>\n",
              "      <td>49.06298</td>\n",
              "      <td>31.95202</td>\n",
              "      <td>1.683831</td>\n",
              "      <td>419.8926</td>\n",
              "      <td>461.4906</td>\n",
              "      <td>468.2206</td>\n",
              "      <td>2.604785</td>\n",
              "      <td>663.2111</td>\n",
              "      <td>400.5426</td>\n",
              "      <td>882.5874</td>\n",
              "      <td>498.5383</td>\n",
              "      <td>979.5755</td>\n",
              "      <td>627.1830</td>\n",
              "      <td>737.6033</td>\n",
              "      <td>846.9182</td>\n",
              "      <td>408.8159</td>\n",
              "      <td>785.2307</td>\n",
              "      <td>704.6937</td>\n",
              "      <td>631.4814</td>\n",
              "      <td>766.1458</td>\n",
              "      <td>702.4431</td>\n",
              "      <td>433.9081</td>\n",
              "      <td>164.7498</td>\n",
              "      <td>339.9630</td>\n",
              "      <td>193.8770</td>\n",
              "      <td>101.70380</td>\n",
              "      <td>42.70833</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>76.82291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>65.10416</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>158.2755</td>\n",
              "      <td>66.55093</td>\n",
              "      <td>242.1875</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220315</th>\n",
              "      <td>220315</td>\n",
              "      <td>2018-08-31 23:55:00</td>\n",
              "      <td>2.407350</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.520830</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>634.722229</td>\n",
              "      <td>64.59095</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>16.65220</td>\n",
              "      <td>15.65393</td>\n",
              "      <td>15.16204</td>\n",
              "      <td>43.17085</td>\n",
              "      <td>54.16052</td>\n",
              "      <td>38.05424</td>\n",
              "      <td>13.265320</td>\n",
              "      <td>420.7993</td>\n",
              "      <td>463.2318</td>\n",
              "      <td>458.3615</td>\n",
              "      <td>2.499117</td>\n",
              "      <td>676.6655</td>\n",
              "      <td>405.7680</td>\n",
              "      <td>894.5920</td>\n",
              "      <td>543.5801</td>\n",
              "      <td>1109.5010</td>\n",
              "      <td>611.1745</td>\n",
              "      <td>700.5885</td>\n",
              "      <td>796.5964</td>\n",
              "      <td>692.1138</td>\n",
              "      <td>779.2067</td>\n",
              "      <td>485.0358</td>\n",
              "      <td>691.6666</td>\n",
              "      <td>974.9999</td>\n",
              "      <td>927.6135</td>\n",
              "      <td>477.3156</td>\n",
              "      <td>266.0334</td>\n",
              "      <td>578.5221</td>\n",
              "      <td>817.5707</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>47.13541</td>\n",
              "      <td>29.16667</td>\n",
              "      <td>71.61458</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>30.208330</td>\n",
              "      <td>38.28125</td>\n",
              "      <td>68.287030</td>\n",
              "      <td>52.37268</td>\n",
              "      <td>48.32176</td>\n",
              "      <td>41.087960</td>\n",
              "      <td>212.3843</td>\n",
              "      <td>153.64580</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>231.1921</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220316</th>\n",
              "      <td>220316</td>\n",
              "      <td>2018-08-31 23:56:00</td>\n",
              "      <td>2.400463</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.564240</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>630.902771</td>\n",
              "      <td>65.83363</td>\n",
              "      <td>15.15480</td>\n",
              "      <td>16.70284</td>\n",
              "      <td>15.65393</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>43.21038</td>\n",
              "      <td>54.52602</td>\n",
              "      <td>38.53485</td>\n",
              "      <td>13.242270</td>\n",
              "      <td>422.1567</td>\n",
              "      <td>463.1928</td>\n",
              "      <td>468.4388</td>\n",
              "      <td>2.618476</td>\n",
              "      <td>676.6547</td>\n",
              "      <td>406.2575</td>\n",
              "      <td>895.5599</td>\n",
              "      <td>541.7014</td>\n",
              "      <td>1106.3710</td>\n",
              "      <td>609.4917</td>\n",
              "      <td>698.4915</td>\n",
              "      <td>800.1906</td>\n",
              "      <td>697.8002</td>\n",
              "      <td>797.5571</td>\n",
              "      <td>510.9510</td>\n",
              "      <td>672.2222</td>\n",
              "      <td>927.0833</td>\n",
              "      <td>907.9463</td>\n",
              "      <td>487.8679</td>\n",
              "      <td>262.2222</td>\n",
              "      <td>568.1035</td>\n",
              "      <td>807.0151</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>46.87500</td>\n",
              "      <td>28.90625</td>\n",
              "      <td>73.17708</td>\n",
              "      <td>30.208332</td>\n",
              "      <td>29.947920</td>\n",
              "      <td>38.28125</td>\n",
              "      <td>66.840280</td>\n",
              "      <td>50.63657</td>\n",
              "      <td>48.03241</td>\n",
              "      <td>40.798610</td>\n",
              "      <td>213.8310</td>\n",
              "      <td>156.25000</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>231.1921</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220317</th>\n",
              "      <td>220317</td>\n",
              "      <td>2018-08-31 23:57:00</td>\n",
              "      <td>2.396528</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.520830</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>625.925903</td>\n",
              "      <td>67.29445</td>\n",
              "      <td>15.08970</td>\n",
              "      <td>16.70284</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>43.12836</td>\n",
              "      <td>55.11779</td>\n",
              "      <td>38.52678</td>\n",
              "      <td>13.188660</td>\n",
              "      <td>420.2166</td>\n",
              "      <td>462.4065</td>\n",
              "      <td>468.6293</td>\n",
              "      <td>2.620500</td>\n",
              "      <td>677.3162</td>\n",
              "      <td>407.1144</td>\n",
              "      <td>892.2204</td>\n",
              "      <td>542.8578</td>\n",
              "      <td>1106.6980</td>\n",
              "      <td>610.9940</td>\n",
              "      <td>703.1645</td>\n",
              "      <td>800.3767</td>\n",
              "      <td>704.6601</td>\n",
              "      <td>799.3120</td>\n",
              "      <td>492.7720</td>\n",
              "      <td>689.3519</td>\n",
              "      <td>924.4791</td>\n",
              "      <td>926.8102</td>\n",
              "      <td>494.1249</td>\n",
              "      <td>260.8372</td>\n",
              "      <td>553.8872</td>\n",
              "      <td>805.5605</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>46.09375</td>\n",
              "      <td>28.64583</td>\n",
              "      <td>77.08333</td>\n",
              "      <td>29.947920</td>\n",
              "      <td>30.208330</td>\n",
              "      <td>39.06250</td>\n",
              "      <td>65.393520</td>\n",
              "      <td>48.90046</td>\n",
              "      <td>48.03241</td>\n",
              "      <td>40.798610</td>\n",
              "      <td>217.3032</td>\n",
              "      <td>155.38190</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>232.0602</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220318</th>\n",
              "      <td>220318</td>\n",
              "      <td>2018-08-31 23:58:00</td>\n",
              "      <td>2.406366</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.520832</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>635.648100</td>\n",
              "      <td>65.09175</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>16.56539</td>\n",
              "      <td>15.74074</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>42.35746</td>\n",
              "      <td>55.99321</td>\n",
              "      <td>38.89159</td>\n",
              "      <td>13.173460</td>\n",
              "      <td>420.5700</td>\n",
              "      <td>457.0362</td>\n",
              "      <td>459.7941</td>\n",
              "      <td>2.514596</td>\n",
              "      <td>672.6165</td>\n",
              "      <td>404.3277</td>\n",
              "      <td>887.9969</td>\n",
              "      <td>539.3630</td>\n",
              "      <td>1103.9550</td>\n",
              "      <td>605.7183</td>\n",
              "      <td>697.3713</td>\n",
              "      <td>793.7070</td>\n",
              "      <td>706.9692</td>\n",
              "      <td>793.0610</td>\n",
              "      <td>490.2170</td>\n",
              "      <td>687.0370</td>\n",
              "      <td>931.7708</td>\n",
              "      <td>915.4362</td>\n",
              "      <td>484.1161</td>\n",
              "      <td>261.3184</td>\n",
              "      <td>559.4439</td>\n",
              "      <td>807.0808</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>45.83333</td>\n",
              "      <td>28.38542</td>\n",
              "      <td>78.64583</td>\n",
              "      <td>29.947916</td>\n",
              "      <td>30.208332</td>\n",
              "      <td>40.62500</td>\n",
              "      <td>64.236110</td>\n",
              "      <td>47.74306</td>\n",
              "      <td>48.32176</td>\n",
              "      <td>40.509258</td>\n",
              "      <td>222.5116</td>\n",
              "      <td>153.93520</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>234.0856</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220319</th>\n",
              "      <td>220319</td>\n",
              "      <td>2018-08-31 23:59:00</td>\n",
              "      <td>2.396528</td>\n",
              "      <td>47.69965</td>\n",
              "      <td>50.520832</td>\n",
              "      <td>43.142361</td>\n",
              "      <td>639.814800</td>\n",
              "      <td>65.45634</td>\n",
              "      <td>15.11863</td>\n",
              "      <td>16.65220</td>\n",
              "      <td>15.65393</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>42.62814</td>\n",
              "      <td>56.49642</td>\n",
              "      <td>39.40957</td>\n",
              "      <td>13.125930</td>\n",
              "      <td>421.2080</td>\n",
              "      <td>468.9915</td>\n",
              "      <td>456.5726</td>\n",
              "      <td>2.487299</td>\n",
              "      <td>676.5834</td>\n",
              "      <td>405.6293</td>\n",
              "      <td>897.8508</td>\n",
              "      <td>542.0950</td>\n",
              "      <td>1108.8270</td>\n",
              "      <td>608.5364</td>\n",
              "      <td>698.0792</td>\n",
              "      <td>800.0387</td>\n",
              "      <td>703.6251</td>\n",
              "      <td>800.2143</td>\n",
              "      <td>496.4068</td>\n",
              "      <td>686.1111</td>\n",
              "      <td>917.7083</td>\n",
              "      <td>926.3979</td>\n",
              "      <td>489.0367</td>\n",
              "      <td>258.4387</td>\n",
              "      <td>558.0558</td>\n",
              "      <td>811.1204</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>45.31250</td>\n",
              "      <td>27.86458</td>\n",
              "      <td>77.86458</td>\n",
              "      <td>29.947916</td>\n",
              "      <td>30.208332</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>62.789350</td>\n",
              "      <td>46.29630</td>\n",
              "      <td>48.90046</td>\n",
              "      <td>40.219910</td>\n",
              "      <td>227.4306</td>\n",
              "      <td>150.46300</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>234.0856</td>\n",
              "      <td>NORMAL</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>220320 rows × 55 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0            timestamp  ...  machine_status  RUL\n",
              "0                0  2018-04-01 00:00:00  ...          NORMAL    0\n",
              "1                1  2018-04-01 00:01:00  ...          NORMAL    0\n",
              "2                2  2018-04-01 00:02:00  ...          NORMAL    0\n",
              "3                3  2018-04-01 00:03:00  ...          NORMAL    0\n",
              "4                4  2018-04-01 00:04:00  ...          NORMAL    0\n",
              "...            ...                  ...  ...             ...  ...\n",
              "220315      220315  2018-08-31 23:55:00  ...          NORMAL    0\n",
              "220316      220316  2018-08-31 23:56:00  ...          NORMAL    0\n",
              "220317      220317  2018-08-31 23:57:00  ...          NORMAL    0\n",
              "220318      220318  2018-08-31 23:58:00  ...          NORMAL    0\n",
              "220319      220319  2018-08-31 23:59:00  ...          NORMAL    0\n",
              "\n",
              "[220320 rows x 55 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyCA3JWhumBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "k = 0\n",
        "for i in range(len(train_df)):\n",
        "    if(train_df[\"machine_status\"][i] == \"BROKEN\" ):\n",
        "        count = 0\n",
        "        for j in range(i,k-1,-1):\n",
        "            train_df.at[j,'RUL']= count\n",
        "            count = count + 1\n",
        "        k = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCpXOU0VpJKm",
        "colab_type": "code",
        "outputId": "ebdcde16-4453-4523-f2bb-2b5270c74d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "\n",
        "n = len(train_df)-1\n",
        "while(train_df[\"machine_status\"][n] != \"BROKEN\"):\n",
        "    n -= 1   \n",
        "train_df = train_df[:n+1]        \n",
        "train_df[\"machine_status\"][n+1] = 0    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlHUvnzKyZuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "from pandas import read_csv, DataFrame\n",
        "from numpy.random import seed\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h46Dj6SRwSkB",
        "colab_type": "code",
        "outputId": "12c74d94-f146-448e-9d53-26323919d551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "train_df.drop(['machine_status'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBk9fV-zwXYf",
        "colab_type": "code",
        "outputId": "eb055349-a64d-482d-f7ac-6a25af18e9fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "\n",
        "train_df.drop(['Unnamed: 0'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ5P2v4WMUQC",
        "colab_type": "code",
        "outputId": "d61622d5-f21e-487d-e3d8-0a4009132b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        }
      },
      "source": [
        "\n",
        "\n",
        "train_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>sensor_00</th>\n",
              "      <th>sensor_01</th>\n",
              "      <th>sensor_02</th>\n",
              "      <th>sensor_03</th>\n",
              "      <th>sensor_04</th>\n",
              "      <th>sensor_05</th>\n",
              "      <th>sensor_06</th>\n",
              "      <th>sensor_07</th>\n",
              "      <th>sensor_08</th>\n",
              "      <th>sensor_09</th>\n",
              "      <th>sensor_10</th>\n",
              "      <th>sensor_11</th>\n",
              "      <th>sensor_12</th>\n",
              "      <th>sensor_13</th>\n",
              "      <th>sensor_14</th>\n",
              "      <th>sensor_16</th>\n",
              "      <th>sensor_17</th>\n",
              "      <th>sensor_18</th>\n",
              "      <th>sensor_19</th>\n",
              "      <th>sensor_20</th>\n",
              "      <th>sensor_21</th>\n",
              "      <th>sensor_22</th>\n",
              "      <th>sensor_23</th>\n",
              "      <th>sensor_24</th>\n",
              "      <th>sensor_25</th>\n",
              "      <th>sensor_26</th>\n",
              "      <th>sensor_27</th>\n",
              "      <th>sensor_28</th>\n",
              "      <th>sensor_29</th>\n",
              "      <th>sensor_30</th>\n",
              "      <th>sensor_31</th>\n",
              "      <th>sensor_32</th>\n",
              "      <th>sensor_33</th>\n",
              "      <th>sensor_34</th>\n",
              "      <th>sensor_35</th>\n",
              "      <th>sensor_36</th>\n",
              "      <th>sensor_37</th>\n",
              "      <th>sensor_38</th>\n",
              "      <th>sensor_39</th>\n",
              "      <th>sensor_40</th>\n",
              "      <th>sensor_41</th>\n",
              "      <th>sensor_42</th>\n",
              "      <th>sensor_43</th>\n",
              "      <th>sensor_44</th>\n",
              "      <th>sensor_45</th>\n",
              "      <th>sensor_46</th>\n",
              "      <th>sensor_47</th>\n",
              "      <th>sensor_48</th>\n",
              "      <th>sensor_49</th>\n",
              "      <th>sensor_50</th>\n",
              "      <th>sensor_51</th>\n",
              "      <th>RUL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-04-01 00:00:00</td>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-04-01 00:01:00</td>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-04-01 00:02:00</td>\n",
              "      <td>2.444734</td>\n",
              "      <td>47.352430</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397570</td>\n",
              "      <td>638.888900</td>\n",
              "      <td>73.54598</td>\n",
              "      <td>13.32465</td>\n",
              "      <td>16.03733</td>\n",
              "      <td>15.61777</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>37.86777</td>\n",
              "      <td>48.17723</td>\n",
              "      <td>32.08894</td>\n",
              "      <td>1.708474</td>\n",
              "      <td>420.8480</td>\n",
              "      <td>462.7798</td>\n",
              "      <td>459.6364</td>\n",
              "      <td>2.500062</td>\n",
              "      <td>666.2234</td>\n",
              "      <td>399.9418</td>\n",
              "      <td>880.4237</td>\n",
              "      <td>501.3617</td>\n",
              "      <td>982.7342</td>\n",
              "      <td>631.1326</td>\n",
              "      <td>740.8031</td>\n",
              "      <td>849.8997</td>\n",
              "      <td>454.2390</td>\n",
              "      <td>778.5734</td>\n",
              "      <td>715.6266</td>\n",
              "      <td>661.5740</td>\n",
              "      <td>721.8750</td>\n",
              "      <td>694.7721</td>\n",
              "      <td>441.2635</td>\n",
              "      <td>169.9820</td>\n",
              "      <td>343.1955</td>\n",
              "      <td>200.9694</td>\n",
              "      <td>93.90508</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>31.25000</td>\n",
              "      <td>69.53125</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.770830</td>\n",
              "      <td>41.66666</td>\n",
              "      <td>39.351852</td>\n",
              "      <td>65.39352</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194443</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>241.3194</td>\n",
              "      <td>203.7037</td>\n",
              "      <td>17153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-04-01 00:03:00</td>\n",
              "      <td>2.460474</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.16840</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>628.125000</td>\n",
              "      <td>76.98898</td>\n",
              "      <td>13.31742</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>38.57977</td>\n",
              "      <td>48.65607</td>\n",
              "      <td>31.67221</td>\n",
              "      <td>1.579427</td>\n",
              "      <td>420.7494</td>\n",
              "      <td>462.8980</td>\n",
              "      <td>460.8858</td>\n",
              "      <td>2.509521</td>\n",
              "      <td>666.0114</td>\n",
              "      <td>399.1046</td>\n",
              "      <td>878.8917</td>\n",
              "      <td>499.0430</td>\n",
              "      <td>977.7520</td>\n",
              "      <td>625.4076</td>\n",
              "      <td>739.2722</td>\n",
              "      <td>847.7579</td>\n",
              "      <td>474.8731</td>\n",
              "      <td>779.5091</td>\n",
              "      <td>690.4011</td>\n",
              "      <td>686.1111</td>\n",
              "      <td>754.6875</td>\n",
              "      <td>683.3831</td>\n",
              "      <td>446.2493</td>\n",
              "      <td>166.4987</td>\n",
              "      <td>343.9586</td>\n",
              "      <td>193.1689</td>\n",
              "      <td>101.04060</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>72.13541</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>40.88541</td>\n",
              "      <td>39.062500</td>\n",
              "      <td>64.81481</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>66.84028</td>\n",
              "      <td>240.4514</td>\n",
              "      <td>203.1250</td>\n",
              "      <td>17152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-04-01 00:04:00</td>\n",
              "      <td>2.445718</td>\n",
              "      <td>47.135410</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>636.458300</td>\n",
              "      <td>76.58897</td>\n",
              "      <td>13.35359</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>39.48939</td>\n",
              "      <td>49.06298</td>\n",
              "      <td>31.95202</td>\n",
              "      <td>1.683831</td>\n",
              "      <td>419.8926</td>\n",
              "      <td>461.4906</td>\n",
              "      <td>468.2206</td>\n",
              "      <td>2.604785</td>\n",
              "      <td>663.2111</td>\n",
              "      <td>400.5426</td>\n",
              "      <td>882.5874</td>\n",
              "      <td>498.5383</td>\n",
              "      <td>979.5755</td>\n",
              "      <td>627.1830</td>\n",
              "      <td>737.6033</td>\n",
              "      <td>846.9182</td>\n",
              "      <td>408.8159</td>\n",
              "      <td>785.2307</td>\n",
              "      <td>704.6937</td>\n",
              "      <td>631.4814</td>\n",
              "      <td>766.1458</td>\n",
              "      <td>702.4431</td>\n",
              "      <td>433.9081</td>\n",
              "      <td>164.7498</td>\n",
              "      <td>339.9630</td>\n",
              "      <td>193.8770</td>\n",
              "      <td>101.70380</td>\n",
              "      <td>42.70833</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>76.82291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>65.10416</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>158.2755</td>\n",
              "      <td>66.55093</td>\n",
              "      <td>242.1875</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166436</th>\n",
              "      <td>2018-07-25 13:56:00</td>\n",
              "      <td>2.313889</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>71.464119</td>\n",
              "      <td>73.00980</td>\n",
              "      <td>14.36632</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.12587</td>\n",
              "      <td>38.34975</td>\n",
              "      <td>49.88500</td>\n",
              "      <td>31.94489</td>\n",
              "      <td>12.285810</td>\n",
              "      <td>421.4294</td>\n",
              "      <td>458.9118</td>\n",
              "      <td>456.3463</td>\n",
              "      <td>2.459503</td>\n",
              "      <td>666.8698</td>\n",
              "      <td>398.0684</td>\n",
              "      <td>879.7014</td>\n",
              "      <td>536.2166</td>\n",
              "      <td>1091.1240</td>\n",
              "      <td>629.8835</td>\n",
              "      <td>740.9833</td>\n",
              "      <td>981.7558</td>\n",
              "      <td>497.2223</td>\n",
              "      <td>995.5433</td>\n",
              "      <td>563.5882</td>\n",
              "      <td>676.8519</td>\n",
              "      <td>949.9999</td>\n",
              "      <td>990.9057</td>\n",
              "      <td>564.7968</td>\n",
              "      <td>342.3690</td>\n",
              "      <td>498.5037</td>\n",
              "      <td>804.9069</td>\n",
              "      <td>55.82830</td>\n",
              "      <td>53.38541</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>93.48958</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>45.138890</td>\n",
              "      <td>252.3148</td>\n",
              "      <td>69.15509</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>198.2060</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166437</th>\n",
              "      <td>2018-07-25 13:57:00</td>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923611</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>72.47576</td>\n",
              "      <td>14.33015</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.76968</td>\n",
              "      <td>15.16204</td>\n",
              "      <td>38.58192</td>\n",
              "      <td>49.96759</td>\n",
              "      <td>31.46310</td>\n",
              "      <td>12.213550</td>\n",
              "      <td>420.6030</td>\n",
              "      <td>460.6981</td>\n",
              "      <td>467.5996</td>\n",
              "      <td>2.603880</td>\n",
              "      <td>665.2674</td>\n",
              "      <td>399.1180</td>\n",
              "      <td>879.2195</td>\n",
              "      <td>534.4346</td>\n",
              "      <td>1089.8550</td>\n",
              "      <td>628.6547</td>\n",
              "      <td>738.9200</td>\n",
              "      <td>979.7875</td>\n",
              "      <td>504.8127</td>\n",
              "      <td>1004.9950</td>\n",
              "      <td>569.5920</td>\n",
              "      <td>719.9074</td>\n",
              "      <td>934.3749</td>\n",
              "      <td>982.0567</td>\n",
              "      <td>559.5000</td>\n",
              "      <td>345.2228</td>\n",
              "      <td>500.3699</td>\n",
              "      <td>804.2908</td>\n",
              "      <td>49.17398</td>\n",
              "      <td>52.60416</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>95.57291</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>253.1829</td>\n",
              "      <td>71.46991</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>202.8356</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166438</th>\n",
              "      <td>2018-07-25 13:58:00</td>\n",
              "      <td>2.322743</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>70.48889</td>\n",
              "      <td>14.46036</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.73351</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>38.87765</td>\n",
              "      <td>49.75908</td>\n",
              "      <td>31.78839</td>\n",
              "      <td>12.180410</td>\n",
              "      <td>419.6096</td>\n",
              "      <td>462.2329</td>\n",
              "      <td>466.0083</td>\n",
              "      <td>2.577086</td>\n",
              "      <td>667.1351</td>\n",
              "      <td>398.3854</td>\n",
              "      <td>881.7244</td>\n",
              "      <td>536.0204</td>\n",
              "      <td>1095.5440</td>\n",
              "      <td>630.5479</td>\n",
              "      <td>743.5366</td>\n",
              "      <td>981.5860</td>\n",
              "      <td>488.0563</td>\n",
              "      <td>1009.6940</td>\n",
              "      <td>559.3523</td>\n",
              "      <td>678.2407</td>\n",
              "      <td>947.9166</td>\n",
              "      <td>1016.9990</td>\n",
              "      <td>558.1667</td>\n",
              "      <td>344.4498</td>\n",
              "      <td>490.6038</td>\n",
              "      <td>796.3863</td>\n",
              "      <td>39.34733</td>\n",
              "      <td>51.82291</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>100.52080</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>49.73958</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>52.08333</td>\n",
              "      <td>61.34259</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>249.1319</td>\n",
              "      <td>72.33796</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>206.8866</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166439</th>\n",
              "      <td>2018-07-25 13:59:00</td>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.789930</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>68.74365</td>\n",
              "      <td>14.43866</td>\n",
              "      <td>16.07350</td>\n",
              "      <td>15.53096</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>38.21849</td>\n",
              "      <td>49.73039</td>\n",
              "      <td>32.32672</td>\n",
              "      <td>12.209670</td>\n",
              "      <td>420.0334</td>\n",
              "      <td>460.0883</td>\n",
              "      <td>460.4448</td>\n",
              "      <td>2.509833</td>\n",
              "      <td>664.8566</td>\n",
              "      <td>399.6012</td>\n",
              "      <td>880.7674</td>\n",
              "      <td>535.4742</td>\n",
              "      <td>1092.3240</td>\n",
              "      <td>628.6292</td>\n",
              "      <td>743.1452</td>\n",
              "      <td>982.4372</td>\n",
              "      <td>511.7988</td>\n",
              "      <td>1025.1100</td>\n",
              "      <td>559.1011</td>\n",
              "      <td>743.5185</td>\n",
              "      <td>960.4166</td>\n",
              "      <td>998.0515</td>\n",
              "      <td>551.9908</td>\n",
              "      <td>339.3840</td>\n",
              "      <td>492.3628</td>\n",
              "      <td>796.2476</td>\n",
              "      <td>53.73619</td>\n",
              "      <td>51.04166</td>\n",
              "      <td>40.62500</td>\n",
              "      <td>104.68750</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>50.52083</td>\n",
              "      <td>52.662040</td>\n",
              "      <td>53.24074</td>\n",
              "      <td>63.36805</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>244.5023</td>\n",
              "      <td>72.62731</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>209.7801</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166440</th>\n",
              "      <td>2018-07-25 14:00:00</td>\n",
              "      <td>2.318808</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>420.503448</td>\n",
              "      <td>72.52040</td>\n",
              "      <td>14.18547</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>36.71748</td>\n",
              "      <td>50.04619</td>\n",
              "      <td>30.74288</td>\n",
              "      <td>12.217860</td>\n",
              "      <td>420.8600</td>\n",
              "      <td>463.5181</td>\n",
              "      <td>455.2702</td>\n",
              "      <td>2.468381</td>\n",
              "      <td>665.9402</td>\n",
              "      <td>399.0930</td>\n",
              "      <td>880.8410</td>\n",
              "      <td>531.8727</td>\n",
              "      <td>1093.5790</td>\n",
              "      <td>626.4651</td>\n",
              "      <td>740.6738</td>\n",
              "      <td>981.4561</td>\n",
              "      <td>490.7728</td>\n",
              "      <td>1023.1660</td>\n",
              "      <td>551.2756</td>\n",
              "      <td>703.7037</td>\n",
              "      <td>971.8749</td>\n",
              "      <td>1013.3760</td>\n",
              "      <td>546.6656</td>\n",
              "      <td>363.0320</td>\n",
              "      <td>522.5594</td>\n",
              "      <td>812.8298</td>\n",
              "      <td>44.77601</td>\n",
              "      <td>50.00000</td>\n",
              "      <td>39.06250</td>\n",
              "      <td>102.86460</td>\n",
              "      <td>35.416660</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>51.30208</td>\n",
              "      <td>52.102000</td>\n",
              "      <td>52.66204</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>43.981480</td>\n",
              "      <td>230.3241</td>\n",
              "      <td>69.73380</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>205.7292</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166441 rows × 53 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  timestamp  sensor_00  sensor_01  ...  sensor_50  sensor_51    RUL\n",
              "0       2018-04-01 00:00:00   2.465394  47.092010  ...   243.0556   201.3889  17155\n",
              "1       2018-04-01 00:01:00   2.465394  47.092010  ...   243.0556   201.3889  17154\n",
              "2       2018-04-01 00:02:00   2.444734  47.352430  ...   241.3194   203.7037  17153\n",
              "3       2018-04-01 00:03:00   2.460474  47.092010  ...   240.4514   203.1250  17152\n",
              "4       2018-04-01 00:04:00   2.445718  47.135410  ...   242.1875   201.3889  17151\n",
              "...                     ...        ...        ...  ...        ...        ...    ...\n",
              "166436  2018-07-25 13:56:00   2.313889  45.833330  ...  1000.0000   198.2060      4\n",
              "166437  2018-07-25 13:57:00   2.315856  45.833332  ...  1000.0000   202.8356      3\n",
              "166438  2018-07-25 13:58:00   2.322743  45.833330  ...  1000.0000   206.8866      2\n",
              "166439  2018-07-25 13:59:00   2.315856  45.789930  ...  1000.0000   209.7801      1\n",
              "166440  2018-07-25 14:00:00   2.318808  45.833332  ...  1000.0000   205.7292      0\n",
              "\n",
              "[166441 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbOabGb9bMxh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Multi Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3HfBku6vmqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "period1 = 1000\n",
        "period2 = 2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vniy7NAvvwMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.insert(53, \"lab_mc\", 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMLE8tFNvwcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(train_df)):\n",
        "    if(train_df[\"RUL\"][i] < period1):\n",
        "        train_df.at[i,'lab_mc'] = 2   \n",
        "    elif(train_df[\"RUL\"][i] < period2):\n",
        "        train_df.at[i,'lab_mc'] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC8u8bjMwxI0",
        "colab_type": "code",
        "outputId": "d8a3d84e-c053-4ced-bc9c-544f4880aba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        }
      },
      "source": [
        "train_df    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>sensor_00</th>\n",
              "      <th>sensor_01</th>\n",
              "      <th>sensor_02</th>\n",
              "      <th>sensor_03</th>\n",
              "      <th>sensor_04</th>\n",
              "      <th>sensor_05</th>\n",
              "      <th>sensor_06</th>\n",
              "      <th>sensor_07</th>\n",
              "      <th>sensor_08</th>\n",
              "      <th>sensor_09</th>\n",
              "      <th>sensor_10</th>\n",
              "      <th>sensor_11</th>\n",
              "      <th>sensor_12</th>\n",
              "      <th>sensor_13</th>\n",
              "      <th>sensor_14</th>\n",
              "      <th>sensor_16</th>\n",
              "      <th>sensor_17</th>\n",
              "      <th>sensor_18</th>\n",
              "      <th>sensor_19</th>\n",
              "      <th>sensor_20</th>\n",
              "      <th>sensor_21</th>\n",
              "      <th>sensor_22</th>\n",
              "      <th>sensor_23</th>\n",
              "      <th>sensor_24</th>\n",
              "      <th>sensor_25</th>\n",
              "      <th>sensor_26</th>\n",
              "      <th>sensor_27</th>\n",
              "      <th>sensor_28</th>\n",
              "      <th>sensor_29</th>\n",
              "      <th>sensor_30</th>\n",
              "      <th>sensor_31</th>\n",
              "      <th>sensor_32</th>\n",
              "      <th>sensor_33</th>\n",
              "      <th>sensor_34</th>\n",
              "      <th>sensor_35</th>\n",
              "      <th>sensor_36</th>\n",
              "      <th>sensor_37</th>\n",
              "      <th>sensor_38</th>\n",
              "      <th>sensor_39</th>\n",
              "      <th>sensor_40</th>\n",
              "      <th>sensor_41</th>\n",
              "      <th>sensor_42</th>\n",
              "      <th>sensor_43</th>\n",
              "      <th>sensor_44</th>\n",
              "      <th>sensor_45</th>\n",
              "      <th>sensor_46</th>\n",
              "      <th>sensor_47</th>\n",
              "      <th>sensor_48</th>\n",
              "      <th>sensor_49</th>\n",
              "      <th>sensor_50</th>\n",
              "      <th>sensor_51</th>\n",
              "      <th>RUL</th>\n",
              "      <th>lab_mc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-04-01 00:00:00</td>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17155</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-04-01 00:01:00</td>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17154</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-04-01 00:02:00</td>\n",
              "      <td>2.444734</td>\n",
              "      <td>47.352430</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397570</td>\n",
              "      <td>638.888900</td>\n",
              "      <td>73.54598</td>\n",
              "      <td>13.32465</td>\n",
              "      <td>16.03733</td>\n",
              "      <td>15.61777</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>37.86777</td>\n",
              "      <td>48.17723</td>\n",
              "      <td>32.08894</td>\n",
              "      <td>1.708474</td>\n",
              "      <td>420.8480</td>\n",
              "      <td>462.7798</td>\n",
              "      <td>459.6364</td>\n",
              "      <td>2.500062</td>\n",
              "      <td>666.2234</td>\n",
              "      <td>399.9418</td>\n",
              "      <td>880.4237</td>\n",
              "      <td>501.3617</td>\n",
              "      <td>982.7342</td>\n",
              "      <td>631.1326</td>\n",
              "      <td>740.8031</td>\n",
              "      <td>849.8997</td>\n",
              "      <td>454.2390</td>\n",
              "      <td>778.5734</td>\n",
              "      <td>715.6266</td>\n",
              "      <td>661.5740</td>\n",
              "      <td>721.8750</td>\n",
              "      <td>694.7721</td>\n",
              "      <td>441.2635</td>\n",
              "      <td>169.9820</td>\n",
              "      <td>343.1955</td>\n",
              "      <td>200.9694</td>\n",
              "      <td>93.90508</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>31.25000</td>\n",
              "      <td>69.53125</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.770830</td>\n",
              "      <td>41.66666</td>\n",
              "      <td>39.351852</td>\n",
              "      <td>65.39352</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194443</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>241.3194</td>\n",
              "      <td>203.7037</td>\n",
              "      <td>17153</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-04-01 00:03:00</td>\n",
              "      <td>2.460474</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.16840</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>628.125000</td>\n",
              "      <td>76.98898</td>\n",
              "      <td>13.31742</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>38.57977</td>\n",
              "      <td>48.65607</td>\n",
              "      <td>31.67221</td>\n",
              "      <td>1.579427</td>\n",
              "      <td>420.7494</td>\n",
              "      <td>462.8980</td>\n",
              "      <td>460.8858</td>\n",
              "      <td>2.509521</td>\n",
              "      <td>666.0114</td>\n",
              "      <td>399.1046</td>\n",
              "      <td>878.8917</td>\n",
              "      <td>499.0430</td>\n",
              "      <td>977.7520</td>\n",
              "      <td>625.4076</td>\n",
              "      <td>739.2722</td>\n",
              "      <td>847.7579</td>\n",
              "      <td>474.8731</td>\n",
              "      <td>779.5091</td>\n",
              "      <td>690.4011</td>\n",
              "      <td>686.1111</td>\n",
              "      <td>754.6875</td>\n",
              "      <td>683.3831</td>\n",
              "      <td>446.2493</td>\n",
              "      <td>166.4987</td>\n",
              "      <td>343.9586</td>\n",
              "      <td>193.1689</td>\n",
              "      <td>101.04060</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>72.13541</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>40.88541</td>\n",
              "      <td>39.062500</td>\n",
              "      <td>64.81481</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>66.84028</td>\n",
              "      <td>240.4514</td>\n",
              "      <td>203.1250</td>\n",
              "      <td>17152</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-04-01 00:04:00</td>\n",
              "      <td>2.445718</td>\n",
              "      <td>47.135410</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>636.458300</td>\n",
              "      <td>76.58897</td>\n",
              "      <td>13.35359</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>39.48939</td>\n",
              "      <td>49.06298</td>\n",
              "      <td>31.95202</td>\n",
              "      <td>1.683831</td>\n",
              "      <td>419.8926</td>\n",
              "      <td>461.4906</td>\n",
              "      <td>468.2206</td>\n",
              "      <td>2.604785</td>\n",
              "      <td>663.2111</td>\n",
              "      <td>400.5426</td>\n",
              "      <td>882.5874</td>\n",
              "      <td>498.5383</td>\n",
              "      <td>979.5755</td>\n",
              "      <td>627.1830</td>\n",
              "      <td>737.6033</td>\n",
              "      <td>846.9182</td>\n",
              "      <td>408.8159</td>\n",
              "      <td>785.2307</td>\n",
              "      <td>704.6937</td>\n",
              "      <td>631.4814</td>\n",
              "      <td>766.1458</td>\n",
              "      <td>702.4431</td>\n",
              "      <td>433.9081</td>\n",
              "      <td>164.7498</td>\n",
              "      <td>339.9630</td>\n",
              "      <td>193.8770</td>\n",
              "      <td>101.70380</td>\n",
              "      <td>42.70833</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>76.82291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>65.10416</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>158.2755</td>\n",
              "      <td>66.55093</td>\n",
              "      <td>242.1875</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17151</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166436</th>\n",
              "      <td>2018-07-25 13:56:00</td>\n",
              "      <td>2.313889</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>71.464119</td>\n",
              "      <td>73.00980</td>\n",
              "      <td>14.36632</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.12587</td>\n",
              "      <td>38.34975</td>\n",
              "      <td>49.88500</td>\n",
              "      <td>31.94489</td>\n",
              "      <td>12.285810</td>\n",
              "      <td>421.4294</td>\n",
              "      <td>458.9118</td>\n",
              "      <td>456.3463</td>\n",
              "      <td>2.459503</td>\n",
              "      <td>666.8698</td>\n",
              "      <td>398.0684</td>\n",
              "      <td>879.7014</td>\n",
              "      <td>536.2166</td>\n",
              "      <td>1091.1240</td>\n",
              "      <td>629.8835</td>\n",
              "      <td>740.9833</td>\n",
              "      <td>981.7558</td>\n",
              "      <td>497.2223</td>\n",
              "      <td>995.5433</td>\n",
              "      <td>563.5882</td>\n",
              "      <td>676.8519</td>\n",
              "      <td>949.9999</td>\n",
              "      <td>990.9057</td>\n",
              "      <td>564.7968</td>\n",
              "      <td>342.3690</td>\n",
              "      <td>498.5037</td>\n",
              "      <td>804.9069</td>\n",
              "      <td>55.82830</td>\n",
              "      <td>53.38541</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>93.48958</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>45.138890</td>\n",
              "      <td>252.3148</td>\n",
              "      <td>69.15509</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>198.2060</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166437</th>\n",
              "      <td>2018-07-25 13:57:00</td>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923611</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>72.47576</td>\n",
              "      <td>14.33015</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.76968</td>\n",
              "      <td>15.16204</td>\n",
              "      <td>38.58192</td>\n",
              "      <td>49.96759</td>\n",
              "      <td>31.46310</td>\n",
              "      <td>12.213550</td>\n",
              "      <td>420.6030</td>\n",
              "      <td>460.6981</td>\n",
              "      <td>467.5996</td>\n",
              "      <td>2.603880</td>\n",
              "      <td>665.2674</td>\n",
              "      <td>399.1180</td>\n",
              "      <td>879.2195</td>\n",
              "      <td>534.4346</td>\n",
              "      <td>1089.8550</td>\n",
              "      <td>628.6547</td>\n",
              "      <td>738.9200</td>\n",
              "      <td>979.7875</td>\n",
              "      <td>504.8127</td>\n",
              "      <td>1004.9950</td>\n",
              "      <td>569.5920</td>\n",
              "      <td>719.9074</td>\n",
              "      <td>934.3749</td>\n",
              "      <td>982.0567</td>\n",
              "      <td>559.5000</td>\n",
              "      <td>345.2228</td>\n",
              "      <td>500.3699</td>\n",
              "      <td>804.2908</td>\n",
              "      <td>49.17398</td>\n",
              "      <td>52.60416</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>95.57291</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>253.1829</td>\n",
              "      <td>71.46991</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>202.8356</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166438</th>\n",
              "      <td>2018-07-25 13:58:00</td>\n",
              "      <td>2.322743</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>70.48889</td>\n",
              "      <td>14.46036</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.73351</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>38.87765</td>\n",
              "      <td>49.75908</td>\n",
              "      <td>31.78839</td>\n",
              "      <td>12.180410</td>\n",
              "      <td>419.6096</td>\n",
              "      <td>462.2329</td>\n",
              "      <td>466.0083</td>\n",
              "      <td>2.577086</td>\n",
              "      <td>667.1351</td>\n",
              "      <td>398.3854</td>\n",
              "      <td>881.7244</td>\n",
              "      <td>536.0204</td>\n",
              "      <td>1095.5440</td>\n",
              "      <td>630.5479</td>\n",
              "      <td>743.5366</td>\n",
              "      <td>981.5860</td>\n",
              "      <td>488.0563</td>\n",
              "      <td>1009.6940</td>\n",
              "      <td>559.3523</td>\n",
              "      <td>678.2407</td>\n",
              "      <td>947.9166</td>\n",
              "      <td>1016.9990</td>\n",
              "      <td>558.1667</td>\n",
              "      <td>344.4498</td>\n",
              "      <td>490.6038</td>\n",
              "      <td>796.3863</td>\n",
              "      <td>39.34733</td>\n",
              "      <td>51.82291</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>100.52080</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>49.73958</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>52.08333</td>\n",
              "      <td>61.34259</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>249.1319</td>\n",
              "      <td>72.33796</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>206.8866</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166439</th>\n",
              "      <td>2018-07-25 13:59:00</td>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.789930</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>68.74365</td>\n",
              "      <td>14.43866</td>\n",
              "      <td>16.07350</td>\n",
              "      <td>15.53096</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>38.21849</td>\n",
              "      <td>49.73039</td>\n",
              "      <td>32.32672</td>\n",
              "      <td>12.209670</td>\n",
              "      <td>420.0334</td>\n",
              "      <td>460.0883</td>\n",
              "      <td>460.4448</td>\n",
              "      <td>2.509833</td>\n",
              "      <td>664.8566</td>\n",
              "      <td>399.6012</td>\n",
              "      <td>880.7674</td>\n",
              "      <td>535.4742</td>\n",
              "      <td>1092.3240</td>\n",
              "      <td>628.6292</td>\n",
              "      <td>743.1452</td>\n",
              "      <td>982.4372</td>\n",
              "      <td>511.7988</td>\n",
              "      <td>1025.1100</td>\n",
              "      <td>559.1011</td>\n",
              "      <td>743.5185</td>\n",
              "      <td>960.4166</td>\n",
              "      <td>998.0515</td>\n",
              "      <td>551.9908</td>\n",
              "      <td>339.3840</td>\n",
              "      <td>492.3628</td>\n",
              "      <td>796.2476</td>\n",
              "      <td>53.73619</td>\n",
              "      <td>51.04166</td>\n",
              "      <td>40.62500</td>\n",
              "      <td>104.68750</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>50.52083</td>\n",
              "      <td>52.662040</td>\n",
              "      <td>53.24074</td>\n",
              "      <td>63.36805</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>244.5023</td>\n",
              "      <td>72.62731</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>209.7801</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166440</th>\n",
              "      <td>2018-07-25 14:00:00</td>\n",
              "      <td>2.318808</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>420.503448</td>\n",
              "      <td>72.52040</td>\n",
              "      <td>14.18547</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>36.71748</td>\n",
              "      <td>50.04619</td>\n",
              "      <td>30.74288</td>\n",
              "      <td>12.217860</td>\n",
              "      <td>420.8600</td>\n",
              "      <td>463.5181</td>\n",
              "      <td>455.2702</td>\n",
              "      <td>2.468381</td>\n",
              "      <td>665.9402</td>\n",
              "      <td>399.0930</td>\n",
              "      <td>880.8410</td>\n",
              "      <td>531.8727</td>\n",
              "      <td>1093.5790</td>\n",
              "      <td>626.4651</td>\n",
              "      <td>740.6738</td>\n",
              "      <td>981.4561</td>\n",
              "      <td>490.7728</td>\n",
              "      <td>1023.1660</td>\n",
              "      <td>551.2756</td>\n",
              "      <td>703.7037</td>\n",
              "      <td>971.8749</td>\n",
              "      <td>1013.3760</td>\n",
              "      <td>546.6656</td>\n",
              "      <td>363.0320</td>\n",
              "      <td>522.5594</td>\n",
              "      <td>812.8298</td>\n",
              "      <td>44.77601</td>\n",
              "      <td>50.00000</td>\n",
              "      <td>39.06250</td>\n",
              "      <td>102.86460</td>\n",
              "      <td>35.416660</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>51.30208</td>\n",
              "      <td>52.102000</td>\n",
              "      <td>52.66204</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>43.981480</td>\n",
              "      <td>230.3241</td>\n",
              "      <td>69.73380</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>205.7292</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166441 rows × 54 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  timestamp  sensor_00  sensor_01  ...  sensor_51    RUL  lab_mc\n",
              "0       2018-04-01 00:00:00   2.465394  47.092010  ...   201.3889  17155       0\n",
              "1       2018-04-01 00:01:00   2.465394  47.092010  ...   201.3889  17154       0\n",
              "2       2018-04-01 00:02:00   2.444734  47.352430  ...   203.7037  17153       0\n",
              "3       2018-04-01 00:03:00   2.460474  47.092010  ...   203.1250  17152       0\n",
              "4       2018-04-01 00:04:00   2.445718  47.135410  ...   201.3889  17151       0\n",
              "...                     ...        ...        ...  ...        ...    ...     ...\n",
              "166436  2018-07-25 13:56:00   2.313889  45.833330  ...   198.2060      4       2\n",
              "166437  2018-07-25 13:57:00   2.315856  45.833332  ...   202.8356      3       2\n",
              "166438  2018-07-25 13:58:00   2.322743  45.833330  ...   206.8866      2       2\n",
              "166439  2018-07-25 13:59:00   2.315856  45.789930  ...   209.7801      1       2\n",
              "166440  2018-07-25 14:00:00   2.318808  45.833332  ...   205.7292      0       2\n",
              "\n",
              "[166441 rows x 54 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0C5MhmYw5et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn import model_selection\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "\n",
        "from sklearn.metrics import roc_auc_score,roc_curve\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import GRU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqgNFK4dbwrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = train_df['lab_bin']\n",
        "X = train_df.iloc[:, 1:52]\n",
        "X.head()\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.6, random_state = seed(20))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTcUS0cMfWX3",
        "colab_type": "text"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-dmh49Dgd6j",
        "colab_type": "code",
        "outputId": "5d8ae657-2d27-4980-98c4-38075c729d67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# dataset = np.column_stack((X, Y))\n",
        "train_df.drop(['timestamp'], axis=1, inplace=True)\n",
        "train_df.drop(['RUL'], axis=1, inplace=True)\n",
        "\n",
        "dataset = train_df.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb88ymMYcO2V",
        "colab_type": "code",
        "outputId": "0d8cf2c6-5479-4a92-dd64-813e9482c2d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "train_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sensor_00</th>\n",
              "      <th>sensor_01</th>\n",
              "      <th>sensor_02</th>\n",
              "      <th>sensor_03</th>\n",
              "      <th>sensor_04</th>\n",
              "      <th>sensor_05</th>\n",
              "      <th>sensor_06</th>\n",
              "      <th>sensor_07</th>\n",
              "      <th>sensor_08</th>\n",
              "      <th>sensor_09</th>\n",
              "      <th>sensor_10</th>\n",
              "      <th>sensor_11</th>\n",
              "      <th>sensor_12</th>\n",
              "      <th>sensor_13</th>\n",
              "      <th>sensor_14</th>\n",
              "      <th>sensor_16</th>\n",
              "      <th>sensor_17</th>\n",
              "      <th>sensor_18</th>\n",
              "      <th>sensor_19</th>\n",
              "      <th>sensor_20</th>\n",
              "      <th>sensor_21</th>\n",
              "      <th>sensor_22</th>\n",
              "      <th>sensor_23</th>\n",
              "      <th>sensor_24</th>\n",
              "      <th>sensor_25</th>\n",
              "      <th>sensor_26</th>\n",
              "      <th>sensor_27</th>\n",
              "      <th>sensor_28</th>\n",
              "      <th>sensor_29</th>\n",
              "      <th>sensor_30</th>\n",
              "      <th>sensor_31</th>\n",
              "      <th>sensor_32</th>\n",
              "      <th>sensor_33</th>\n",
              "      <th>sensor_34</th>\n",
              "      <th>sensor_35</th>\n",
              "      <th>sensor_36</th>\n",
              "      <th>sensor_37</th>\n",
              "      <th>sensor_38</th>\n",
              "      <th>sensor_39</th>\n",
              "      <th>sensor_40</th>\n",
              "      <th>sensor_41</th>\n",
              "      <th>sensor_42</th>\n",
              "      <th>sensor_43</th>\n",
              "      <th>sensor_44</th>\n",
              "      <th>sensor_45</th>\n",
              "      <th>sensor_46</th>\n",
              "      <th>sensor_47</th>\n",
              "      <th>sensor_48</th>\n",
              "      <th>sensor_49</th>\n",
              "      <th>sensor_50</th>\n",
              "      <th>sensor_51</th>\n",
              "      <th>lab_mc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.444734</td>\n",
              "      <td>47.352430</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397570</td>\n",
              "      <td>638.888900</td>\n",
              "      <td>73.54598</td>\n",
              "      <td>13.32465</td>\n",
              "      <td>16.03733</td>\n",
              "      <td>15.61777</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>37.86777</td>\n",
              "      <td>48.17723</td>\n",
              "      <td>32.08894</td>\n",
              "      <td>1.708474</td>\n",
              "      <td>420.8480</td>\n",
              "      <td>462.7798</td>\n",
              "      <td>459.6364</td>\n",
              "      <td>2.500062</td>\n",
              "      <td>666.2234</td>\n",
              "      <td>399.9418</td>\n",
              "      <td>880.4237</td>\n",
              "      <td>501.3617</td>\n",
              "      <td>982.7342</td>\n",
              "      <td>631.1326</td>\n",
              "      <td>740.8031</td>\n",
              "      <td>849.8997</td>\n",
              "      <td>454.2390</td>\n",
              "      <td>778.5734</td>\n",
              "      <td>715.6266</td>\n",
              "      <td>661.5740</td>\n",
              "      <td>721.8750</td>\n",
              "      <td>694.7721</td>\n",
              "      <td>441.2635</td>\n",
              "      <td>169.9820</td>\n",
              "      <td>343.1955</td>\n",
              "      <td>200.9694</td>\n",
              "      <td>93.90508</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>31.25000</td>\n",
              "      <td>69.53125</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.770830</td>\n",
              "      <td>41.66666</td>\n",
              "      <td>39.351852</td>\n",
              "      <td>65.39352</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194443</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>241.3194</td>\n",
              "      <td>203.7037</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.460474</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.16840</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>628.125000</td>\n",
              "      <td>76.98898</td>\n",
              "      <td>13.31742</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>38.57977</td>\n",
              "      <td>48.65607</td>\n",
              "      <td>31.67221</td>\n",
              "      <td>1.579427</td>\n",
              "      <td>420.7494</td>\n",
              "      <td>462.8980</td>\n",
              "      <td>460.8858</td>\n",
              "      <td>2.509521</td>\n",
              "      <td>666.0114</td>\n",
              "      <td>399.1046</td>\n",
              "      <td>878.8917</td>\n",
              "      <td>499.0430</td>\n",
              "      <td>977.7520</td>\n",
              "      <td>625.4076</td>\n",
              "      <td>739.2722</td>\n",
              "      <td>847.7579</td>\n",
              "      <td>474.8731</td>\n",
              "      <td>779.5091</td>\n",
              "      <td>690.4011</td>\n",
              "      <td>686.1111</td>\n",
              "      <td>754.6875</td>\n",
              "      <td>683.3831</td>\n",
              "      <td>446.2493</td>\n",
              "      <td>166.4987</td>\n",
              "      <td>343.9586</td>\n",
              "      <td>193.1689</td>\n",
              "      <td>101.04060</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>72.13541</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>40.88541</td>\n",
              "      <td>39.062500</td>\n",
              "      <td>64.81481</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>66.84028</td>\n",
              "      <td>240.4514</td>\n",
              "      <td>203.1250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.445718</td>\n",
              "      <td>47.135410</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>636.458300</td>\n",
              "      <td>76.58897</td>\n",
              "      <td>13.35359</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>39.48939</td>\n",
              "      <td>49.06298</td>\n",
              "      <td>31.95202</td>\n",
              "      <td>1.683831</td>\n",
              "      <td>419.8926</td>\n",
              "      <td>461.4906</td>\n",
              "      <td>468.2206</td>\n",
              "      <td>2.604785</td>\n",
              "      <td>663.2111</td>\n",
              "      <td>400.5426</td>\n",
              "      <td>882.5874</td>\n",
              "      <td>498.5383</td>\n",
              "      <td>979.5755</td>\n",
              "      <td>627.1830</td>\n",
              "      <td>737.6033</td>\n",
              "      <td>846.9182</td>\n",
              "      <td>408.8159</td>\n",
              "      <td>785.2307</td>\n",
              "      <td>704.6937</td>\n",
              "      <td>631.4814</td>\n",
              "      <td>766.1458</td>\n",
              "      <td>702.4431</td>\n",
              "      <td>433.9081</td>\n",
              "      <td>164.7498</td>\n",
              "      <td>339.9630</td>\n",
              "      <td>193.8770</td>\n",
              "      <td>101.70380</td>\n",
              "      <td>42.70833</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>76.82291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>65.10416</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>158.2755</td>\n",
              "      <td>66.55093</td>\n",
              "      <td>242.1875</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166436</th>\n",
              "      <td>2.313889</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>71.464119</td>\n",
              "      <td>73.00980</td>\n",
              "      <td>14.36632</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.12587</td>\n",
              "      <td>38.34975</td>\n",
              "      <td>49.88500</td>\n",
              "      <td>31.94489</td>\n",
              "      <td>12.285810</td>\n",
              "      <td>421.4294</td>\n",
              "      <td>458.9118</td>\n",
              "      <td>456.3463</td>\n",
              "      <td>2.459503</td>\n",
              "      <td>666.8698</td>\n",
              "      <td>398.0684</td>\n",
              "      <td>879.7014</td>\n",
              "      <td>536.2166</td>\n",
              "      <td>1091.1240</td>\n",
              "      <td>629.8835</td>\n",
              "      <td>740.9833</td>\n",
              "      <td>981.7558</td>\n",
              "      <td>497.2223</td>\n",
              "      <td>995.5433</td>\n",
              "      <td>563.5882</td>\n",
              "      <td>676.8519</td>\n",
              "      <td>949.9999</td>\n",
              "      <td>990.9057</td>\n",
              "      <td>564.7968</td>\n",
              "      <td>342.3690</td>\n",
              "      <td>498.5037</td>\n",
              "      <td>804.9069</td>\n",
              "      <td>55.82830</td>\n",
              "      <td>53.38541</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>93.48958</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>45.138890</td>\n",
              "      <td>252.3148</td>\n",
              "      <td>69.15509</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>198.2060</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166437</th>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923611</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>72.47576</td>\n",
              "      <td>14.33015</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.76968</td>\n",
              "      <td>15.16204</td>\n",
              "      <td>38.58192</td>\n",
              "      <td>49.96759</td>\n",
              "      <td>31.46310</td>\n",
              "      <td>12.213550</td>\n",
              "      <td>420.6030</td>\n",
              "      <td>460.6981</td>\n",
              "      <td>467.5996</td>\n",
              "      <td>2.603880</td>\n",
              "      <td>665.2674</td>\n",
              "      <td>399.1180</td>\n",
              "      <td>879.2195</td>\n",
              "      <td>534.4346</td>\n",
              "      <td>1089.8550</td>\n",
              "      <td>628.6547</td>\n",
              "      <td>738.9200</td>\n",
              "      <td>979.7875</td>\n",
              "      <td>504.8127</td>\n",
              "      <td>1004.9950</td>\n",
              "      <td>569.5920</td>\n",
              "      <td>719.9074</td>\n",
              "      <td>934.3749</td>\n",
              "      <td>982.0567</td>\n",
              "      <td>559.5000</td>\n",
              "      <td>345.2228</td>\n",
              "      <td>500.3699</td>\n",
              "      <td>804.2908</td>\n",
              "      <td>49.17398</td>\n",
              "      <td>52.60416</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>95.57291</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>253.1829</td>\n",
              "      <td>71.46991</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>202.8356</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166438</th>\n",
              "      <td>2.322743</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>70.48889</td>\n",
              "      <td>14.46036</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.73351</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>38.87765</td>\n",
              "      <td>49.75908</td>\n",
              "      <td>31.78839</td>\n",
              "      <td>12.180410</td>\n",
              "      <td>419.6096</td>\n",
              "      <td>462.2329</td>\n",
              "      <td>466.0083</td>\n",
              "      <td>2.577086</td>\n",
              "      <td>667.1351</td>\n",
              "      <td>398.3854</td>\n",
              "      <td>881.7244</td>\n",
              "      <td>536.0204</td>\n",
              "      <td>1095.5440</td>\n",
              "      <td>630.5479</td>\n",
              "      <td>743.5366</td>\n",
              "      <td>981.5860</td>\n",
              "      <td>488.0563</td>\n",
              "      <td>1009.6940</td>\n",
              "      <td>559.3523</td>\n",
              "      <td>678.2407</td>\n",
              "      <td>947.9166</td>\n",
              "      <td>1016.9990</td>\n",
              "      <td>558.1667</td>\n",
              "      <td>344.4498</td>\n",
              "      <td>490.6038</td>\n",
              "      <td>796.3863</td>\n",
              "      <td>39.34733</td>\n",
              "      <td>51.82291</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>100.52080</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>49.73958</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>52.08333</td>\n",
              "      <td>61.34259</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>249.1319</td>\n",
              "      <td>72.33796</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>206.8866</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166439</th>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.789930</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>68.74365</td>\n",
              "      <td>14.43866</td>\n",
              "      <td>16.07350</td>\n",
              "      <td>15.53096</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>38.21849</td>\n",
              "      <td>49.73039</td>\n",
              "      <td>32.32672</td>\n",
              "      <td>12.209670</td>\n",
              "      <td>420.0334</td>\n",
              "      <td>460.0883</td>\n",
              "      <td>460.4448</td>\n",
              "      <td>2.509833</td>\n",
              "      <td>664.8566</td>\n",
              "      <td>399.6012</td>\n",
              "      <td>880.7674</td>\n",
              "      <td>535.4742</td>\n",
              "      <td>1092.3240</td>\n",
              "      <td>628.6292</td>\n",
              "      <td>743.1452</td>\n",
              "      <td>982.4372</td>\n",
              "      <td>511.7988</td>\n",
              "      <td>1025.1100</td>\n",
              "      <td>559.1011</td>\n",
              "      <td>743.5185</td>\n",
              "      <td>960.4166</td>\n",
              "      <td>998.0515</td>\n",
              "      <td>551.9908</td>\n",
              "      <td>339.3840</td>\n",
              "      <td>492.3628</td>\n",
              "      <td>796.2476</td>\n",
              "      <td>53.73619</td>\n",
              "      <td>51.04166</td>\n",
              "      <td>40.62500</td>\n",
              "      <td>104.68750</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>50.52083</td>\n",
              "      <td>52.662040</td>\n",
              "      <td>53.24074</td>\n",
              "      <td>63.36805</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>244.5023</td>\n",
              "      <td>72.62731</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>209.7801</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166440</th>\n",
              "      <td>2.318808</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>420.503448</td>\n",
              "      <td>72.52040</td>\n",
              "      <td>14.18547</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>36.71748</td>\n",
              "      <td>50.04619</td>\n",
              "      <td>30.74288</td>\n",
              "      <td>12.217860</td>\n",
              "      <td>420.8600</td>\n",
              "      <td>463.5181</td>\n",
              "      <td>455.2702</td>\n",
              "      <td>2.468381</td>\n",
              "      <td>665.9402</td>\n",
              "      <td>399.0930</td>\n",
              "      <td>880.8410</td>\n",
              "      <td>531.8727</td>\n",
              "      <td>1093.5790</td>\n",
              "      <td>626.4651</td>\n",
              "      <td>740.6738</td>\n",
              "      <td>981.4561</td>\n",
              "      <td>490.7728</td>\n",
              "      <td>1023.1660</td>\n",
              "      <td>551.2756</td>\n",
              "      <td>703.7037</td>\n",
              "      <td>971.8749</td>\n",
              "      <td>1013.3760</td>\n",
              "      <td>546.6656</td>\n",
              "      <td>363.0320</td>\n",
              "      <td>522.5594</td>\n",
              "      <td>812.8298</td>\n",
              "      <td>44.77601</td>\n",
              "      <td>50.00000</td>\n",
              "      <td>39.06250</td>\n",
              "      <td>102.86460</td>\n",
              "      <td>35.416660</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>51.30208</td>\n",
              "      <td>52.102000</td>\n",
              "      <td>52.66204</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>43.981480</td>\n",
              "      <td>230.3241</td>\n",
              "      <td>69.73380</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>205.7292</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166441 rows × 52 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sensor_00  sensor_01  sensor_02  ...  sensor_50  sensor_51  lab_mc\n",
              "0        2.465394  47.092010   53.21180  ...   243.0556   201.3889       0\n",
              "1        2.465394  47.092010   53.21180  ...   243.0556   201.3889       0\n",
              "2        2.444734  47.352430   53.21180  ...   241.3194   203.7037       0\n",
              "3        2.460474  47.092010   53.16840  ...   240.4514   203.1250       0\n",
              "4        2.445718  47.135410   53.21180  ...   242.1875   201.3889       0\n",
              "...           ...        ...        ...  ...        ...        ...     ...\n",
              "166436   2.313889  45.833330   53.03819  ...  1000.0000   198.2060       2\n",
              "166437   2.315856  45.833332   53.03819  ...  1000.0000   202.8356       2\n",
              "166438   2.322743  45.833330   52.99479  ...  1000.0000   206.8866       2\n",
              "166439   2.315856  45.789930   53.03819  ...  1000.0000   209.7801       2\n",
              "166440   2.318808  45.833332   52.99479  ...  1000.0000   205.7292       2\n",
              "\n",
              "[166441 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnfDQsiOfYfE",
        "colab_type": "code",
        "outputId": "badb43cf-c968-4c52-e31f-a42657c6c908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# multivariate data preparation\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "\n",
        "# split a multivariate sequence into samples\n",
        "def split_sequences(sequences, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequences)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the dataset\n",
        "\t\tif end_ix > len(sequences):\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)\n",
        "\n",
        "# define input sequence\n",
        "#print(dataset)\n",
        "# choose a number of time steps\n",
        "n_steps = 10\n",
        "\n",
        "# convert into input/output\n",
        "\n",
        "x, y = split_sequences(dataset, n_steps)\n",
        "#X, y = split_sequences(, n_steps)\n",
        "print(x.shape, y.shape)\n",
        "# summarize the data\n",
        "# for i in range(len(X)):\n",
        "# \tprint(X[i], y[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(166432, 10, 51) (166432,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk9xy3__18K0",
        "colab_type": "code",
        "outputId": "c115028f-b63e-4bbf-a872-3c97b411bb8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "166441"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwGEQVRmhobp",
        "colab_type": "code",
        "outputId": "a7a5f929-f16d-48a1-bb63-84285694d0aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random import seed\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size = 0.6, random_state = seed(20))\n",
        "print(X_train)\n",
        "print(\"=================================\")\n",
        "print(Y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[2.49982600e+00 4.60503500e+01 5.32552100e+01 ... 6.10532400e+01\n",
            "   1.00000000e+03 1.84317100e+02]\n",
            "  [2.51064800e+00 4.60937500e+01 5.33854100e+01 ... 5.96064800e+01\n",
            "   1.00000000e+03 1.84027800e+02]\n",
            "  [2.49982600e+00 4.60937500e+01 5.33420100e+01 ... 5.81597214e+01\n",
            "   1.00000000e+03 1.83449100e+02]\n",
            "  ...\n",
            "  [2.51064800e+00 4.60503500e+01 5.33854100e+01 ... 5.26620400e+01\n",
            "   1.00000000e+03 1.75636600e+02]\n",
            "  [2.49982600e+00 4.61805500e+01 5.33854141e+01 ... 5.23726800e+01\n",
            "   1.00000000e+03 1.72453700e+02]\n",
            "  [2.51064800e+00 4.61805500e+01 5.33854100e+01 ... 5.17939800e+01\n",
            "   1.00000000e+03 1.68981500e+02]]\n",
            "\n",
            " [[8.52297861e-02 3.95833321e+01 3.91493034e+01 ... 3.29861107e+01\n",
            "   3.38541679e+01 1.00000000e+03]\n",
            "  [8.52108451e-02 3.95833321e+01 3.91493034e+01 ... 3.29861107e+01\n",
            "   3.38541679e+01 1.00000000e+03]\n",
            "  [8.51919042e-02 3.95833300e+01 3.91493000e+01 ... 3.29861107e+01\n",
            "   3.38541679e+01 1.00000000e+03]\n",
            "  ...\n",
            "  [8.50971994e-02 3.95399284e+01 3.92361107e+01 ... 3.29861107e+01\n",
            "   3.38541679e+01 1.00000000e+03]\n",
            "  [8.50782584e-02 3.95399284e+01 3.92361107e+01 ... 3.29861107e+01\n",
            "   3.38541679e+01 1.00000000e+03]\n",
            "  [8.50593174e-02 3.95399300e+01 3.92361107e+01 ... 3.29861107e+01\n",
            "   3.38541679e+01 1.00000000e+03]]\n",
            "\n",
            " [[2.39259300e+00 4.51388900e+01 5.02170100e+01 ... 4.83217600e+01\n",
            "   1.95601900e+02 1.78530100e+02]\n",
            "  [2.39652800e+00 4.51388900e+01 5.02170100e+01 ... 4.80324100e+01\n",
            "   1.95023100e+02 1.77662000e+02]\n",
            "  [2.40735000e+00 4.50954900e+01 5.03038200e+01 ... 4.77430600e+01\n",
            "   1.95023100e+02 1.76504600e+02]\n",
            "  ...\n",
            "  [2.40735000e+00 4.50954900e+01 5.02170100e+01 ... 4.77430600e+01\n",
            "   1.97916700e+02 1.72743100e+02]\n",
            "  [2.39554400e+00 4.50954900e+01 5.02170100e+01 ... 4.74537000e+01\n",
            "   1.95312500e+02 1.73321800e+02]\n",
            "  [2.39652800e+00 4.51388900e+01 5.01736100e+01 ... 4.74537000e+01\n",
            "   1.92129600e+02 1.72743100e+02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[2.35225700e+00 4.35763900e+01 4.76128500e+01 ... 4.48495400e+01\n",
            "   1.78240700e+02 1.76215300e+02]\n",
            "  [2.35619200e+00 4.34895800e+01 4.75694400e+01 ... 4.48495400e+01\n",
            "   1.76215300e+02 1.69849500e+02]\n",
            "  [2.35127300e+00 4.35763900e+01 4.76128500e+01 ... 4.45601800e+01\n",
            "   1.72743100e+02 1.64641200e+02]\n",
            "  ...\n",
            "  [2.35816000e+00 4.34895800e+01 4.75694427e+01 ... 4.36921300e+01\n",
            "   1.59432900e+02 1.58854200e+02]\n",
            "  [2.34832200e+00 4.34895800e+01 4.75694427e+01 ... 4.34027786e+01\n",
            "   1.58275500e+02 1.59722200e+02]\n",
            "  [2.35520800e+00 4.34895800e+01 4.75694427e+01 ... 4.31134300e+01\n",
            "   1.57407400e+02 1.59722200e+02]]\n",
            "\n",
            " [[2.44079900e+00 4.67447900e+01 5.28211784e+01 ... 8.56481500e+01\n",
            "   2.92245400e+02 2.44213000e+02]\n",
            "  [2.42997700e+00 4.68750000e+01 5.28211784e+01 ... 8.79629600e+01\n",
            "   2.87615800e+02 2.45949100e+02]\n",
            "  [2.43883100e+00 4.68316000e+01 5.28211784e+01 ... 8.94097200e+01\n",
            "   2.84722200e+02 2.47106500e+02]\n",
            "  ...\n",
            "  [2.44079900e+00 4.68750000e+01 5.28211800e+01 ... 8.59375000e+01\n",
            "   2.80960600e+02 2.46527800e+02]\n",
            "  [2.43096100e+00 4.68750000e+01 5.29079900e+01 ... 8.65162000e+01\n",
            "   2.81828700e+02 2.47106500e+02]\n",
            "  [2.44079900e+00 4.68750000e+01 5.28211800e+01 ... 8.53588000e+01\n",
            "   2.83564800e+02 2.47685200e+02]]\n",
            "\n",
            " [[2.45162000e+00 4.96527700e+01 5.32552100e+01 ... 4.77430573e+01\n",
            "   1.74768500e+02 1.75057900e+02]\n",
            "  [2.45949100e+00 4.96527700e+01 5.32552100e+01 ... 4.77430573e+01\n",
            "   1.75057900e+02 1.77662000e+02]\n",
            "  [2.45162000e+00 4.96961800e+01 5.32552100e+01 ... 4.77430600e+01\n",
            "   1.75057900e+02 1.81134300e+02]\n",
            "  ...\n",
            "  [2.45555600e+00 5.00868000e+01 5.32552100e+01 ... 4.68750000e+01\n",
            "   1.74768500e+02 1.80844900e+02]\n",
            "  [2.45457200e+00 4.99566000e+01 5.33420100e+01 ... 4.68750000e+01\n",
            "   1.74768500e+02 1.75636600e+02]\n",
            "  [2.45162000e+00 4.99131900e+01 5.33420105e+01 ... 4.65856500e+01\n",
            "   1.74189800e+02 1.70717600e+02]]]\n",
            "=================================\n",
            "[0. 0. 0. ... 2. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPVBtxPZ15mt",
        "colab_type": "code",
        "outputId": "8c7ada6b-2b2a-4a5b-fd3a-ec6a09105d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(X_test[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtB0hHBw1hv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "from google.colab import files\n",
        "filename = \"test_X_dataset_ret_0.csv\"\n",
        "i = 0\n",
        "with open(filename, 'w') as csvfile: \n",
        "  for i in range(10):\n",
        "      csvwriter = csv.writer(csvfile)  \n",
        "      csvwriter.writerow(X_test[0][i])  \n",
        "      i += 1\n",
        "files.download('test_X_dataset_ret_0.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trqOrinnTeNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from numpy import hstack\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l1\n",
        "from keras.layers import LeakyReLU\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iiVksr6hDfb",
        "colab_type": "code",
        "outputId": "a697a29b-e80b-44fd-f36f-5766ae621b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "\n",
        "\n",
        "# class_weights = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\n",
        "# print(class_weights)\n",
        "# class_weight_dict = dict(enumerate(class_weight))\n",
        "# print(class_weight_dict)\n",
        "\n",
        "#class_weights = {0:1, 1:50}\n",
        "weights = {0:1, 1:100, 2:100}\n",
        "n_features = X_train.shape[2]\n",
        "print(n_features)\n",
        "# y_bin = to_categorical(Y_train)\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, return_sequences= True, input_shape=(n_steps, n_features)))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(LSTM(75, return_sequences= True))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(LSTM(50))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "opt = SGD(lr=0.05, momentum=0.9, clipnorm=1.0, clipvalue=5.0)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "# model.compile(optimizer='adam', loss='mse',\n",
        "             # metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X_train, Y_train, epochs=5, verbose=1, class_weight=weights)\n",
        "# demonstrate prediction\n",
        "#yhat = model.predict(X_test, verbose=0)\n",
        "#print(yhat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51\n",
            "Epoch 1/5\n",
            "99859/99859 [==============================] - 80s 798us/step - loss: 8.4004 - accuracy: 0.1656\n",
            "Epoch 2/5\n",
            "99859/99859 [==============================] - 79s 792us/step - loss: 8.2121 - accuracy: 0.1603\n",
            "Epoch 3/5\n",
            "99859/99859 [==============================] - 78s 785us/step - loss: 8.2166 - accuracy: 0.1615\n",
            "Epoch 4/5\n",
            "99859/99859 [==============================] - 78s 782us/step - loss: 8.0780 - accuracy: 0.1637\n",
            "Epoch 5/5\n",
            "99859/99859 [==============================] - 78s 778us/step - loss: 8.0774 - accuracy: 0.1702\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fddd86d8fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ier6I4_XnbL3",
        "colab_type": "text"
      },
      "source": [
        "#pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ8DNKyznd81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCSdK4S9nhkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'lstm_multi_class_modelsEPOCH10_new.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLCn3QmhnrB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('lstm_multi_class_modelsEPOCH10_new.sav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP8YsR5Mni8x",
        "colab_type": "text"
      },
      "source": [
        "#predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DxKhtusrAVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ypred = model.predict_classes(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ri9T7CwJeV4",
        "colab_type": "code",
        "outputId": "305d61bc-de9c-4b52-ef95-b4fe3c01dd25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ypred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cqsR_85LqOW",
        "colab_type": "code",
        "outputId": "64a47712-324f-4a81-9ea1-b20ad278c026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i in range(60000, len(ypred)):\n",
        "  if ypred[i] == 1:\n",
        "    print(i)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b13wtfF5LqXU",
        "colab_type": "code",
        "outputId": "d3e834bd-f033-4fc8-93cc-172a75f864d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_test[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNUXfWhCLqVj",
        "colab_type": "code",
        "outputId": "9c9f39bd-4a60-4c5c-f0b1-5f43cbea39ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i in range(len(Y_test)):\n",
        "  if Y_test[i] == 1:\n",
        "    print(i)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZDYQneZLqSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9UH4quAKUWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calcp(a):\n",
        "  l=[]\n",
        "  for j in range(3):\n",
        "    summ = 0\n",
        "    for i in range(3):\n",
        "      summ = summ + a[i][j]\n",
        "    l.append(a[j][j]/summ)\n",
        "  return l\n",
        "\n",
        "def calcr(a):\n",
        "  l=[]\n",
        "  for i in range(3):\n",
        "    summ = 0\n",
        "    for j in range(3):\n",
        "      summ = summ + a[i][j]\n",
        "    l.append(a[i][i]/summ)\n",
        "  return l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V648uffX0jGW",
        "colab_type": "code",
        "outputId": "e9477a10-70e1-481f-bd64-61a81598a1b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "y_pred1 = []\n",
        "for i in range(len(Y_test)):\n",
        "    y_pred1.append(ypred[i][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-9d91b2825269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my_pred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0my_pred1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMKOh0hqmU-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_test,len(Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlEBmlzMmXyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ypred, len(ypred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz7zGO_SmZ8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE19x4zTJyCk",
        "colab_type": "code",
        "outputId": "440e032e-a32f-4a53-a6d3-db8788945d0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c = 0\n",
        "for ele in ypred:\n",
        "    if ele == 2:\n",
        "        c += 1\n",
        "print(c) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS3ggc0bKiH9",
        "colab_type": "code",
        "outputId": "7714593b-65b2-4d8a-b167-e325fc94302c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(Y_test, ypred)\n",
        "print(cm)\n",
        "print(calcp(cm))\n",
        "print(calcr(cm))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[65994     0    14]\n",
            " [  276    12     2]\n",
            " [  250     8    17]]\n",
            "[0.9920926037282021, 0.6, 0.5151515151515151]\n",
            "[0.9997879044964246, 0.041379310344827586, 0.06181818181818182]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUo-PpE7SAb0",
        "colab_type": "code",
        "outputId": "8510cba6-b038-4e6e-d18b-fae5d37d1aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X_test[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2.453588  ,  46.26736   ,  51.73611   ,  45.22569   ,\n",
              "        633.3333    ,  73.59009   ,  14.06973   ,  16.21094   ,\n",
              "         15.76968   ,  15.05353   ,  41.22567   ,  42.85013   ,\n",
              "         32.98205   ,   1.894322  ,  40.90165   ,  31.82649   ,\n",
              "          0.        ,   0.        ,   0.        ,  67.41518   ,\n",
              "        133.198     ,   0.        , 116.0851    ,   0.        ,\n",
              "          0.        , 112.7443    , 162.2008    ,  75.53671   ,\n",
              "         58.06888   , 108.7963    , 107.8125    , 133.0399    ,\n",
              "         59.00945   ,  61.72133   , 100.221     , 111.0354    ,\n",
              "         85.13418   ,  58.85416   ,  40.36458   ,  63.02083   ,\n",
              "         38.28125   ,  36.71875   ,  42.70833   ,  46.00694   ,\n",
              "         52.08333   ,  48.32176   ,  53.53009   ,  76.38889   ,\n",
              "         51.21527863, 169.8495    , 195.8912    ],\n",
              "       [  2.45162   ,  46.18055   ,  51.73611   ,  45.3125    ,\n",
              "        636.5741    ,  76.03027   ,  14.17101   ,  16.16753   ,\n",
              "         15.70457   ,  15.11863   ,  40.37008   ,  42.79751   ,\n",
              "         32.7526    ,   1.960529  ,  40.90165   ,  31.88505   ,\n",
              "          0.        ,   0.        ,   0.        ,  68.09219   ,\n",
              "        130.3692    ,   0.        , 114.3911    ,   0.        ,\n",
              "          0.        , 112.7449    , 168.938     ,  76.5083    ,\n",
              "         60.98227   , 106.4815    , 113.0208    , 134.5592    ,\n",
              "         63.24313   ,  61.69289   , 101.3205    , 109.8627    ,\n",
              "         92.23566   ,  58.59375   ,  39.58333   ,  64.84375   ,\n",
              "         38.02083   ,  36.97916   ,  43.48958   ,  45.42824   ,\n",
              "         52.08333   ,  47.4537    ,  52.08333   ,  73.78472   ,\n",
              "         51.21527863, 168.9815    , 196.7593    ],\n",
              "       [  2.455556  ,  46.22396   ,  51.82291   ,  45.3125    ,\n",
              "        636.9213    ,  76.90753   ,  13.92506   ,  16.21094   ,\n",
              "         15.65393   ,  15.08247   ,  41.26267   ,  43.31363   ,\n",
              "         32.60442   ,   1.915992  ,  40.88392   ,  31.31104   ,\n",
              "          0.        ,   0.        ,   0.        ,  67.89553   ,\n",
              "        132.6794    ,   0.        , 108.479     ,   0.        ,\n",
              "          0.        , 112.7449    , 164.3186    ,  71.00385   ,\n",
              "         57.45862   , 103.7037    , 107.2917    , 127.6181    ,\n",
              "         60.8136    ,  61.34266   , 101.6017    , 111.0342    ,\n",
              "         94.11044   ,  59.63541   ,  39.0625    ,  66.14583   ,\n",
              "         38.28125   ,  36.97916412,  43.48958   ,  44.84954   ,\n",
              "         52.08333   ,  46.58565   ,  50.63657379,  71.46991   ,\n",
              "         51.21527863, 168.1134    , 196.4699    ],\n",
              "       [  2.45162   ,  46.22396   ,  51.82291412,  45.3125    ,\n",
              "        630.4398    ,  76.09146   ,  14.03356   ,  16.13136   ,\n",
              "         15.82031   ,  15.11863   ,  41.63631   ,  43.01551   ,\n",
              "         33.17742   ,   2.126528  ,  40.90164   ,  31.88494   ,\n",
              "          0.        ,   0.        ,   0.        ,  67.45276   ,\n",
              "        133.8578    ,   0.        , 111.8429    ,   0.        ,\n",
              "          0.        , 112.7393    , 173.5466    ,  73.84657   ,\n",
              "         53.4742    , 107.4074    , 110.4167    , 132.5887    ,\n",
              "         62.30038   ,  61.73858   ,  94.73756   , 111.0354    ,\n",
              "         82.53537   ,  60.41666   ,  38.02083206,  66.40625   ,\n",
              "         38.28125   ,  36.97916412,  44.01041   ,  43.98148   ,\n",
              "         51.79398   ,  46.29629517,  49.18982   ,  69.7338    ,\n",
              "         51.21527863, 167.5347    , 195.0231    ],\n",
              "       [  2.455556  ,  46.18055   ,  51.82291   ,  45.3125    ,\n",
              "        631.0185    ,  75.15626   ,  14.13484   ,  16.21094   ,\n",
              "         15.45139   ,  15.11863   ,  42.95436   ,  42.51029   ,\n",
              "         33.6552    ,   2.067672  ,  40.8996    ,  31.88529   ,\n",
              "          0.        ,   0.        ,   0.        ,  67.84709   ,\n",
              "        132.1047    ,   0.        , 115.7832    ,   0.        ,\n",
              "          0.        , 112.4867    , 171.7733    ,  73.81602   ,\n",
              "         55.08039   , 108.3333    , 118.2292    , 128.1089    ,\n",
              "         60.84623   ,  61.84138   ,  94.28177   , 109.3073    ,\n",
              "         91.65108   ,  61.71875   ,  36.97916   ,  66.14583   ,\n",
              "         38.80208   ,  36.97916   ,  44.27083   ,  43.69213   ,\n",
              "         51.21528   ,  46.00694   ,  48.03241   ,  67.99769   ,\n",
              "         51.21527863, 165.5093    , 194.1551    ],\n",
              "       [  2.45162   ,  46.18055   ,  51.86632   ,  45.3125    ,\n",
              "        632.8704    ,  76.94429   ,  14.03356   ,  16.13136   ,\n",
              "         15.74074   ,  15.08247   ,  43.68709   ,  42.27246   ,\n",
              "         33.29139   ,   2.11345   ,  40.83897   ,  31.6142    ,\n",
              "          0.        ,   0.        ,   0.        ,  67.98743   ,\n",
              "        132.927     ,   0.        , 112.1425    ,   0.        ,\n",
              "          0.        , 112.574     , 174.932     ,  73.11892   ,\n",
              "         61.83959   , 109.2593    , 117.1875    , 133.0219    ,\n",
              "         56.67656   ,  61.65325   ,  92.94026   , 111.0352    ,\n",
              "         86.78497   ,  62.5       ,  36.19791412,  63.54166   ,\n",
              "         39.0625    ,  36.71875   ,  44.53125   ,  43.11343   ,\n",
              "         51.79398   ,  45.42824   ,  47.4537    ,  66.26157   ,\n",
              "         51.21527863, 164.0625    , 196.7593    ],\n",
              "       [  2.455556  ,  46.26736   ,  51.82291   ,  45.3559    ,\n",
              "        641.8982    ,  78.21407   ,  14.09867   ,  16.21094   ,\n",
              "         15.61777   ,  15.05353   ,  43.44365   ,  43.43155   ,\n",
              "         32.84568   ,   2.075769  ,  40.78794   ,  31.88408   ,\n",
              "          0.        ,   0.        ,   0.        ,  67.93549   ,\n",
              "        133.3247    ,   0.        , 112.6617    ,   0.        ,\n",
              "          0.        , 112.8315    , 183.599     ,  70.58563   ,\n",
              "         47.02621   , 109.7222    , 132.8125    , 136.9316    ,\n",
              "         61.72752   ,  61.51148   ,  92.384     , 101.4484    ,\n",
              "         85.98624   ,  62.23958   ,  35.41666   ,  60.41666   ,\n",
              "         39.0625    ,  36.71875   ,  44.27083   ,  43.11343   ,\n",
              "         51.79398   ,  45.13889   ,  47.45370483,  65.10416   ,\n",
              "         51.21528   , 163.7731    , 197.9167    ],\n",
              "       [  2.453588  ,  46.18055   ,  51.73611   ,  45.35589981,\n",
              "        620.6018    ,  77.88525   ,  14.03356   ,  16.13136   ,\n",
              "         15.69734   ,  15.08247   ,  43.21756   ,  44.01592   ,\n",
              "         32.48016   ,   2.1957    ,  42.26837   ,  31.81107   ,\n",
              "          0.        ,   0.        ,   0.        ,  67.80709   ,\n",
              "        132.8221    ,   0.        , 107.037     ,   0.        ,\n",
              "          0.        , 113.0181    , 176.6955    ,  76.99091   ,\n",
              "         57.99255   , 109.2593    , 115.625     , 133.2101    ,\n",
              "         64.00097   ,  61.647     ,  94.55086   , 111.0338    ,\n",
              "         92.00256   ,  61.19791   ,  35.15625   ,  59.63541   ,\n",
              "         38.80208   ,  36.45833   ,  43.48958   ,  43.11343   ,\n",
              "         51.21528   ,  44.56018448,  47.45370483,  63.94676   ,\n",
              "         50.92593   , 163.7731    , 198.4954    ],\n",
              "       [  2.455556  ,  46.22396   ,  51.82291   ,  45.35589981,\n",
              "        622.9166    ,  77.58377   ,  14.03356   ,  16.16753   ,\n",
              "         15.56713   ,  15.08247   ,  42.72834   ,  45.29336   ,\n",
              "         32.72369   ,   2.112827  ,  42.87831   ,  31.8682    ,\n",
              "          0.        ,   0.        ,   0.        ,  67.89223   ,\n",
              "        132.7918    ,   0.        , 113.1355    ,   0.        ,\n",
              "          0.        , 119.4266    , 175.9785    ,  74.73721   ,\n",
              "         57.55612   , 109.2593    , 128.125     , 136.836     ,\n",
              "         61.70529   ,  61.45142   ,  94.74887   , 110.7488    ,\n",
              "         82.8273    ,  59.89583   ,  34.89583   ,  64.0625    ,\n",
              "         38.28125   ,  36.45833206,  43.48958   ,  43.11343   ,\n",
              "         50.63657   ,  43.98148   ,  47.4537    ,  63.65741   ,\n",
              "         50.92592621, 161.7477    , 199.6528    ],\n",
              "       [  2.453588  ,  46.18055   ,  51.86631775,  45.35589981,\n",
              "        623.7268    ,  76.90094   ,  14.00463   ,  16.13136   ,\n",
              "         15.53096   ,  15.01013   ,  43.04007   ,  46.02494   ,\n",
              "         31.95595   ,   2.101915  ,  43.0619    ,  30.94643   ,\n",
              "          0.        ,   0.        ,   0.        ,  67.82573   ,\n",
              "        133.1251    ,   0.        , 106.6283    ,   0.        ,\n",
              "          0.        , 125.7814    , 169.3006    ,  71.83839   ,\n",
              "         55.89109   , 107.4074    , 100.        , 132.9353    ,\n",
              "         60.71901   ,  61.49644   , 102.2653    , 110.8091    ,\n",
              "         93.44414   ,  58.85416   ,  34.89583   ,  72.65625   ,\n",
              "         37.76041   ,  36.45833   ,  44.01041   ,  42.53472   ,\n",
              "         50.34722   ,  43.40278   ,  46.875     ,  63.94676   ,\n",
              "         50.92592621, 159.4329    , 201.3889    ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vc6LNXVSAZl",
        "colab_type": "code",
        "outputId": "63d3b1ca-5b9d-4e56-9af0-958709036c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_test[1801]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfheesK8SAXa",
        "colab_type": "code",
        "outputId": "95db6dec-1828-436b-f102-dd7e54847971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i in range(296, len(Y_test)):\n",
        "  if Y_test[i] == 2:\n",
        "    print(i)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMqnBGt3SASj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_re = X_test[5908].reshape(1, 10, 51)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljuxb1kgST-J",
        "colab_type": "code",
        "outputId": "35463367-afac-4292-bbde-4558093ffaa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test_re.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 10, 51)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkVWIOa6ST73",
        "colab_type": "code",
        "outputId": "948470f2-1f03-47b9-f646-4be47916e2ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "ypred = model.predict_classes(X_test_re[5908])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-b7b2faeee32c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_re\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5908\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: index 5908 is out of bounds for axis 0 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOgTUuQOT6vH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ypred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vFL9WCnT_JR",
        "colab_type": "code",
        "outputId": "e3509e05-8953-46b8-cfc5-2aac78225c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i in range(len(ypred)):\n",
        "  if ypred[i] == 2:\n",
        "    print(i)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm0hjg2MST5k",
        "colab_type": "code",
        "outputId": "43ff19fe-7605-4b27-e995-68d1920d65b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sum(ypred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58859"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTNMxnucK-jX",
        "colab_type": "code",
        "outputId": "6d03679c-5471-4b1c-a22d-a12611dba4b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X_test[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2.453588  ,  46.26736   ,  51.73611   ,  45.22569   ,\n",
              "        633.3333    ,  73.59009   ,  14.06973   ,  16.21094   ,\n",
              "         15.76968   ,  15.05353   ,  41.22567   ,  42.85013   ,\n",
              "         32.98205   ,   1.894322  ,  40.90165   ,  31.82649   ,\n",
              "          0.        ,   0.        ,   0.        ,  67.41518   ,\n",
              "        133.198     ,   0.        , 116.0851    ,   0.        ,\n",
              "          0.        , 112.7443    , 162.2008    ,  75.53671   ,\n",
              "         58.06888   , 108.7963    , 107.8125    , 133.0399    ,\n",
              "         59.00945   ,  61.72133   , 100.221     , 111.0354    ,\n",
              "         85.13418   ,  58.85416   ,  40.36458   ,  63.02083   ,\n",
              "         38.28125   ,  36.71875   ,  42.70833   ,  46.00694   ,\n",
              "         52.08333   ,  48.32176   ,  53.53009   ,  76.38889   ,\n",
              "         51.21527863, 169.8495    , 195.8912    ],\n",
              "       [  2.45162   ,  46.18055   ,  51.73611   ,  45.3125    ,\n",
              "        636.5741    ,  76.03027   ,  14.17101   ,  16.16753   ,\n",
              "         15.70457   ,  15.11863   ,  40.37008   ,  42.79751   ,\n",
              "         32.7526    ,   1.960529  ,  40.90165   ,  31.88505   ,\n",
              "          0.        ,   0.        ,   0.        ,  68.09219   ,\n",
              "        130.3692    ,   0.        , 114.3911    ,   0.        ,\n",
              "          0.        , 112.7449    , 168.938     ,  76.5083    ,\n",
              "         60.98227   , 106.4815    , 113.0208    , 134.5592    ,\n",
              "         63.24313   ,  61.69289   , 101.3205    , 109.8627    ,\n",
              "         92.23566   ,  58.59375   ,  39.58333   ,  64.84375   ,\n",
              "         38.02083   ,  36.97916   ,  43.48958   ,  45.42824   ,\n",
              "         52.08333   ,  47.4537    ,  52.08333   ,  73.78472   ,\n",
              "         51.21527863, 168.9815    , 196.7593    ],\n",
              "       [  2.455556  ,  46.22396   ,  51.82291   ,  45.3125    ,\n",
              "        636.9213    ,  76.90753   ,  13.92506   ,  16.21094   ,\n",
              "         15.65393   ,  15.08247   ,  41.26267   ,  43.31363   ,\n",
              "         32.60442   ,   1.915992  ,  40.88392   ,  31.31104   ,\n",
              "          0.        ,   0.        ,   0.        ,  67.89553   ,\n",
              "        132.6794    ,   0.        , 108.479     ,   0.        ,\n",
              "          0.        , 112.7449    , 164.3186    ,  71.00385   ,\n",
              "         57.45862   , 103.7037    , 107.2917    , 127.6181    ,\n",
              "         60.8136    ,  61.34266   , 101.6017    , 111.0342    ,\n",
              "         94.11044   ,  59.63541   ,  39.0625    ,  66.14583   ,\n",
              "         38.28125   ,  36.97916412,  43.48958   ,  44.84954   ,\n",
              "         52.08333   ,  46.58565   ,  50.63657379,  71.46991   ,\n",
              "         51.21527863, 168.1134    , 196.4699    ],\n",
              "       [  2.45162   ,  46.22396   ,  51.82291412,  45.3125    ,\n",
              "        630.4398    ,  76.09146   ,  14.03356   ,  16.13136   ,\n",
              "         15.82031   ,  15.11863   ,  41.63631   ,  43.01551   ,\n",
              "         33.17742   ,   2.126528  ,  40.90164   ,  31.88494   ,\n",
              "          0.        ,   0.        ,   0.        ,  67.45276   ,\n",
              "        133.8578    ,   0.        , 111.8429    ,   0.        ,\n",
              "          0.        , 112.7393    , 173.5466    ,  73.84657   ,\n",
              "         53.4742    , 107.4074    , 110.4167    , 132.5887    ,\n",
              "         62.30038   ,  61.73858   ,  94.73756   , 111.0354    ,\n",
              "         82.53537   ,  60.41666   ,  38.02083206,  66.40625   ,\n",
              "         38.28125   ,  36.97916412,  44.01041   ,  43.98148   ,\n",
              "         51.79398   ,  46.29629517,  49.18982   ,  69.7338    ,\n",
              "         51.21527863, 167.5347    , 195.0231    ],\n",
              "       [  2.455556  ,  46.18055   ,  51.82291   ,  45.3125    ,\n",
              "        631.0185    ,  75.15626   ,  14.13484   ,  16.21094   ,\n",
              "         15.45139   ,  15.11863   ,  42.95436   ,  42.51029   ,\n",
              "         33.6552    ,   2.067672  ,  40.8996    ,  31.88529   ,\n",
              "          0.        ,   0.        ,   0.        ,  67.84709   ,\n",
              "        132.1047    ,   0.        , 115.7832    ,   0.        ,\n",
              "          0.        , 112.4867    , 171.7733    ,  73.81602   ,\n",
              "         55.08039   , 108.3333    , 118.2292    , 128.1089    ,\n",
              "         60.84623   ,  61.84138   ,  94.28177   , 109.3073    ,\n",
              "         91.65108   ,  61.71875   ,  36.97916   ,  66.14583   ,\n",
              "         38.80208   ,  36.97916   ,  44.27083   ,  43.69213   ,\n",
              "         51.21528   ,  46.00694   ,  48.03241   ,  67.99769   ,\n",
              "         51.21527863, 165.5093    , 194.1551    ],\n",
              "       [  2.45162   ,  46.18055   ,  51.86632   ,  45.3125    ,\n",
              "        632.8704    ,  76.94429   ,  14.03356   ,  16.13136   ,\n",
              "         15.74074   ,  15.08247   ,  43.68709   ,  42.27246   ,\n",
              "         33.29139   ,   2.11345   ,  40.83897   ,  31.6142    ,\n",
              "          0.        ,   0.        ,   0.        ,  67.98743   ,\n",
              "        132.927     ,   0.        , 112.1425    ,   0.        ,\n",
              "          0.        , 112.574     , 174.932     ,  73.11892   ,\n",
              "         61.83959   , 109.2593    , 117.1875    , 133.0219    ,\n",
              "         56.67656   ,  61.65325   ,  92.94026   , 111.0352    ,\n",
              "         86.78497   ,  62.5       ,  36.19791412,  63.54166   ,\n",
              "         39.0625    ,  36.71875   ,  44.53125   ,  43.11343   ,\n",
              "         51.79398   ,  45.42824   ,  47.4537    ,  66.26157   ,\n",
              "         51.21527863, 164.0625    , 196.7593    ],\n",
              "       [  2.455556  ,  46.26736   ,  51.82291   ,  45.3559    ,\n",
              "        641.8982    ,  78.21407   ,  14.09867   ,  16.21094   ,\n",
              "         15.61777   ,  15.05353   ,  43.44365   ,  43.43155   ,\n",
              "         32.84568   ,   2.075769  ,  40.78794   ,  31.88408   ,\n",
              "          0.        ,   0.        ,   0.        ,  67.93549   ,\n",
              "        133.3247    ,   0.        , 112.6617    ,   0.        ,\n",
              "          0.        , 112.8315    , 183.599     ,  70.58563   ,\n",
              "         47.02621   , 109.7222    , 132.8125    , 136.9316    ,\n",
              "         61.72752   ,  61.51148   ,  92.384     , 101.4484    ,\n",
              "         85.98624   ,  62.23958   ,  35.41666   ,  60.41666   ,\n",
              "         39.0625    ,  36.71875   ,  44.27083   ,  43.11343   ,\n",
              "         51.79398   ,  45.13889   ,  47.45370483,  65.10416   ,\n",
              "         51.21528   , 163.7731    , 197.9167    ],\n",
              "       [  2.453588  ,  46.18055   ,  51.73611   ,  45.35589981,\n",
              "        620.6018    ,  77.88525   ,  14.03356   ,  16.13136   ,\n",
              "         15.69734   ,  15.08247   ,  43.21756   ,  44.01592   ,\n",
              "         32.48016   ,   2.1957    ,  42.26837   ,  31.81107   ,\n",
              "          0.        ,   0.        ,   0.        ,  67.80709   ,\n",
              "        132.8221    ,   0.        , 107.037     ,   0.        ,\n",
              "          0.        , 113.0181    , 176.6955    ,  76.99091   ,\n",
              "         57.99255   , 109.2593    , 115.625     , 133.2101    ,\n",
              "         64.00097   ,  61.647     ,  94.55086   , 111.0338    ,\n",
              "         92.00256   ,  61.19791   ,  35.15625   ,  59.63541   ,\n",
              "         38.80208   ,  36.45833   ,  43.48958   ,  43.11343   ,\n",
              "         51.21528   ,  44.56018448,  47.45370483,  63.94676   ,\n",
              "         50.92593   , 163.7731    , 198.4954    ],\n",
              "       [  2.455556  ,  46.22396   ,  51.82291   ,  45.35589981,\n",
              "        622.9166    ,  77.58377   ,  14.03356   ,  16.16753   ,\n",
              "         15.56713   ,  15.08247   ,  42.72834   ,  45.29336   ,\n",
              "         32.72369   ,   2.112827  ,  42.87831   ,  31.8682    ,\n",
              "          0.        ,   0.        ,   0.        ,  67.89223   ,\n",
              "        132.7918    ,   0.        , 113.1355    ,   0.        ,\n",
              "          0.        , 119.4266    , 175.9785    ,  74.73721   ,\n",
              "         57.55612   , 109.2593    , 128.125     , 136.836     ,\n",
              "         61.70529   ,  61.45142   ,  94.74887   , 110.7488    ,\n",
              "         82.8273    ,  59.89583   ,  34.89583   ,  64.0625    ,\n",
              "         38.28125   ,  36.45833206,  43.48958   ,  43.11343   ,\n",
              "         50.63657   ,  43.98148   ,  47.4537    ,  63.65741   ,\n",
              "         50.92592621, 161.7477    , 199.6528    ],\n",
              "       [  2.453588  ,  46.18055   ,  51.86631775,  45.35589981,\n",
              "        623.7268    ,  76.90094   ,  14.00463   ,  16.13136   ,\n",
              "         15.53096   ,  15.01013   ,  43.04007   ,  46.02494   ,\n",
              "         31.95595   ,   2.101915  ,  43.0619    ,  30.94643   ,\n",
              "          0.        ,   0.        ,   0.        ,  67.82573   ,\n",
              "        133.1251    ,   0.        , 106.6283    ,   0.        ,\n",
              "          0.        , 125.7814    , 169.3006    ,  71.83839   ,\n",
              "         55.89109   , 107.4074    , 100.        , 132.9353    ,\n",
              "         60.71901   ,  61.49644   , 102.2653    , 110.8091    ,\n",
              "         93.44414   ,  58.85416   ,  34.89583   ,  72.65625   ,\n",
              "         37.76041   ,  36.45833   ,  44.01041   ,  42.53472   ,\n",
              "         50.34722   ,  43.40278   ,  46.875     ,  63.94676   ,\n",
              "         50.92592621, 159.4329    , 201.3889    ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B9f-XI5EnBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_all = X_test[1].reshape(1, 10, 51)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Po2hd7TEnHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ypred = model.predict_classes(X_test_all)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOEdkN7JEm_D",
        "colab_type": "code",
        "outputId": "26e47678-7d6b-45e4-a295-51ae81f69f09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i in range(len(Y_test)):\n",
        "  if Y_test[i] == 2:\n",
        "    print(i)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzOy6YvOEm7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(ypred)):\n",
        "  if ypred[i] == 1:\n",
        "    print(i)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScXiwc8EST3J",
        "colab_type": "code",
        "outputId": "d013428c-a7f9-4c34-bc8b-b7e819c10f58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ypred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvScpmWy198i",
        "colab_type": "text"
      },
      "source": [
        "LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwfNhMJA2AC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_features = len(X_train[0])\n",
        "n_steps = 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDEpjDpFmkGw",
        "colab_type": "code",
        "outputId": "f84c5d6f-96ca-4ba5-9448-276753029e27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu9YCmU878LO",
        "colab_type": "code",
        "outputId": "31317823-2044-49a2-ff70-1956fd771ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99859, 10, 51)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrL0RnG82EqJ",
        "colab_type": "code",
        "outputId": "69def2c9-a5bf-445a-e77a-65845f414ca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "X_train= X_train.reshape(len(X_train), n_steps, n_features)\n",
        "X_test=X_test.reshape(len(X_test), n_steps, n_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-510343e4552b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 50928090 into shape (99859,1,10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8rFzm672FCW",
        "colab_type": "code",
        "outputId": "b6c2fb2e-1194-4d79-dd62-2aae63449124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99859, 10, 51)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cNW0WSQkXCP",
        "colab_type": "code",
        "outputId": "cb1077b4-d2bd-47af-bea7-83a6c40960fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j_4qwZ4nEtq",
        "colab_type": "code",
        "outputId": "49014ea6-69ba-41f9-c463-fbfccc1a4d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "# model.add(LSTM(50, activation='relu', input_shape=(1, n_features)))\n",
        "# model.add(Dense(1))\n",
        "# model.compile(optimizer='adam', loss='mse')\n",
        "model.add(LSTM(10, dropout=0.2, recurrent_dropout=0.2,input_shape=(1, n_features)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, epochs=10, verbose=1)\n",
        "ypred = model.predict(X_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-cb2b7c8dc125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_56_input to have shape (1, 10) but got array with shape (10, 51)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfOLkA8zaPap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# history_lstm = model_lstm.fit(X_train, Y_train, epochs=10, batch_size=64, validation_data=(X_test, Y_test),  shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KuBYX_5t1qY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3qHFaTlbbW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ypred = model_lstm.predict(X_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_HLqkPBkPoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ypred = model.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au4Gw87sbqFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics.precision_score(Y_test, ypred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13j7e7-7Ev16",
        "colab_type": "text"
      },
      "source": [
        "GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDjLC2hnkOxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "# model.add(LSTM(50, activation='relu', input_shape=(1, n_features)))\n",
        "# model.add(Dense(1))\n",
        "# model.compile(optimizer='adam', loss='mse')\n",
        "model.add(GRU(10, dropout=0.2, recurrent_dropout=0.2,input_shape=(1, n_features)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, epochs=10, verbose=1)\n",
        "ypred = model.predict(X_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1EUXyTlAV-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av9vRznrBOP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ypred = model.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbH2CcgRAXEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics.precision_score(Y_test, ypred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpbUSawSaQRA",
        "colab_type": "text"
      },
      "source": [
        "CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW9f1KwCaU1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NcEEMtqzKAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cm = confusion_matrix(y_test, y_pred)\n",
        "# print(cm)\n",
        "# print(metrics.accuracy_score(y_test, y_pred, normalize=True))\n",
        "# print(calcp(cm))\n",
        "# print(calcr(cm))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtPKulgiD0Ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnpUt55U_XPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " dataset = train_df.iloc[:, 1:53]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovphGO9PAuud",
        "colab_type": "code",
        "outputId": "4a8695ed-7729-49de-c125-87e3eabd634e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sensor_00</th>\n",
              "      <th>sensor_01</th>\n",
              "      <th>sensor_02</th>\n",
              "      <th>sensor_03</th>\n",
              "      <th>sensor_04</th>\n",
              "      <th>sensor_05</th>\n",
              "      <th>sensor_06</th>\n",
              "      <th>sensor_07</th>\n",
              "      <th>sensor_08</th>\n",
              "      <th>sensor_09</th>\n",
              "      <th>sensor_10</th>\n",
              "      <th>sensor_11</th>\n",
              "      <th>sensor_12</th>\n",
              "      <th>sensor_13</th>\n",
              "      <th>sensor_14</th>\n",
              "      <th>sensor_16</th>\n",
              "      <th>sensor_17</th>\n",
              "      <th>sensor_18</th>\n",
              "      <th>sensor_19</th>\n",
              "      <th>sensor_20</th>\n",
              "      <th>sensor_21</th>\n",
              "      <th>sensor_22</th>\n",
              "      <th>sensor_23</th>\n",
              "      <th>sensor_24</th>\n",
              "      <th>sensor_25</th>\n",
              "      <th>sensor_26</th>\n",
              "      <th>sensor_27</th>\n",
              "      <th>sensor_28</th>\n",
              "      <th>sensor_29</th>\n",
              "      <th>sensor_30</th>\n",
              "      <th>sensor_31</th>\n",
              "      <th>sensor_32</th>\n",
              "      <th>sensor_33</th>\n",
              "      <th>sensor_34</th>\n",
              "      <th>sensor_35</th>\n",
              "      <th>sensor_36</th>\n",
              "      <th>sensor_37</th>\n",
              "      <th>sensor_38</th>\n",
              "      <th>sensor_39</th>\n",
              "      <th>sensor_40</th>\n",
              "      <th>sensor_41</th>\n",
              "      <th>sensor_42</th>\n",
              "      <th>sensor_43</th>\n",
              "      <th>sensor_44</th>\n",
              "      <th>sensor_45</th>\n",
              "      <th>sensor_46</th>\n",
              "      <th>sensor_47</th>\n",
              "      <th>sensor_48</th>\n",
              "      <th>sensor_49</th>\n",
              "      <th>sensor_50</th>\n",
              "      <th>sensor_51</th>\n",
              "      <th>RUL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.465394</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.310760</td>\n",
              "      <td>634.375000</td>\n",
              "      <td>76.45975</td>\n",
              "      <td>13.41146</td>\n",
              "      <td>16.13136</td>\n",
              "      <td>15.56713</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>37.22740</td>\n",
              "      <td>47.52422</td>\n",
              "      <td>31.11716</td>\n",
              "      <td>1.681353</td>\n",
              "      <td>419.5747</td>\n",
              "      <td>461.8781</td>\n",
              "      <td>466.3284</td>\n",
              "      <td>2.565284</td>\n",
              "      <td>665.3993</td>\n",
              "      <td>398.9862</td>\n",
              "      <td>880.0001</td>\n",
              "      <td>498.8926</td>\n",
              "      <td>975.9409</td>\n",
              "      <td>627.6740</td>\n",
              "      <td>741.7151</td>\n",
              "      <td>848.0708</td>\n",
              "      <td>429.0377</td>\n",
              "      <td>785.1935</td>\n",
              "      <td>684.9443</td>\n",
              "      <td>594.4445</td>\n",
              "      <td>682.8125</td>\n",
              "      <td>680.4416</td>\n",
              "      <td>433.7037</td>\n",
              "      <td>171.9375</td>\n",
              "      <td>341.9039</td>\n",
              "      <td>195.0655</td>\n",
              "      <td>90.32386</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>70.57291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.770832</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>39.641200</td>\n",
              "      <td>65.68287</td>\n",
              "      <td>50.92593</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>157.9861</td>\n",
              "      <td>67.70834</td>\n",
              "      <td>243.0556</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.444734</td>\n",
              "      <td>47.352430</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397570</td>\n",
              "      <td>638.888900</td>\n",
              "      <td>73.54598</td>\n",
              "      <td>13.32465</td>\n",
              "      <td>16.03733</td>\n",
              "      <td>15.61777</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>37.86777</td>\n",
              "      <td>48.17723</td>\n",
              "      <td>32.08894</td>\n",
              "      <td>1.708474</td>\n",
              "      <td>420.8480</td>\n",
              "      <td>462.7798</td>\n",
              "      <td>459.6364</td>\n",
              "      <td>2.500062</td>\n",
              "      <td>666.2234</td>\n",
              "      <td>399.9418</td>\n",
              "      <td>880.4237</td>\n",
              "      <td>501.3617</td>\n",
              "      <td>982.7342</td>\n",
              "      <td>631.1326</td>\n",
              "      <td>740.8031</td>\n",
              "      <td>849.8997</td>\n",
              "      <td>454.2390</td>\n",
              "      <td>778.5734</td>\n",
              "      <td>715.6266</td>\n",
              "      <td>661.5740</td>\n",
              "      <td>721.8750</td>\n",
              "      <td>694.7721</td>\n",
              "      <td>441.2635</td>\n",
              "      <td>169.9820</td>\n",
              "      <td>343.1955</td>\n",
              "      <td>200.9694</td>\n",
              "      <td>93.90508</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>31.25000</td>\n",
              "      <td>69.53125</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.770830</td>\n",
              "      <td>41.66666</td>\n",
              "      <td>39.351852</td>\n",
              "      <td>65.39352</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194443</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>241.3194</td>\n",
              "      <td>203.7037</td>\n",
              "      <td>17153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.460474</td>\n",
              "      <td>47.092010</td>\n",
              "      <td>53.16840</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>628.125000</td>\n",
              "      <td>76.98898</td>\n",
              "      <td>13.31742</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>38.57977</td>\n",
              "      <td>48.65607</td>\n",
              "      <td>31.67221</td>\n",
              "      <td>1.579427</td>\n",
              "      <td>420.7494</td>\n",
              "      <td>462.8980</td>\n",
              "      <td>460.8858</td>\n",
              "      <td>2.509521</td>\n",
              "      <td>666.0114</td>\n",
              "      <td>399.1046</td>\n",
              "      <td>878.8917</td>\n",
              "      <td>499.0430</td>\n",
              "      <td>977.7520</td>\n",
              "      <td>625.4076</td>\n",
              "      <td>739.2722</td>\n",
              "      <td>847.7579</td>\n",
              "      <td>474.8731</td>\n",
              "      <td>779.5091</td>\n",
              "      <td>690.4011</td>\n",
              "      <td>686.1111</td>\n",
              "      <td>754.6875</td>\n",
              "      <td>683.3831</td>\n",
              "      <td>446.2493</td>\n",
              "      <td>166.4987</td>\n",
              "      <td>343.9586</td>\n",
              "      <td>193.1689</td>\n",
              "      <td>101.04060</td>\n",
              "      <td>41.92708</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>72.13541</td>\n",
              "      <td>30.468750</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>40.88541</td>\n",
              "      <td>39.062500</td>\n",
              "      <td>64.81481</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>38.194440</td>\n",
              "      <td>155.9606</td>\n",
              "      <td>66.84028</td>\n",
              "      <td>240.4514</td>\n",
              "      <td>203.1250</td>\n",
              "      <td>17152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.445718</td>\n",
              "      <td>47.135410</td>\n",
              "      <td>53.21180</td>\n",
              "      <td>46.397568</td>\n",
              "      <td>636.458300</td>\n",
              "      <td>76.58897</td>\n",
              "      <td>13.35359</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.08247</td>\n",
              "      <td>39.48939</td>\n",
              "      <td>49.06298</td>\n",
              "      <td>31.95202</td>\n",
              "      <td>1.683831</td>\n",
              "      <td>419.8926</td>\n",
              "      <td>461.4906</td>\n",
              "      <td>468.2206</td>\n",
              "      <td>2.604785</td>\n",
              "      <td>663.2111</td>\n",
              "      <td>400.5426</td>\n",
              "      <td>882.5874</td>\n",
              "      <td>498.5383</td>\n",
              "      <td>979.5755</td>\n",
              "      <td>627.1830</td>\n",
              "      <td>737.6033</td>\n",
              "      <td>846.9182</td>\n",
              "      <td>408.8159</td>\n",
              "      <td>785.2307</td>\n",
              "      <td>704.6937</td>\n",
              "      <td>631.4814</td>\n",
              "      <td>766.1458</td>\n",
              "      <td>702.4431</td>\n",
              "      <td>433.9081</td>\n",
              "      <td>164.7498</td>\n",
              "      <td>339.9630</td>\n",
              "      <td>193.8770</td>\n",
              "      <td>101.70380</td>\n",
              "      <td>42.70833</td>\n",
              "      <td>31.51042</td>\n",
              "      <td>76.82291</td>\n",
              "      <td>30.989580</td>\n",
              "      <td>31.510420</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>65.10416</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>38.773150</td>\n",
              "      <td>158.2755</td>\n",
              "      <td>66.55093</td>\n",
              "      <td>242.1875</td>\n",
              "      <td>201.3889</td>\n",
              "      <td>17151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166436</th>\n",
              "      <td>2.313889</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>71.464119</td>\n",
              "      <td>73.00980</td>\n",
              "      <td>14.36632</td>\n",
              "      <td>16.21094</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.12587</td>\n",
              "      <td>38.34975</td>\n",
              "      <td>49.88500</td>\n",
              "      <td>31.94489</td>\n",
              "      <td>12.285810</td>\n",
              "      <td>421.4294</td>\n",
              "      <td>458.9118</td>\n",
              "      <td>456.3463</td>\n",
              "      <td>2.459503</td>\n",
              "      <td>666.8698</td>\n",
              "      <td>398.0684</td>\n",
              "      <td>879.7014</td>\n",
              "      <td>536.2166</td>\n",
              "      <td>1091.1240</td>\n",
              "      <td>629.8835</td>\n",
              "      <td>740.9833</td>\n",
              "      <td>981.7558</td>\n",
              "      <td>497.2223</td>\n",
              "      <td>995.5433</td>\n",
              "      <td>563.5882</td>\n",
              "      <td>676.8519</td>\n",
              "      <td>949.9999</td>\n",
              "      <td>990.9057</td>\n",
              "      <td>564.7968</td>\n",
              "      <td>342.3690</td>\n",
              "      <td>498.5037</td>\n",
              "      <td>804.9069</td>\n",
              "      <td>55.82830</td>\n",
              "      <td>53.38541</td>\n",
              "      <td>41.40625</td>\n",
              "      <td>93.48958</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.79398</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>45.138890</td>\n",
              "      <td>252.3148</td>\n",
              "      <td>69.15509</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>198.2060</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166437</th>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.923611</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>72.47576</td>\n",
              "      <td>14.33015</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.76968</td>\n",
              "      <td>15.16204</td>\n",
              "      <td>38.58192</td>\n",
              "      <td>49.96759</td>\n",
              "      <td>31.46310</td>\n",
              "      <td>12.213550</td>\n",
              "      <td>420.6030</td>\n",
              "      <td>460.6981</td>\n",
              "      <td>467.5996</td>\n",
              "      <td>2.603880</td>\n",
              "      <td>665.2674</td>\n",
              "      <td>399.1180</td>\n",
              "      <td>879.2195</td>\n",
              "      <td>534.4346</td>\n",
              "      <td>1089.8550</td>\n",
              "      <td>628.6547</td>\n",
              "      <td>738.9200</td>\n",
              "      <td>979.7875</td>\n",
              "      <td>504.8127</td>\n",
              "      <td>1004.9950</td>\n",
              "      <td>569.5920</td>\n",
              "      <td>719.9074</td>\n",
              "      <td>934.3749</td>\n",
              "      <td>982.0567</td>\n",
              "      <td>559.5000</td>\n",
              "      <td>345.2228</td>\n",
              "      <td>500.3699</td>\n",
              "      <td>804.2908</td>\n",
              "      <td>49.17398</td>\n",
              "      <td>52.60416</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>95.57291</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>49.47916</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>51.21528</td>\n",
              "      <td>60.47454</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>253.1829</td>\n",
              "      <td>71.46991</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>202.8356</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166438</th>\n",
              "      <td>2.322743</td>\n",
              "      <td>45.833330</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.923610</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>70.48889</td>\n",
              "      <td>14.46036</td>\n",
              "      <td>16.16030</td>\n",
              "      <td>15.73351</td>\n",
              "      <td>15.01013</td>\n",
              "      <td>38.87765</td>\n",
              "      <td>49.75908</td>\n",
              "      <td>31.78839</td>\n",
              "      <td>12.180410</td>\n",
              "      <td>419.6096</td>\n",
              "      <td>462.2329</td>\n",
              "      <td>466.0083</td>\n",
              "      <td>2.577086</td>\n",
              "      <td>667.1351</td>\n",
              "      <td>398.3854</td>\n",
              "      <td>881.7244</td>\n",
              "      <td>536.0204</td>\n",
              "      <td>1095.5440</td>\n",
              "      <td>630.5479</td>\n",
              "      <td>743.5366</td>\n",
              "      <td>981.5860</td>\n",
              "      <td>488.0563</td>\n",
              "      <td>1009.6940</td>\n",
              "      <td>559.3523</td>\n",
              "      <td>678.2407</td>\n",
              "      <td>947.9166</td>\n",
              "      <td>1016.9990</td>\n",
              "      <td>558.1667</td>\n",
              "      <td>344.4498</td>\n",
              "      <td>490.6038</td>\n",
              "      <td>796.3863</td>\n",
              "      <td>39.34733</td>\n",
              "      <td>51.82291</td>\n",
              "      <td>40.36458</td>\n",
              "      <td>100.52080</td>\n",
              "      <td>36.197914</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>49.73958</td>\n",
              "      <td>52.083330</td>\n",
              "      <td>52.08333</td>\n",
              "      <td>61.34259</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>249.1319</td>\n",
              "      <td>72.33796</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>206.8866</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166439</th>\n",
              "      <td>2.315856</td>\n",
              "      <td>45.789930</td>\n",
              "      <td>53.03819</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>3.336227</td>\n",
              "      <td>68.74365</td>\n",
              "      <td>14.43866</td>\n",
              "      <td>16.07350</td>\n",
              "      <td>15.53096</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>38.21849</td>\n",
              "      <td>49.73039</td>\n",
              "      <td>32.32672</td>\n",
              "      <td>12.209670</td>\n",
              "      <td>420.0334</td>\n",
              "      <td>460.0883</td>\n",
              "      <td>460.4448</td>\n",
              "      <td>2.509833</td>\n",
              "      <td>664.8566</td>\n",
              "      <td>399.6012</td>\n",
              "      <td>880.7674</td>\n",
              "      <td>535.4742</td>\n",
              "      <td>1092.3240</td>\n",
              "      <td>628.6292</td>\n",
              "      <td>743.1452</td>\n",
              "      <td>982.4372</td>\n",
              "      <td>511.7988</td>\n",
              "      <td>1025.1100</td>\n",
              "      <td>559.1011</td>\n",
              "      <td>743.5185</td>\n",
              "      <td>960.4166</td>\n",
              "      <td>998.0515</td>\n",
              "      <td>551.9908</td>\n",
              "      <td>339.3840</td>\n",
              "      <td>492.3628</td>\n",
              "      <td>796.2476</td>\n",
              "      <td>53.73619</td>\n",
              "      <td>51.04166</td>\n",
              "      <td>40.62500</td>\n",
              "      <td>104.68750</td>\n",
              "      <td>36.197910</td>\n",
              "      <td>36.458332</td>\n",
              "      <td>50.52083</td>\n",
              "      <td>52.662040</td>\n",
              "      <td>53.24074</td>\n",
              "      <td>63.36805</td>\n",
              "      <td>44.849540</td>\n",
              "      <td>244.5023</td>\n",
              "      <td>72.62731</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>209.7801</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166440</th>\n",
              "      <td>2.318808</td>\n",
              "      <td>45.833332</td>\n",
              "      <td>52.99479</td>\n",
              "      <td>43.880210</td>\n",
              "      <td>420.503448</td>\n",
              "      <td>72.52040</td>\n",
              "      <td>14.18547</td>\n",
              "      <td>16.24711</td>\n",
              "      <td>15.69734</td>\n",
              "      <td>15.05353</td>\n",
              "      <td>36.71748</td>\n",
              "      <td>50.04619</td>\n",
              "      <td>30.74288</td>\n",
              "      <td>12.217860</td>\n",
              "      <td>420.8600</td>\n",
              "      <td>463.5181</td>\n",
              "      <td>455.2702</td>\n",
              "      <td>2.468381</td>\n",
              "      <td>665.9402</td>\n",
              "      <td>399.0930</td>\n",
              "      <td>880.8410</td>\n",
              "      <td>531.8727</td>\n",
              "      <td>1093.5790</td>\n",
              "      <td>626.4651</td>\n",
              "      <td>740.6738</td>\n",
              "      <td>981.4561</td>\n",
              "      <td>490.7728</td>\n",
              "      <td>1023.1660</td>\n",
              "      <td>551.2756</td>\n",
              "      <td>703.7037</td>\n",
              "      <td>971.8749</td>\n",
              "      <td>1013.3760</td>\n",
              "      <td>546.6656</td>\n",
              "      <td>363.0320</td>\n",
              "      <td>522.5594</td>\n",
              "      <td>812.8298</td>\n",
              "      <td>44.77601</td>\n",
              "      <td>50.00000</td>\n",
              "      <td>39.06250</td>\n",
              "      <td>102.86460</td>\n",
              "      <td>35.416660</td>\n",
              "      <td>36.458330</td>\n",
              "      <td>51.30208</td>\n",
              "      <td>52.102000</td>\n",
              "      <td>52.66204</td>\n",
              "      <td>67.12963</td>\n",
              "      <td>43.981480</td>\n",
              "      <td>230.3241</td>\n",
              "      <td>69.73380</td>\n",
              "      <td>1000.0000</td>\n",
              "      <td>205.7292</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>166441 rows × 52 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        sensor_00  sensor_01  sensor_02  ...  sensor_50  sensor_51    RUL\n",
              "0        2.465394  47.092010   53.21180  ...   243.0556   201.3889  17155\n",
              "1        2.465394  47.092010   53.21180  ...   243.0556   201.3889  17154\n",
              "2        2.444734  47.352430   53.21180  ...   241.3194   203.7037  17153\n",
              "3        2.460474  47.092010   53.16840  ...   240.4514   203.1250  17152\n",
              "4        2.445718  47.135410   53.21180  ...   242.1875   201.3889  17151\n",
              "...           ...        ...        ...  ...        ...        ...    ...\n",
              "166436   2.313889  45.833330   53.03819  ...  1000.0000   198.2060      4\n",
              "166437   2.315856  45.833332   53.03819  ...  1000.0000   202.8356      3\n",
              "166438   2.322743  45.833330   52.99479  ...  1000.0000   206.8866      2\n",
              "166439   2.315856  45.789930   53.03819  ...  1000.0000   209.7801      1\n",
              "166440   2.318808  45.833332   52.99479  ...  1000.0000   205.7292      0\n",
              "\n",
              "[166441 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqHNXO8O8T06",
        "colab_type": "code",
        "outputId": "f8727143-264e-491b-d8f9-98ce624c0a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# multivariate data preparation\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "\n",
        "# split a multivariate sequence into samples\n",
        "def split_sequences(sequences, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequences)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the dataset\n",
        "\t\tif end_ix > len(sequences):\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)\n",
        "\n",
        "# define input sequence\n",
        "\n",
        "dataset = dataset.to_numpy()\n",
        "print(dataset)\n",
        "# choose a number of time steps\n",
        "n_steps = 100\n",
        "# convert into input/output\n",
        "X, y = split_sequences(dataset, n_steps)\n",
        "print(X.shape, y.shape)\n",
        "# summarize the data\n",
        "# for i in range(len(X)):\n",
        "# \tprint(X[i], y[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.46539400e+00 4.70920100e+01 5.32118000e+01 ... 2.43055600e+02\n",
            "  2.01388900e+02 1.71550000e+04]\n",
            " [2.46539400e+00 4.70920100e+01 5.32118000e+01 ... 2.43055600e+02\n",
            "  2.01388900e+02 1.71540000e+04]\n",
            " [2.44473400e+00 4.73524300e+01 5.32118000e+01 ... 2.41319400e+02\n",
            "  2.03703700e+02 1.71530000e+04]\n",
            " ...\n",
            " [2.32274300e+00 4.58333300e+01 5.29947900e+01 ... 1.00000000e+03\n",
            "  2.06886600e+02 2.00000000e+00]\n",
            " [2.31585600e+00 4.57899300e+01 5.30381900e+01 ... 1.00000000e+03\n",
            "  2.09780100e+02 1.00000000e+00]\n",
            " [2.31880800e+00 4.58333321e+01 5.29947900e+01 ... 1.00000000e+03\n",
            "  2.05729200e+02 0.00000000e+00]]\n",
            "(166342, 100, 51) (166342,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y9nWQtO4RBt",
        "colab_type": "text"
      },
      "source": [
        "K FOLD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmoGZdYm4K-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "for train, test in kfold.split(X, Y):\n",
        "  # create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(12, input_dim=8, activation='relu'))\n",
        "\tmodel.add(Dense(8, activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# Fit the model\n",
        "\tmodel.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
        "\t# evaluate the model\n",
        "\tscores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "\tprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\tcvscores.append(scores[1] * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry2qXwWDFXZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size = 0.6, random_state = seed(20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFYfDz5pEXeh",
        "colab_type": "code",
        "outputId": "6cb6fc64-4ab9-4746-8139-3fad24209fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_features = X.shape[2]\n",
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99805, 100, 51)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGpHtnaXCHRw",
        "colab_type": "code",
        "outputId": "8090a146-438e-4698-a51e-a73fe2d389f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCejAeGsDmwC",
        "colab_type": "code",
        "outputId": "32cef93e-a642-47d5-b03c-52dc494aaa60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model.fit(X_train, Y_train, epochs=10, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "99805/99805 [==============================] - 22s 217us/step - loss: -275575.9461 - acc: 3.0059e-05\n",
            "Epoch 2/10\n",
            "99805/99805 [==============================] - 22s 217us/step - loss: -276042.9212 - acc: 3.0059e-05\n",
            "Epoch 3/10\n",
            "99805/99805 [==============================] - 22s 224us/step - loss: -276042.9216 - acc: 3.0059e-05\n",
            "Epoch 4/10\n",
            "99805/99805 [==============================] - 22s 222us/step - loss: -276042.9216 - acc: 3.0059e-05\n",
            "Epoch 5/10\n",
            "99805/99805 [==============================] - 22s 221us/step - loss: -276042.9218 - acc: 3.0059e-05\n",
            "Epoch 6/10\n",
            "99805/99805 [==============================] - 22s 221us/step - loss: -276042.9219 - acc: 3.0059e-05\n",
            "Epoch 7/10\n",
            "99805/99805 [==============================] - 22s 216us/step - loss: -276042.9214 - acc: 3.0059e-05\n",
            "Epoch 8/10\n",
            "99805/99805 [==============================] - 21s 215us/step - loss: -276042.9213 - acc: 3.0059e-05\n",
            "Epoch 9/10\n",
            "99805/99805 [==============================] - 21s 207us/step - loss: -276042.9217 - acc: 3.0059e-05\n",
            "Epoch 10/10\n",
            "99805/99805 [==============================] - 20s 205us/step - loss: -276042.9218 - acc: 3.0059e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fee1995ccf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niNJBINdEkLQ",
        "colab_type": "code",
        "outputId": "77b2091b-97d7-41ce-9451-984bb199cb72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "yhat = model.predict(X_test, verbose=0)\n",
        "print(yhat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " ...\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0PD3zM2F9iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlODTD8GYsbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpXXJjNiGBPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'cnn_model10EPOCH.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6Zk7uP2ZKyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('cnn_model10EPOCH.sav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkHhKvWMZPSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename2 = 'TEST10EPOCH.sav'\n",
        "pickle.dump(X_test, open(filename2, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j935oP_3dMLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('TEST10EPOCH.sav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4avdrhcZPra4",
        "colab_type": "text"
      },
      "source": [
        "# attention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Czr48_iQseV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _time_distributed_dense(x, w, b=None, dropout=None,\n",
        "                           input_dim=None, output_dim=None, timesteps=None):\n",
        "    '''Apply y.w + b for every temporal slice y of x.\n",
        "    '''\n",
        "    if not input_dim:\n",
        "        # won't work with TensorFlow\n",
        "        input_dim = K.shape(x)[2]\n",
        "    if not timesteps:\n",
        "        # won't work with TensorFlow\n",
        "        timesteps = K.shape(x)[1]\n",
        "    if not output_dim:\n",
        "        # won't work with TensorFlow\n",
        "        output_dim = K.shape(w)[1]\n",
        "\n",
        "    if dropout:\n",
        "        # apply the same dropout pattern at every timestep\n",
        "        ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
        "        dropout_matrix = K.dropout(ones, dropout)\n",
        "        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
        "        x *= expanded_dropout_matrix\n",
        "\n",
        "    # collapse time dimension and batch dimension together\n",
        "    x = K.reshape(x, (-1, input_dim))\n",
        "\n",
        "    x = K.dot(x, w)\n",
        "    if b:\n",
        "        x = x + b\n",
        "    # reshape to 3D tensor\n",
        "    x = K.reshape(x, (-1, timesteps, output_dim))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7sG_dT3QTrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras import regularizers, constraints, initializers, activations\n",
        "from keras.layers.recurrent import Recurrent\n",
        "from keras.engine import InputSpec\n",
        "\n",
        "tfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n",
        "\n",
        "class AttentionDecoder(Recurrent):\n",
        "\n",
        "    def __init__(self, units, output_dim,\n",
        "                 activation='tanh',\n",
        "                 return_probabilities=False,\n",
        "                 name='AttentionDecoder',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
        "        encoder and outputs the decoded states\n",
        "        :param units: dimension of the hidden state and the attention matrices\n",
        "        :param output_dim: the number of labels in the output space\n",
        "\n",
        "        references:\n",
        "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
        "            \"Neural machine translation by jointly learning to align and translate.\"\n",
        "            arXiv preprint arXiv:1409.0473 (2014).\n",
        "        \"\"\"\n",
        "        self.units = units\n",
        "        self.output_dim = output_dim\n",
        "        self.return_probabilities = return_probabilities\n",
        "        self.activation = activations.get(activation)\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        super(AttentionDecoder, self).__init__(**kwargs)\n",
        "        self.name = name\n",
        "        self.return_sequences = True  # must return sequences\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
        "          for model details that correspond to the matrices here.\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
        "\n",
        "        if self.stateful:\n",
        "            super(AttentionDecoder, self).reset_states()\n",
        "\n",
        "        self.states = [None, None]  # y, s\n",
        "\n",
        "        \"\"\"\n",
        "            Matrices for creating the context vector\n",
        "        \"\"\"\n",
        "\n",
        "        self.V_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='V_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='W_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='U_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.b_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='b_a',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the r (reset) gate\n",
        "        \"\"\"\n",
        "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_r = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_r',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "\n",
        "        \"\"\"\n",
        "            Matrices for the z (update) gate\n",
        "        \"\"\"\n",
        "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_z = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_z',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the proposal\n",
        "        \"\"\"\n",
        "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_p = self.add_weight(shape=(self.units, ),\n",
        "                                   name='b_p',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for making the final prediction vector\n",
        "        \"\"\"\n",
        "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
        "                                   name='C_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
        "                                   name='U_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
        "                                   name='W_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_o = self.add_weight(shape=(self.output_dim, ),\n",
        "                                   name='b_o',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "\n",
        "        # For creating the initial state:\n",
        "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='W_s',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "\n",
        "        self.input_spec = [\n",
        "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, x):\n",
        "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
        "        self.x_seq = x\n",
        "\n",
        "        # apply the a dense layer over the time dimension of the sequence\n",
        "        # do it here because it doesn't depend on any previous steps\n",
        "        # thefore we can save computation time:\n",
        "        self._uxpb = _time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
        "                                             input_dim=self.input_dim,\n",
        "                                             timesteps=self.timesteps,\n",
        "                                             output_dim=self.units)\n",
        "\n",
        "        return super(AttentionDecoder, self).call(x)\n",
        "\n",
        "    def get_initial_state(self, inputs):\n",
        "        # apply the matrix on the first time step to get the initial s0.\n",
        "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
        "\n",
        "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
        "        # output_dim)\n",
        "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
        "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
        "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
        "        y0 = K.tile(y0, [1, self.output_dim])\n",
        "\n",
        "        return [y0, s0]\n",
        "\n",
        "    def step(self, x, states):\n",
        "\n",
        "        ytm, stm = states\n",
        "\n",
        "        # repeat the hidden state to the length of the sequence\n",
        "        _stm = K.repeat(stm, self.timesteps)\n",
        "\n",
        "        # now multiplty the weight matrix with the repeated hidden state\n",
        "        _Wxstm = K.dot(_stm, self.W_a)\n",
        "\n",
        "        # calculate the attention probabilities\n",
        "        # this relates how much other timesteps contributed to this one.\n",
        "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
        "                   K.expand_dims(self.V_a))\n",
        "        at = K.exp(et)\n",
        "        at_sum = K.sum(at, axis=1)\n",
        "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
        "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
        "\n",
        "        # calculate the context vector\n",
        "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
        "        # ~~~> calculate new hidden state\n",
        "        # first calculate the \"r\" gate:\n",
        "\n",
        "        rt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_r)\n",
        "            + K.dot(stm, self.U_r)\n",
        "            + K.dot(context, self.C_r)\n",
        "            + self.b_r)\n",
        "\n",
        "        # now calculate the \"z\" gate\n",
        "        zt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_z)\n",
        "            + K.dot(stm, self.U_z)\n",
        "            + K.dot(context, self.C_z)\n",
        "            + self.b_z)\n",
        "\n",
        "        # calculate the proposal hidden state:\n",
        "        s_tp = activations.tanh(\n",
        "            K.dot(ytm, self.W_p)\n",
        "            + K.dot((rt * stm), self.U_p)\n",
        "            + K.dot(context, self.C_p)\n",
        "            + self.b_p)\n",
        "\n",
        "        # new hidden state:\n",
        "        st = (1-zt)*stm + zt * s_tp\n",
        "\n",
        "        yt = activations.softmax(\n",
        "            K.dot(ytm, self.W_o)\n",
        "            + K.dot(stm, self.U_o)\n",
        "            + K.dot(context, self.C_o)\n",
        "            + self.b_o)\n",
        "\n",
        "        if self.return_probabilities:\n",
        "            return at, [yt, st]\n",
        "        else:\n",
        "            return yt, [yt, st]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "            For Keras internal compatability checking\n",
        "        \"\"\"\n",
        "        if self.return_probabilities:\n",
        "            return (None, self.timesteps, self.timesteps)\n",
        "        else:\n",
        "            return (None, self.timesteps, self.output_dim)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "            For rebuilding models on load time.\n",
        "        \"\"\"\n",
        "        config = {\n",
        "            'output_dim': self.output_dim,\n",
        "            'units': self.units,\n",
        "            'return_probabilities': self.return_probabilities\n",
        "        }\n",
        "        base_config = super(AttentionDecoder, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Uwcz9lxP5q1",
        "colab_type": "code",
        "outputId": "acf2cad8-7ec2-410a-bd62-f6fc082f615d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from random import randint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import array_equal\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "#from attention_decoder import AttentionDecoder\n",
        "\n",
        "# generate a sequence of random integers\n",
        "def generate_sequence(length, n_unique):\n",
        "\treturn [randint(0, n_unique-1) for _ in range(length)]\n",
        "\n",
        "# one hot encode sequence\n",
        "def one_hot_encode(sequence, n_unique):\n",
        "\tencoding = list()\n",
        "\tfor value in sequence:\n",
        "\t\tvector = [0 for _ in range(n_unique)]\n",
        "\t\tvector[value] = 1\n",
        "\t\tencoding.append(vector)\n",
        "\treturn array(encoding)\n",
        "\n",
        "# decode a one hot encoded string\n",
        "def one_hot_decode(encoded_seq):\n",
        "\treturn [argmax(vector) for vector in encoded_seq]\n",
        "\n",
        "# prepare data for the LSTM\n",
        "def get_pair(n_in, n_out, cardinality):\n",
        "\t# generate random sequence\n",
        "\tsequence_in = generate_sequence(n_in, cardinality)\n",
        "\tsequence_out = sequence_in[:n_out] + [0 for _ in range(n_in-n_out)]\n",
        "\t# one hot encode\n",
        "\tX = one_hot_encode(sequence_in, cardinality)\n",
        "\ty = one_hot_encode(sequence_out, cardinality)\n",
        "\t# reshape as 3D\n",
        "\tX = X.reshape((1, X.shape[0], X.shape[1]))\n",
        "\ty = y.reshape((1, y.shape[0], y.shape[1]))\n",
        "\treturn X,y\n",
        "\n",
        "# configure problem\n",
        "n_features = 51\n",
        "n_timesteps_in = 5\n",
        "n_timesteps_out = 2\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(150, input_shape=(n_timesteps_in, n_features), return_sequences=True))\n",
        "model.add(AttentionDecoder(150, n_features))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# train LSTM\n",
        "for epoch in range(50):\n",
        "\t# generate new random sequence\n",
        "\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\t# fit model for one epoch on this sequence\n",
        "\tmodel.fit(X, y, epochs=1, verbose=2)\n",
        "# evaluate LSTM\n",
        "total, correct = 100, 0\n",
        "for _ in range(total):\n",
        "\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\tyhat = model.predict(X, verbose=0)\n",
        "\tif array_equal(one_hot_decode(y[0]), one_hot_decode(yhat[0])):\n",
        "\t\tcorrect += 1\n",
        "print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))\n",
        "# spot check some examples\n",
        "for _ in range(10):\n",
        "\tX,y = get_pair(n_timesteps_in, n_timesteps_out, n_features)\n",
        "\tyhat = model.predict(X, verbose=0)\n",
        "\tprint('Expected:', one_hot_decode(y[0]), 'Predicted', one_hot_decode(yhat[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " - 2s - loss: 3.9314 - acc: 0.0000e+00\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.9043 - acc: 0.0000e+00\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.8870 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.8843 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.8475 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.8058 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.8141 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.7850 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.7478 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.7178 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.7274 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.6410 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.6446 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.5723 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.5123 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.4008 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.4592 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.3366 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.3486 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.2930 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 3.0967 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 2.6441 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 2.5207 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 2.5731 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 2.4822 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 2.1989 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.8524 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.8654 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7730 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5124 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6694 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6400 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.8406 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7282 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.8420 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7594 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6758 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7642 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7516 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.8430 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7783 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6663 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7388 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7354 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6522 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6536 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6752 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.0634 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6296 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6473 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5634 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.0523 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6703 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6307 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5859 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6483 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5407 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5902 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5066 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5533 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5659 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6345 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6422 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5525 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6352 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6118 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5975 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5830 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5769 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7488 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6283 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6127 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6129 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7674 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7419 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5678 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7081 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6123 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5974 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6870 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5616 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6215 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7389 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6285 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4723 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7324 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6515 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5286 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6916 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6486 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5695 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5764 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6745 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6411 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5144 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7205 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6026 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6659 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5571 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6333 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6627 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6658 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6337 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5404 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5046 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5680 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5240 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5915 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5446 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7624 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6580 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6526 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5475 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6115 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4657 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6642 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5817 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5176 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5956 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5477 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5104 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6572 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5212 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7500 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5103 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5746 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5184 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6063 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6952 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5830 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5832 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5979 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7434 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6834 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5127 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5730 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6103 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7238 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.2009 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7231 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6066 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5644 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5004 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6712 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6119 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6136 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7183 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5989 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7474 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4764 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5513 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6072 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6769 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6150 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6113 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6340 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6457 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5344 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5133 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5061 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5830 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6106 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6469 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6397 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4722 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5065 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5104 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6361 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5553 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5259 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6430 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5684 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7735 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.1083 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5826 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5255 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6250 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5561 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6127 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5173 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5942 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6087 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5813 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6957 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6208 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7113 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5580 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7114 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6434 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5825 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5509 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5918 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5525 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5298 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5429 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6621 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5207 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5094 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4341 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4848 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5250 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5006 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4486 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6406 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5478 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5441 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5241 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6164 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5980 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4695 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5522 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5861 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6624 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4002 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5176 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6610 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5946 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6873 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4570 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4864 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5950 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6193 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5374 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6768 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4750 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5595 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4903 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5276 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6257 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5115 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5209 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5511 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4595 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4510 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5780 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5137 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5659 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6445 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5966 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5714 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6571 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6373 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6589 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6262 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5859 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4994 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5189 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5215 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6266 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6280 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6081 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4580 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5234 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5868 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4548 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5054 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5190 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5352 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5406 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6068 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5586 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5350 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6453 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5931 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5065 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6005 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5275 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5362 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6791 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4989 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5020 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5252 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.8251 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4872 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4825 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6394 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5800 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6097 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5526 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5072 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4975 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4008 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5280 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6056 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4872 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5456 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6493 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5134 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5956 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5479 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6705 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5017 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5145 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5769 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5907 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4511 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5267 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5467 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4536 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4938 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4882 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7281 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5101 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5587 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.7492 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5031 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5306 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5598 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5218 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4919 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4337 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5110 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4858 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5232 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4166 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5511 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6444 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5355 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4869 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5370 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6740 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6286 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5636 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5472 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4614 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6328 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5102 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5911 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5862 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5018 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5025 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6629 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4354 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4892 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5253 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.0528 - acc: 1.0000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5978 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6429 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5057 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6347 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4936 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5213 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5539 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5947 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4457 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.3784 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4615 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5978 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5071 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5618 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5679 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5855 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4311 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.1370 - acc: 1.0000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4415 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4889 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.8568 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5394 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5338 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6645 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6583 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5380 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5041 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6112 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.3935 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5350 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5777 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5809 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6251 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5007 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5084 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.6899 - acc: 0.6000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.5097 - acc: 0.8000\n",
            "Epoch 1/1\n",
            " - 0s - loss: 1.4697 - acc: 0.6000\n",
            "Epoch 1/1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-f91a92a90a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_timesteps_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_timesteps_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# fit model for one epoch on this sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;31m# evaluate LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                 \u001b[0mval_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0mval_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Last batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshould_run_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m     \"\"\"Prints `message` and the tensor value when evaluated.\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m     \u001b[0mNote\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mprint_tensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0midentical\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m     \u001b[0mwhich\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOtherwise\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0moperation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtaken\u001b[0m \u001b[0minto\u001b[0m \u001b[0maccount\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m     \u001b[0;31m# Returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m     \"\"\"\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_keras_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[1;32m   1473\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m                                                run_metadata_ptr)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TL2_It6Cqw4",
        "colab_type": "text"
      },
      "source": [
        "# Attention based enc-dec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INMiEU6CGdmz",
        "colab_type": "code",
        "outputId": "a63fb5d2-f271-4d86-882f-693e7d53f5dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install keract"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keract\n",
            "  Downloading https://files.pythonhosted.org/packages/38/50/42e135986807064315de6dc40e2013d4db417186768ffa05f9980ea7db83/keract-3.0.2-py3-none-any.whl\n",
            "Collecting tensorflow>=2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 35kB/s \n",
            "\u001b[?25hCollecting keras>=2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 26.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.6/dist-packages (from keract) (1.17.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (1.11.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (1.27.1)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 39.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (0.1.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (0.9.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 51.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0->keract) (0.34.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.3.1->keract) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.3.1->keract) (3.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow>=2.0->keract) (45.2.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (2.21.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (0.4.1)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (0.2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0->keract) (3.1.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow, keras, keract\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keract-3.0.2 keras-2.3.1 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llnm3mEHCqMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Lambda, dot, Activation, concatenate\n",
        "\n",
        "\n",
        "def attention_3d_block(hidden_states):\n",
        "    # @author: felixhao28.\n",
        "    # hidden_states.shape = (batch_size, time_steps, hidden_size)\n",
        "    hidden_size = int(hidden_states.shape[2])\n",
        "    # Inside dense layer\n",
        "    #              hidden_states            dot               W            =>           score_first_part\n",
        "    # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\n",
        "    # W is the trainable weight matrix of attention Luong's multiplicative style score\n",
        "    score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n",
        "    #            score_first_part           dot        last_hidden_state     => attention_weights\n",
        "    # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)\n",
        "    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n",
        "    score = dot([score_first_part, h_t], [2, 1], name='attention_score')\n",
        "    attention_weights = Activation('softmax', name='attention_weight')(score)\n",
        "    # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\n",
        "    context_vector = dot([hidden_states, attention_weights], [1, 1], name='context_vector')\n",
        "    pre_activation = concatenate([context_vector, h_t], name='attention_output')\n",
        "    attention_vector = Dense(128, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n",
        "    return attention_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cezgNfssEJCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keract import get_activations\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0lX2xggEUGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_recurrent(n, time_steps, input_dim, attention_column=10):\n",
        "    \"\"\"\n",
        "    Data generation. x is purely random except that it's first value equals the target y.\n",
        "    In practice, the network should learn that the target = x[attention_column].\n",
        "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
        "    :param n: the number of samples to retrieve.\n",
        "    :param time_steps: the number of time steps of your series.\n",
        "    :param input_dim: the number of dimensions of each element in the series.\n",
        "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
        "    :return: x: model inputs, y: model targets\n",
        "    \"\"\"\n",
        "    x = np.random.randint(input_dim, size=(n, time_steps))\n",
        "    x = np.eye(input_dim)[x]\n",
        "    y = x[:, attention_column, :]\n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e92BT-JMEXwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
        "    rnn_out = LSTM(32, return_sequences=True)(inputs)\n",
        "    attention_output = attention_3d_block(rnn_out)\n",
        "    output = Dense(INPUT_DIM, activation='sigmoid', name='output')(attention_output)\n",
        "    m = Model(inputs=[inputs], outputs=[output])\n",
        "    print(m.summary())\n",
        "    return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuCpPckrEcl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = 51\n",
        "TIME_STEPS = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2aiIliSEdLn",
        "colab_type": "code",
        "outputId": "46c1ee75-e7ea-4da5-9551-2c1f9f82b57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "n = 30000\n",
        "inputs, outputs = get_data_recurrent(n, TIME_STEPS, INPUT_DIM)\n",
        "m = get_model()\n",
        "m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "m.fit(x=[inputs], y=outputs, epochs=5, batch_size=64, validation_split=0)\n",
        "\n",
        "num_simulations = 10\n",
        "attention_vectors = np.zeros(shape=(num_simulations, TIME_STEPS))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 20, 100)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 20, 32)       17024       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_score_vec (Dense)     (None, 20, 32)       1024        lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "last_hidden_state (Lambda)      (None, 32)           0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_score (Dot)           (None, 20)           0           attention_score_vec[0][0]        \n",
            "                                                                 last_hidden_state[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "attention_weight (Activation)   (None, 20)           0           attention_score[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "context_vector (Dot)            (None, 32)           0           lstm_1[0][0]                     \n",
            "                                                                 attention_weight[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "attention_output (Concatenate)  (None, 64)           0           context_vector[0][0]             \n",
            "                                                                 last_hidden_state[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "attention_vector (Dense)        (None, 128)          8192        attention_output[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 100)          12900       attention_vector[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 39,140\n",
            "Trainable params: 39,140\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 30000 samples\n",
            "Epoch 1/5\n",
            "30000/30000 [==============================] - 17s 569us/sample - loss: 4.5325 - acc: 0.0224\n",
            "Epoch 2/5\n",
            "30000/30000 [==============================] - 17s 568us/sample - loss: 4.2189 - acc: 0.0610\n",
            "Epoch 3/5\n",
            "30000/30000 [==============================] - 17s 560us/sample - loss: 2.7767 - acc: 0.4040\n",
            "Epoch 4/5\n",
            "30000/30000 [==============================] - 17s 556us/sample - loss: 0.4472 - acc: 0.9936\n",
            "Epoch 5/5\n",
            "30000/30000 [==============================] - 17s 559us/sample - loss: 0.0758 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyfyehJIFA8S",
        "colab_type": "code",
        "outputId": "85e461eb-6e56-42bb-d9e6-8ace28ed44f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "for i in range(num_simulations):\n",
        "    testing_inputs_1, testing_outputs = get_data_recurrent(1, TIME_STEPS, INPUT_DIM)\n",
        "    activations = get_activations(m, testing_inputs_1, layer_name='attention_weight')\n",
        "    attention_vec = np.mean(activations['attention_weight'], axis=0).squeeze()\n",
        "    assert np.abs(np.sum(attention_vec) - 1.0) < 1e-5\n",
        "    attention_vectors[i] = attention_vec\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keract/keract.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(model, nodes_to_evaluate, x, y, auto_compile)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_inputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_targets\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_sample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keract/keract.py\u001b[0m in \u001b[0;36meval_fn\u001b[0;34m(k_inputs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_to_evaluate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m     \u001b[0mNote\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mprint_tensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0midentical\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m     \u001b[0mwhich\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOtherwise\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m     \u001b[0;31m# Returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m     \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[1;32m   1473\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Error while reading resource variable attention_score_vec_1/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/attention_score_vec_1/kernel)\n\t [[{{node attention_score_vec_1/Tensordot/ReadVariableOp}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-2a5c064a4682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_simulations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtesting_inputs_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_recurrent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTIME_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_inputs_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'attention_weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mattention_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_vec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keract/keract.py\u001b[0m in \u001b[0;36mget_activations\u001b[0;34m(model, x, layer_name, nodes_to_evaluate, output_format, auto_compile)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0minput_layer_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0minput_layer_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'input_'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_compile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauto_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcraft_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_format_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keract/keract.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(model, nodes_to_evaluate, x, y, auto_compile)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_inputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_targets\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_sample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keract/keract.py\u001b[0m in \u001b[0;36meval_fn\u001b[0;34m(k_inputs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_to_evaluate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m     \"\"\"Prints `message` and the tensor value when evaluated.\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m     \u001b[0mNote\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mprint_tensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0midentical\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m     \u001b[0mwhich\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOtherwise\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0moperation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtaken\u001b[0m \u001b[0minto\u001b[0m \u001b[0maccount\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m     \u001b[0;31m# Returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m     \"\"\"\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_keras_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[1;32m   1473\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m                                                run_metadata_ptr)\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: 2 root error(s) found.\n  (0) Failed precondition: Error while reading resource variable attention_score_vec_1/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/attention_score_vec_1/kernel)\n\t [[{{node attention_score_vec_1/Tensordot/ReadVariableOp}}]]\n\t [[attention_weight_1/Softmax/_31]]\n  (1) Failed precondition: Error while reading resource variable attention_score_vec_1/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/attention_score_vec_1/kernel)\n\t [[{{node attention_score_vec_1/Tensordot/ReadVariableOp}}]]\n0 successful operations.\n0 derived errors ignored."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncr-czkLFDtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention_vector_final = np.mean(attention_vectors, axis=0)\n",
        "attention_df = pd.DataFrame(attention_vector_final, columns=['attention (%)'])\n",
        "attention_df.plot(kind='bar', title='Attention Mechanism as a function of input dimensions.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSGgTc9N4EUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}